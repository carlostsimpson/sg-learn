{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Aucun(e)",
    "colab": {
      "name": "sgLearnNil4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Att-AawHq7"
      },
      "source": [
        "#######   sg-learn-nil4.ipynb\n",
        "#######   a notebook for machine learning proofs for classification of nilpotent semigroups  \n",
        "#######   carlos.simpson@univ-cotedazur.fr\n",
        "#######   https://github.com/carlostsimpson/sg-learn\n",
        "#######   distributed under: GNU General Public License v3.0\n",
        "#######\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "import gc\n",
        "\n",
        "class CoherenceError(Exception):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBdrdHhiwHq-",
        "scrolled": false,
        "collapsed": true
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import optim\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwynvsf3wHrD"
      },
      "source": [
        "# try to put everything on gpu if it exists\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    Dvc = torch.device(\"cuda:0\")  \n",
        "    print(\"Running on GPU cuda:0\")\n",
        "else:\n",
        "    Dvc = torch.device(\"cpu\")\n",
        "    print(\"Running on CPU\")\n",
        "\n",
        "CpuDvc = torch.device(\"cpu\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hECFB_hs8ef6"
      },
      "source": [
        "#Dvc = torch.device(\"cpu\")  # in case you need to force it to cpu (e.g. cuda compatibility too low)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OurblOVMwHrH",
        "scrolled": false
      },
      "source": [
        "### "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s48jNJhUwHrM"
      },
      "source": [
        "def zbinary(depth,z):  # the smallest digits are first here (i.e. reads backward)\n",
        "    binnum = [int(i) for i in bin(z)[2:]]\n",
        "    bnlength = len(binnum)\n",
        "    if bnlength > depth:\n",
        "        print(\"warning applying zbinary to\",z,\"with depth\",depth,\"but bin length was\",bnlength)\n",
        "    outputarray = torch.zeros((depth),dtype = torch.bool,device=Dvc)\n",
        "    for j in range(depth):\n",
        "        if j < bnlength:\n",
        "            outputarray[j] = (binnum[bnlength - j-1] == 1)\n",
        "        else:\n",
        "            outputarray[j] = False\n",
        "    return outputarray\n",
        "\n",
        "def binaryz(depth,binarray):\n",
        "    thez = 0\n",
        "    for i in range(depth):\n",
        "        thez += binarray.to(torch.int)[i] * 2**i\n",
        "    return thez\n",
        "\n",
        "def binaryzbatch(length,depth,binarray_batch):\n",
        "    zbatch = torch.zeros((length),dtype = torch.int64,device=Dvc)\n",
        "    for i in range(depth):\n",
        "        zbatch += binarray_batch.to(torch.int)[:,i] * (2**i)\n",
        "    return zbatch\n",
        "\n",
        "def composepermutations(vector1,vector2):\n",
        "        vector2i64 = vector2.to(torch.int64)\n",
        "        composition = vector1[vector2i64]\n",
        "        return composition\n",
        "    \n",
        "def composedetections(length,detection1,detection2):  # outputs the result of detection vector 2 inserted in detection1\n",
        "    output = torch.zeros((length),dtype = torch.bool,device=Dvc)\n",
        "    output[detection1] = detection2\n",
        "    return output\n",
        "\n",
        "def memReport(style):  # by QuantScientist Solomon K @smth\n",
        "    if style == 'memory':\n",
        "        print(\"gc memory report\")\n",
        "        for obj in gc.get_objects():\n",
        "            if torch.is_tensor(obj):\n",
        "                print(type(obj), obj.size())\n",
        "    if style == 'mg':\n",
        "        print(\"gc memory and garbage report::\",end=' ')\n",
        "        allobjects = gc.get_objects()\n",
        "        all_length = len(allobjects)\n",
        "        elements = torch.zeros((all_length),dtype = torch.int64,device=Dvc)\n",
        "        count = 0\n",
        "        loc = 0\n",
        "        for obj in allobjects:\n",
        "            if torch.is_tensor(obj):\n",
        "                count += 1\n",
        "                elements[loc]= torch.numel(obj)\n",
        "            loc += 1\n",
        "        elcount = elements.sum(0)\n",
        "        print(\"there are\",count,\"torch tensors in play with\",itp(elcount),\"elements\")\n",
        "        #\n",
        "        values,indices = torch.sort(elements,descending = True)\n",
        "        upper = 5\n",
        "        if upper > count:\n",
        "            upper = count\n",
        "        for i in range(upper):\n",
        "            indi = indices[i]\n",
        "            obji = allobjects[indi]\n",
        "            print(obji.size())\n",
        "        print(\"|||\")\n",
        "        for obj in gc.garbage:\n",
        "            if torch.is_tensor(obj):\n",
        "                print(type(obj), obj.size())\n",
        "    return\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrRbLDVtwHrO"
      },
      "source": [
        "def arangeic(x):\n",
        "    ar = torch.arange(x,dtype = torch.int64,device = Dvc)\n",
        "    return ar\n",
        "\n",
        "def itp(x):  # integer to print\n",
        "    return nump(x)\n",
        "\n",
        "def itt(x):  # integer to torch\n",
        "    if torch.is_tensor(x):\n",
        "      return x\n",
        "    else:\n",
        "      return torch.tensor(x,device = Dvc)\n",
        "\n",
        "def itf(x):  # integer to torch.float\n",
        "    return itt(x).to(torch.float)\n",
        "\n",
        "torch_pi = torch.acos(torch.zeros(1)).item() * 2\n",
        "\n",
        "def tdetach(x):\n",
        "    if torch.is_tensor(x):\n",
        "        return x.detach()\n",
        "    else:\n",
        "        return x\n",
        "    \n",
        "def nump(x):\n",
        "    if torch.is_tensor(x):\n",
        "        return x.detach().to(CpuDvc).numpy()\n",
        "    else:\n",
        "        return x\n",
        "    \n",
        "def numpr(x,k):\n",
        "    return np.round(nump(x),k)\n",
        "\n",
        "def numpi(x):\n",
        "    return nump(x.to(torch.int))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mOHWE1rwHrQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF-SPjg1v7S1"
      },
      "source": [
        "class Historical :\n",
        "    def __init__(self,hlength_max):\n",
        "        self.hlength_max = hlength_max\n",
        "        self.hwidth = 20\n",
        "        self.hfwidth = 10\n",
        "        self.prwidth = 20\n",
        "        #\n",
        "        self.hlength = 0\n",
        "        #\n",
        "        self.histi = torch.zeros((self.hlength_max,self.hwidth),dtype = torch.int64,device=Dvc)\n",
        "        self.histf = torch.zeros((self.hlength_max,self.hfwidth),dtype = torch.float,device=Dvc)\n",
        "        #\n",
        "        self.proofrecord = torch.zeros((self.hlength_max,self.prwidth),dtype = torch.int64,device=Dvc)\n",
        "        self.prcursor = 0\n",
        "        self.local_tweak_cursor = 0\n",
        "        self.global_tweak_cursor = 0\n",
        "        #\n",
        "        self.current_proof_valency_frequency = torch.zeros((10),dtype = torch.int64,device=Dvc)\n",
        "        self.current_proof_impossible_count = 0\n",
        "        self.current_proof_done_count = 0\n",
        "        self.current_proof_passive_count = 0\n",
        "        #\n",
        "        self.title_text_sigma_proof = None\n",
        "        self.title_text_sigma_train = None\n",
        "        #\n",
        "        self.training_counter = 0\n",
        "        #\n",
        "        self.proof_nodes_max = 200000\n",
        "        \n",
        "\n",
        "\n",
        "    def reset_current_proof(self):\n",
        "        self.current_proof_valency_frequency[:] = 0\n",
        "        self.current_proof_impossible_count = 0\n",
        "        self.current_proof_done_count = 0\n",
        "        self.current_proof_passive_count = 0\n",
        "        return\n",
        "\n",
        "    def record_current_proof(self):\n",
        "        if self.prcursor >= self.hlength_max:\n",
        "            print(\"proof data recording overflow\")\n",
        "            return\n",
        "        self.proofrecord[self.prcursor,0] = self.current_proof_impossible_count\n",
        "        self.proofrecord[self.prcursor,1] = self.current_proof_done_count\n",
        "        self.proofrecord[self.prcursor,2] = self.current_proof_passive_count\n",
        "        self.proofrecord[self.prcursor,3:13] = self.current_proof_valency_frequency\n",
        "        print(\"recorded\",end=' ')\n",
        "        self.print_proof_recordi(self.prcursor,Pp)\n",
        "        self.prcursor += 1\n",
        "        return\n",
        "\n",
        "    def print_proof_recordi(self,i,Pp):\n",
        "        #\n",
        "        upperval = Pp.beta + 2\n",
        "        if upperval > 10:\n",
        "            upperval = 10\n",
        "        impcount = itp(self.proofrecord[i,0])\n",
        "        donecount = itp(self.proofrecord[i,1])\n",
        "        passivecount = itp(self.proofrecord[i,2])\n",
        "        valency = nump(self.proofrecord[i,3:3 + upperval])\n",
        "        if i <= 0:\n",
        "            print(\"benchmark proof\",end = ' ')\n",
        "        else:\n",
        "            print(\"proof\",(i-1),\".\",end = ' ')\n",
        "        print(\"done\",donecount,\"impossible\",impcount,\"passive\",passivecount,\"by valency\",valency)\n",
        "        return\n",
        "\n",
        "    def print_proof_records(self,Pp):\n",
        "        #\n",
        "        ###\n",
        "        if Pp.profile_filter_on: \n",
        "            prof_filt = 'on'\n",
        "        else:\n",
        "            prof_filt = 'off'\n",
        "        if Pp.halfones_filter_on: \n",
        "            ho_filt = 'on'\n",
        "        else:\n",
        "            ho_filt = 'off'\n",
        "        bl_iter = Pp.basicloop_iterations\n",
        "        bl_train = Pp.basicloop_training_iterations\n",
        "        #\n",
        "        global_p = itp(Pp.global_params)\n",
        "        local_p = itp(Pp.local_params)\n",
        "        ###\n",
        "        #\n",
        "        print(\"-------------------------------------------------------------\")\n",
        "        print(\"proof records for a,b =\",itp(Pp.alpha),itp(Pp.beta),\"model with n=\",Pp.model_n,self.title_text_sigma_proof)\n",
        "        print(F'with {bl_iter} basic loops per proof and {bl_train} training segments per basic loop, profile filter {prof_filt}, halfones filter {ho_filt}')\n",
        "        print(F'global model has {global_p} and local rank+score model has {local_p} trainable parameters')\n",
        "        print(\"-------------------------------------------------------------\")\n",
        "        for i in range(self.prcursor):\n",
        "            self.print_proof_recordi(i,Pp)\n",
        "        print(\"-------------------------------------------------------------\")\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def noiselevel(self,P,count_tensor):\n",
        "        counterf = count_tensor.to(torch.float)\n",
        "        counter_period_units = counterf / P.noise_period\n",
        "        phase = counter_period_units * 2 * torch_pi\n",
        "        one_plus_cos_over_two = (torch.cos(phase) +1.)/2\n",
        "        decay = P.noise_decay ** counter_period_units\n",
        "        level = P.noise_level * one_plus_cos_over_two * decay\n",
        "        return level\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.histi[:,:] = 0\n",
        "        self.histf[:,:] = 0.\n",
        "        self.hlength = 0\n",
        "        self.training_counter = 0\n",
        "        self.prcursor = 0\n",
        "        self.local_tweak_cursor = 0\n",
        "        self.global_tweak_cursor = 0\n",
        "        print(\"reinitialized history\")\n",
        "        return\n",
        "\n",
        "    def increment(self):\n",
        "        if self.hlength >= self.hlength_max:\n",
        "            print(\"Historical at maximum length, can't add any new entries\")\n",
        "            raise CoherenceError(\"exiting\")\n",
        "        cursor = self.hlength\n",
        "        self.hlength += 1\n",
        "        assert 0 <= cursor < self.hlength_max\n",
        "        return cursor\n",
        "\n",
        "    def record_parameters(self,alpha,beta):\n",
        "        cursor = self.increment()\n",
        "        self.histi[cursor,0] = 43  # 43 = parameters\n",
        "        self.histi[cursor,1] = alpha\n",
        "        self.histi[cursor,2] = beta\n",
        "        return\n",
        "\n",
        "    def record_driver(self,alpha,beta):\n",
        "        cursor = self.increment()\n",
        "        self.histi[cursor,0] = 6212  # 6212 = driver\n",
        "        self.histi[cursor,1] = alpha\n",
        "        self.histi[cursor,2] = beta\n",
        "        return\n",
        "\n",
        "    def record_model(self,n):\n",
        "        cursor = self.increment()\n",
        "        self.histi[cursor,0] = 30631  # 30631 = model\n",
        "        self.histi[cursor,1] = n\n",
        "        return\n",
        "\n",
        "\n",
        "    def record_loss(self,style,L1_loss,MSE_loss):\n",
        "        cursor = self.increment()\n",
        "        self.histi[cursor,0] = 1055  # 1055 = loss\n",
        "        if style != 'global' and style != 'local' and style != 'local_ce':\n",
        "            raise CoherenceError(\"unsupported style in record_loss\")\n",
        "        if style == 'global':\n",
        "            self.histi[cursor,1] = 1\n",
        "        if style == 'local':\n",
        "            self.histi[cursor,1] = 2\n",
        "        if style == 'local_ce':\n",
        "            self.histi[cursor,1] = 3\n",
        "        if torch.is_tensor(MSE_loss):\n",
        "            MSE_loss_detach = MSE_loss.detach()\n",
        "        else:\n",
        "            MSE_loss_detach = MSE_loss\n",
        "        self.histf[cursor,0] = L1_loss.detach()\n",
        "        self.histf[cursor,1] = MSE_loss_detach\n",
        "        #\n",
        "        self.training_counter += 1\n",
        "        return\n",
        "\n",
        "\n",
        "    def record_training(self,style,iterations,explore_pre_pool,example_pre_pool,example_pool):\n",
        "        cursor = self.increment()\n",
        "        #\n",
        "        self.histi[cursor,0] = 7246 # 7246 = training\n",
        "        if style != 'global' and style != 'local':\n",
        "            raise CoherenceError(\"unsupported style in record_training\")\n",
        "        if style == 'global':    \n",
        "            self.histi[cursor,1] = 1\n",
        "        if style == 'local':\n",
        "            self.histi[cursor,1] = 2\n",
        "        self.histi[cursor,2] = iterations\n",
        "        self.histi[cursor,3] = explore_pre_pool\n",
        "        self.histi[cursor,4] = example_pre_pool\n",
        "        self.histi[cursor,5] = example_pool\n",
        "        return\n",
        "\n",
        "    def record_full_proof(self,M,steps,cumulative_nodes,done_nodes):\n",
        "        cursor = self.increment()\n",
        "        #\n",
        "        self.histi[cursor,0] = 92007  #  92007 = full proof\n",
        "        self.histi[cursor,1] = steps\n",
        "        self.histi[cursor,2] = cumulative_nodes\n",
        "        self.histi[cursor,3] = done_nodes\n",
        "        if M.benchmark:\n",
        "            self.histi[cursor,4] = 1\n",
        "        else:\n",
        "            self.histi[cursor,4] = 0\n",
        "        return\n",
        "    \n",
        "    def record_dropout_proof(self,style,dropout,steps,ECN):\n",
        "        cursor = self.increment()\n",
        "        #\n",
        "        if style != 'regular' and style != 'adaptive' and style != 'uniform':\n",
        "            raise CoherenceError(\"unsupported style in record_dropout_proof\")\n",
        "        #\n",
        "        self.histi[cursor,0] = 6927  #  6927 = dropout proof\n",
        "        if style == 'regular':\n",
        "            self.histi[cursor,1] = 1\n",
        "        if style == 'adaptive':\n",
        "            self.histi[cursor,1] = 2\n",
        "        if style == 'uniform':\n",
        "            self.histi[cursor,1] = 3\n",
        "        self.histi[cursor,2] = dropout\n",
        "        self.histi[cursor,3] = steps\n",
        "        ecnr = torch.round(ECN).to(torch.int64)\n",
        "        self.histi[cursor,4] = ecnr\n",
        "        return\n",
        "\n",
        "\n",
        "    def print_history(self):\n",
        "        length = self.hlength\n",
        "        print(\"--  --  --  --  --  --  --  --  --  --  --  --  --  --  --\")\n",
        "        print(\"     printing history of length\",itp(length))\n",
        "        print(\"--  --  --  --  --  --  --  --  --  --  --  --  --  --  --\")\n",
        "        for cursor in range(length):\n",
        "            tag = self.histi[cursor,0]\n",
        "            a = itp(self.histi[cursor,1])\n",
        "            b = itp(self.histi[cursor,2])\n",
        "            c = itp(self.histi[cursor,3])\n",
        "            d = itp(self.histi[cursor,4])\n",
        "            e = itp(self.histi[cursor,5])\n",
        "            f = itp(self.histi[cursor,6])\n",
        "            #\n",
        "            x = numpr(self.histf[cursor,0],3)\n",
        "            y = numpr(self.histf[cursor,1],3)\n",
        "            #\n",
        "            print(\"(\",cursor,\")--\",end='')\n",
        "            #\n",
        "            if tag == 43:\n",
        "                print(\"setting parameters for alpha=\",a,\"beta=\",b)\n",
        "            #\n",
        "            if tag == 6212:\n",
        "                print(\"initialize driver for alpha=\",a,\"beta=\",b)\n",
        "            #\n",
        "            if tag == 30631:\n",
        "                print(\"initialize model with n =\",a)\n",
        "            #\n",
        "            if tag == 1055:\n",
        "                if a == 1:\n",
        "                    print(\"test global model, L1 loss\",x,\"MSE loss\",y)\n",
        "                if a == 2:\n",
        "                    print(\"test local model, L1 loss\",x,\"MSE loss\",y)\n",
        "                if a == 3:\n",
        "                    print(\"test local model, CE loss\",x)\n",
        "            #\n",
        "            if tag == 7246:\n",
        "                if a == 1:\n",
        "                    print(\"training global model\",end=' ')\n",
        "                if a == 2:\n",
        "                    print(\"training global model\",end=' ')\n",
        "                print(\"iterations\",b,\"explore prepool\",c,\"example prepool\",d,\"example_pool\",e)\n",
        "            #\n",
        "            if tag == 92007:\n",
        "                if d > 0:\n",
        "                    print(\"FULL PROOF (BENCHMARK) in\",a,\"steps\",b,\"cumulative nodes\",c,\"done leaves\")\n",
        "                else:\n",
        "                    print(\"FULL PROOF in\",a,\"steps\",b,\"cumulative nodes\",c,\"done leaves\")\n",
        "            #\n",
        "            if tag == 6927:\n",
        "                print(\"proof with dropout style\",end='')\n",
        "                if a == 1:\n",
        "                    print(\" regular \",end='')\n",
        "                if a == 2:\n",
        "                    print(\" adaptive \",end='')\n",
        "                if a == 3:\n",
        "                    print(\" uniform \",end='')\n",
        "                print(\"threshold\",b,\"in\",c,\"steps with estimated cumulative nodes\",d)\n",
        "        #\n",
        "        print(\"--  --  --  --  --  --  --  --  --  --  --  --  --  --  --\")\n",
        "        print(\"     end printing history of length\",itp(length))\n",
        "        print(\"--  --  --  --  --  --  --  --  --  --  --  --  --  --  --\")\n",
        "        #\n",
        "        return\n",
        "\n",
        "                  \n",
        "\n",
        "    def graph_history(self,P,style):\n",
        "        #\n",
        "        alpha = P.alpha\n",
        "        beta = P.beta\n",
        "        nu = P.model_n\n",
        "        #\n",
        "        ###\n",
        "        if P.profile_filter_on: \n",
        "            prof_filt = 'on'\n",
        "        else:\n",
        "            prof_filt = 'off'\n",
        "        if P.halfones_filter_on: \n",
        "            ho_filt = 'on'\n",
        "        else:\n",
        "            ho_filt = 'off'\n",
        "        bl_iter = P.basicloop_iterations\n",
        "        bl_train = P.basicloop_training_iterations\n",
        "        #\n",
        "        global_p = itp(P.global_params)\n",
        "        local_p = itp(P.local_params)\n",
        "        ###\n",
        "        plotcount = 0\n",
        "        for cursor in range(self.hlength):\n",
        "            tag = self.histi[cursor,0]\n",
        "            d = self.histi[cursor,4]\n",
        "            if tag == 92007 and d == 0:\n",
        "                plotcount += 1\n",
        "        baseline = 0\n",
        "        ecn_graph = torch.zeros((plotcount),dtype = torch.int64,device=Dvc)\n",
        "        attempts = arangeic(plotcount)\n",
        "        attempt_number = 0\n",
        "        for cursor in range(self.hlength):\n",
        "            tag = self.histi[cursor,0]\n",
        "            b = self.histi[cursor,2]\n",
        "            d = self.histi[cursor,4]\n",
        "            if tag == 92007:\n",
        "                if d > 0:\n",
        "                    baseline = b\n",
        "                else:\n",
        "                    ecn_graph[attempt_number] = b\n",
        "                    attempt_number += 1\n",
        "        ecn_graph = torch.clamp(ecn_graph,0,self.proof_nodes_max)\n",
        "        plt.clf()\n",
        "        if style == 'big':\n",
        "            plt.figure(figsize = (17,8))\n",
        "        else:\n",
        "            plt.figure(figsize = (8,4))\n",
        "        ###\n",
        "        phrase1 = F'Proofs for a= {alpha}, b={beta} for model with n= {nu} and heuristic baseline, {HST.title_text_sigma_proof}'\n",
        "        phrase2 = F'with {bl_iter} basic loops per proof and {bl_train} training segments per basic loop, profile filter {prof_filt}, halfones filter {ho_filt}'\n",
        "        phrase3 = F'global model has {global_p} and local rank+score model has {local_p} trainable parameters'\n",
        "        plt.title(phrase1 + '\\n' + phrase2 + '\\n' + phrase3)\n",
        "        ###\n",
        "        plt.xlabel('number of training rounds')\n",
        "        plt.ylabel('cumulative nodes')\n",
        "        plt.plot([0,plotcount - 1],[itp(baseline),itp(baseline)],'deepskyblue')\n",
        "        plt.plot(nump(attempts),nump(ecn_graph),'.-')\n",
        "        plt.show()\n",
        "        #\n",
        "        pcg = 0\n",
        "        pcl = 0\n",
        "        for cursor in range(self.hlength):\n",
        "            tag = self.histi[cursor,0]\n",
        "            a = self.histi[cursor,1]\n",
        "            if tag == 1055:\n",
        "                if a == 1:\n",
        "                    pcg += 1\n",
        "                if a == 2:\n",
        "                    pcl += 1\n",
        "                if a == 3:\n",
        "                    pcl += 1\n",
        "        L1graph_local = torch.zeros((pcl),dtype = torch.float,device=Dvc)\n",
        "        MSEgraph_local = torch.zeros((pcl),dtype = torch.float,device=Dvc)\n",
        "        CEgraph_local = torch.zeros((pcl),dtype = torch.float,device=Dvc)\n",
        "        L1graph_global = torch.zeros((pcg),dtype = torch.float,device=Dvc)\n",
        "        MSEgraph_global = torch.zeros((pcg),dtype = torch.float,device=Dvc)\n",
        "        measurements_local = arangeic(pcl)\n",
        "        measurements_global = arangeic(pcg)\n",
        "        measurement_number_local = 0\n",
        "        measurement_number_global = 0\n",
        "        for cursor in range(self.hlength):\n",
        "            tag = self.histi[cursor,0]\n",
        "            a = self.histi[cursor,1]\n",
        "            x = self.histf[cursor,0]\n",
        "            y = self.histf[cursor,1]\n",
        "            if tag == 1055:\n",
        "                if a == 1:\n",
        "                    L1graph_global[measurement_number_global] = x\n",
        "                    MSEgraph_global[measurement_number_global] = y\n",
        "                    measurement_number_global += 1\n",
        "                if a == 2:\n",
        "                    L1graph_local[measurement_number_local] = x\n",
        "                    MSEgraph_local[measurement_number_local] = y\n",
        "                    measurement_number_local += 1\n",
        "                if a == 3:\n",
        "                    CEgraph_local[measurement_number_local] = x\n",
        "                    measurement_number_local += 1\n",
        "        L1avg_global = (L1graph_global[0:pcg-2] + L1graph_global[1:pcg-1] + L1graph_global[2:pcg])/3.\n",
        "        L1avg_global = torch.clamp(L1avg_global,0.,0.2)\n",
        "        L1avg_local = (L1graph_local[0:pcl-2] + L1graph_local[1:pcl-1] + L1graph_local[2:pcl])/3.\n",
        "        #\n",
        "        L1avg_local_red = L1avg_local /2.  # so it fits on the graph better, with the current adaptive_score function\n",
        "        #\n",
        "        L1avg_local_red = torch.clamp(L1avg_local_red,0.,0.2)\n",
        "        MSEavg_global = (MSEgraph_global[0:pcg-2] + MSEgraph_global[1:pcg-1] + MSEgraph_global[2:pcg])/3.\n",
        "        MSEavg_global = torch.clamp(MSEavg_global,0.,0.2)\n",
        "        MSEavg_local = (MSEgraph_local[0:pcl-2] + MSEgraph_local[1:pcl-1] + MSEgraph_local[2:pcl])/3.\n",
        "        MSEavg_local = torch.clamp(MSEavg_local,0.,0.2)\n",
        "        CEavg_local = (CEgraph_local[0:pcl-2] + CEgraph_local[1:pcl-1] + CEgraph_local[2:pcl])/3.\n",
        "        CEavg_local = CEavg_local / 10.\n",
        "        CEavg_local = torch.clamp(CEavg_local,0.,0.2) \n",
        "        #\n",
        "        #\n",
        "        noiselevel = HST.noiselevel(P,measurements_global)\n",
        "        #\n",
        "        plt.clf()\n",
        "        if style == 'big':\n",
        "            plt.figure(figsize = (17,8))\n",
        "        else:\n",
        "            plt.figure(figsize = (8,4))\n",
        "        ###\n",
        "        phrase1b = F'Training for a= {alpha}, b={beta} for model with n= {nu}, {HST.title_text_sigma_train}'\n",
        "        # phrase 2 is the same as before\n",
        "        plt.title(phrase1b + '\\n' + phrase2 + '\\n' + phrase3)\n",
        "        ###\n",
        "        plt.xlabel('training')\n",
        "        plt.ylabel('loss')\n",
        "        #\n",
        "        plt.plot(nump(measurements_local[5:pcl]),numpr(L1avg_local_red[3:pcl-2],4),label = 'local-L1/2')\n",
        "        plt.plot(nump(measurements_global[5:pcg]),numpr(L1avg_global[3:pcg-2],4),label = 'global-L1')\n",
        "        #\n",
        "        plt.plot(nump(measurements_local[5:pcl]),numpr(MSEavg_local[3:pcl - 2],5),label = 'local-MSE')\n",
        "        plt.plot(nump(measurements_global[5:pcg]),numpr(MSEavg_global[3:pcg - 2],5),label = 'global-MSE')\n",
        "        #\n",
        "        plt.plot(nump(measurements_global[5:pcg]),numpr(noiselevel[3:pcg-2],5),label = 'noise')\n",
        "        #\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        ###\n",
        "        self.print_proof_records(P)\n",
        "        return\n",
        "\n",
        "    \n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tX8keQpmvQ1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP39mixNAE0W"
      },
      "source": [
        "class SymmetricGroup :\n",
        "    def __init__(self,p):  \n",
        "        #\n",
        "        if p < 1:\n",
        "            print(\"can't initialize a symmetric group with size\",p)\n",
        "            raise CoherenceError(\"exiting\")\n",
        "        if p > 9:\n",
        "            print(\"symmetric group size\",p,\"is probably going to cause a memory overflow somewhere\")\n",
        "            raise CoherenceError(\"exiting\")\n",
        "        #\n",
        "        self.p = p\n",
        "        #self.subgroups_max = subgroups_max # this is not currently used\n",
        "        #\n",
        "        self.gtlength = 1\n",
        "        for i in range(self.p):\n",
        "            self.gtlength *= (i+1)\n",
        "        #\n",
        "        self.grouptable = self.makegrouptable()\n",
        "        self.gtbinary = self.makegrouptablebinary()\n",
        "        #self.multiplicationtable = self.makemult()\n",
        "        #self.inverse = self.makeinverse()\n",
        "        self.inversetable = self.makeinversetable()\n",
        "        #\n",
        "\n",
        "        \n",
        "    def symmetricgrouptable(self,k):\n",
        "        assert k > 0\n",
        "        if k == 1:\n",
        "          sgt = torch.zeros((1),dtype = torch.int64,device = Dvc)\n",
        "          return 1,sgt\n",
        "        length_prev,sgtprev = self.symmetricgrouptable(k-1)\n",
        "        length = length_prev * k\n",
        "        krange = torch.arange((k),dtype = torch.int64,device=Dvc)  # same as arangeic(k)\n",
        "        krangevx = krange.view(k,1).expand(k,length_prev)\n",
        "        krangevx2 = krange.view(k,1,1).expand(k,length_prev,k-1)\n",
        "        #\n",
        "        sgtprev_vx = sgtprev.view(1,length_prev,k-1).expand(k,length_prev,k-1)\n",
        "        #\n",
        "        krange1vxr = krange.view(k,1).expand(k,k).reshape(k*k)\n",
        "        krange2vxr = krange.view(1,k).expand(k,k).reshape(k*k)\n",
        "        gappedtablev = krange2vxr[(krange1vxr != krange2vxr)]\n",
        "        gappedtable = gappedtablev.view(k,k-1)\n",
        "        #\n",
        "        afterpart = gappedtable[krangevx2,sgtprev_vx]\n",
        "        beforepart = krange.view(k,1,1).expand(k,length_prev,1)\n",
        "        newtablev = torch.cat((beforepart,afterpart),2)\n",
        "        newtable = newtablev.view(length,k)\n",
        "        return length,newtable\n",
        "        \n",
        "    def makegrouptable(self):\n",
        "        length,table = self.symmetricgrouptable(self.p)\n",
        "        assert length == self.gtlength\n",
        "        #print(\"making group table for symmetric group, as an array of shape\",table.size())\n",
        "        return table\n",
        "\n",
        "    \n",
        "    def findpermutation(self,batchlength,vector):\n",
        "        vectorvx = vector.view(batchlength,1,self.p).expand(batchlength,self.gtlength,self.p)\n",
        "        grouptablevx = self.grouptable.view(1,self.gtlength,self.p).expand(batchlength,self.gtlength,self.p)\n",
        "        detection = (vectorvx == self.grouptable).all(2)\n",
        "        values,permutation = torch.max((detection.to(torch.int)),1)\n",
        "        assert (values == 1).all(0)\n",
        "        return permutation\n",
        "    \n",
        "    \n",
        "    def makemult(self):\n",
        "        if self.p > 7:\n",
        "          print(\"multiplication table for symmetric group of size\",self.p,\"would probably crash\")\n",
        "          raise CoherenceError(\"exiting\")\n",
        "        print(\"setting up multiplication table...\",end=' ')\n",
        "        mult = torch.zeros((self.gtlength,self.gtlength),dtype = torch.int64,device = Dvc)\n",
        "        for x in range(self.gtlength):\n",
        "            xvector = self.grouptable[x]\n",
        "            comptable = xvector[self.grouptable]\n",
        "            mult[x,:] = self.findpermutation(self.gtlength,comptable)\n",
        "        print(\"done\")\n",
        "        return mult\n",
        "    \n",
        "    def makeinverse(self):\n",
        "        invdetection = (self.multiplicationtable == 0)\n",
        "        values,inverse = torch.max((invdetection.to(torch.int)),1)\n",
        "        return inverse\n",
        "    \n",
        "    def makeinversetable(self):\n",
        "        gl = self.gtlength\n",
        "        p=self.p\n",
        "        tablevx = self.grouptable.view(gl,p,1).expand(gl,p,p)\n",
        "        yrangevx = arangeic(p).view(1,1,p).expand(gl,p,p) \n",
        "        delta = (tablevx == yrangevx).to(torch.int64)\n",
        "        values,inversetable = torch.max(delta,1)\n",
        "        return inversetable\n",
        "\n",
        "    \n",
        "    \n",
        "    ##########@ for the list of subgroups ##########\n",
        "    \n",
        "    def subgroupgen(self,thesubgroup,thex):  # outputs the subgroup generated by thesubgroup and thex\n",
        "        currentsubset = torch.zeros((self.gtlength),dtype = torch.bool,device=Dvc)\n",
        "        currentsubset[thesubgroup] = True\n",
        "        currentsubset[thex] = True\n",
        "        for i in range(1000):\n",
        "            currentlength = currentsubset.to(torch.int).sum(0).clone()\n",
        "            cl2 = currentlength * currentlength\n",
        "            #print(\"current length\",itp(currentlength))\n",
        "            mtcurrent1 = self.multiplicationtable[currentsubset]\n",
        "            mtcurrent1p = mtcurrent1.permute(1,0)\n",
        "            mtcurrent2 = mtcurrent1p[currentsubset]\n",
        "            mtcurrent2vx = mtcurrent2.view(1,cl2).expand(self.gtlength,cl2)\n",
        "            grouparangevx = arangeic(self.gtlength).view(self.gtlength,1).expand(self.gtlength,cl2)\n",
        "            products = (grouparangevx == mtcurrent2vx).any(1)\n",
        "            currentsubset = currentsubset | products\n",
        "            newlength = currentsubset.to(torch.int).sum(0)\n",
        "            if newlength == currentlength:\n",
        "                #print(\"break\")\n",
        "                break\n",
        "        return currentsubset\n",
        "    \n",
        "    def findsubgroup(self,thesubgroup):\n",
        "        currentsglist = self.subgroup[0:self.sglistlength,:]\n",
        "        thesubgroupvx = thesubgroup.view(1,self.gtlength).expand(self.sglistlength,self.gtlength)\n",
        "        findsg = (currentsglist == thesubgroupvx).all(1)\n",
        "        if findsg.any(0):\n",
        "            assert findsg.to(torch.int).sum(0) == 1\n",
        "            sgrange = arangeic(self.sglistlength)\n",
        "            sgnumber = itp(sgrange[findsg][0])\n",
        "            return True,sgnumber\n",
        "        else:\n",
        "            return False, None\n",
        "        \n",
        "    \n",
        "    def addnextsubgroup(self):\n",
        "        for k in range(self.sglistlength):\n",
        "            thesubgroup = self.subgroup[k]\n",
        "            #print(\"try subgroup\",k)\n",
        "            for x in range(self.gtlength):\n",
        "                if not thesubgroup[x]:\n",
        "                    #print(\"try x=\",x)\n",
        "                    sggenx = self.subgroupgen(thesubgroup,x)\n",
        "                    fsg,sgnumber = self.findsubgroup(sggenx)\n",
        "                    if not fsg:\n",
        "                        self.subgroup[self.sglistlength] = sggenx\n",
        "                        self.sgsize[self.sglistlength] = sggenx.to(torch.int).sum(0)\n",
        "                        #print(\"add subgroup\",itp(self.sglistlength),\"of length\",itp(self.sgsize[self.sglistlength]))\n",
        "                        self.sglistlength += 1\n",
        "                        return True\n",
        "        return False\n",
        "    \n",
        "    def createsubgrouplist(self):\n",
        "        # the identity\n",
        "        self.sglistlength = 1\n",
        "        self.subgroup.masked_fill_(truetensor,False)\n",
        "        self.sgsize.masked_fill_(truetensor,0)\n",
        "        self.sgsize[0] = 1\n",
        "        self.subgroup[0,0] = True\n",
        "        for i in range(self.subgroups_max - 1):\n",
        "            ansg = self.addnextsubgroup()\n",
        "            if not ansg:\n",
        "                break\n",
        "        print(\"created a list of\",itp(self.sglistlength),\"subgroups\")\n",
        "        return\n",
        "        \n",
        "        \n",
        "    def findsubgroupbatch(self,batchsize,sgbatch):\n",
        "        currentsglist = self.subgroup[0:self.sglistlength,:]\n",
        "        currentsglistvx = currentsglist.view(1,self.sglistlength,self.gtlength).expand(batchsize,self.sglistlength,self.gtlength)\n",
        "        sgbatchvx = sgbatch.view(batchsize,1,self.gtlength).expand(batchsize,self.sglistlength,self.gtlength)\n",
        "        findsg = (currentsglist == thesubgroupvx).all(2)\n",
        "        #\n",
        "        assert(findsg.to(torch.int).sum(1) == 1).all(0)\n",
        "        #\n",
        "        sglistarangevx = arangeic(self.sglistlength).view(1,self.sglistlength).expand(batchsize,self.sglistlength)\n",
        "        sglistarangevxv = sglistarangevx.reshape(batchsize*self.sglistlength)\n",
        "        findsgv = findsg.reshape(batchsize*self.sglistlength)\n",
        "        output = sglistarangevxv[findsgv]\n",
        "        return output\n",
        "    \n",
        "    def makegrouptablebinary(self):\n",
        "        p = self.p\n",
        "        gl = self.gtlength\n",
        "        if p > 7:\n",
        "            print(\"warning: not making binary table for p=\",itp(p),\"> 7\")\n",
        "            return None\n",
        "        blength = 2**p\n",
        "        brange = arangeic(blength)\n",
        "        zbinarytable = torch.zeros((blength,p),dtype = torch.bool,device=Dvc)\n",
        "        for z in range(blength):\n",
        "            zbinarytable[z,:] = zbinary(p,z)\n",
        "        gtb = self.grouptable.view(gl,1,p).expand(gl,blength,p).reshape(gl*blength,p)\n",
        "        brange = arangeic(blength).view(1,blength,1).expand(gl,blength,p).reshape(gl*blength,p)\n",
        "        #\n",
        "        gtb_mod = zbinarytable[brange,gtb]\n",
        "        #\n",
        "        gt_binaryv = binaryzbatch(gl*blength,p,gtb_mod)\n",
        "        gt_binary = gt_binaryv.view(gl,blength)\n",
        "        #print(\"made group table binary\")\n",
        "        return gt_binary\n",
        "                                                                        \n",
        "                                                                                  \n",
        "                                                                                \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfWcyLvqAE0W"
      },
      "source": [
        "### this is the cell where the neural networks are created. We include input and output\n",
        "### template assertions in case one wants to experiment different network architectures\n",
        "### \n",
        "\n",
        "class PrepareInputLayer(nn.Module):  \n",
        "    \n",
        "    def __init__(self,pp):\n",
        "        super(PrepareInputLayer, self).__init__()\n",
        "        self.pp = pp\n",
        "        self.a = pp.alpha\n",
        "        self.b = pp.beta\n",
        "        self.bz = self.b + 1\n",
        "        #\n",
        "        self.n = self.pp.model_n\n",
        "        #\n",
        "        a=self.a\n",
        "        bz=self.bz\n",
        "        self.channels = 5*bz + 2*a \n",
        "        #\n",
        "        \n",
        "        \n",
        "    def forward(self,Data):\n",
        "        #\n",
        "        a = self.a\n",
        "        b = self.b\n",
        "        bz = self.bz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        ############################\n",
        "        # input template assertions:\n",
        "        #\n",
        "        assert prod.dtype == torch.bool \n",
        "        assert left.dtype == torch.bool \n",
        "        assert right.dtype == torch.bool \n",
        "        assert ternary.dtype == torch.bool \n",
        "        #\n",
        "        assert prod.size() == torch.Size([length,a,a,bz])\n",
        "        assert left.size() == torch.Size([length,a,bz,2])\n",
        "        assert right.size() == torch.Size([length,bz,a,2])\n",
        "        assert ternary.size() == torch.Size([length,a,a,a,2])\n",
        "        #\n",
        "        ############################\n",
        "        #\n",
        "        prod_data = prod.permute(0,3,1,2).view(length,bz,a,a)\n",
        "        left_data = left.permute(0,3,2,1).reshape(length,2,bz,a,1).expand(length,2,bz,a,a)\n",
        "        right_data = right.permute(0,3,1,2).reshape(length,2,bz,1,a).expand(length,2,bz,a,a)\n",
        "        ternary_data = ternary.permute(0,4,2,1,3).reshape(length,2,a,a,a)\n",
        "        with torch.no_grad():\n",
        "            prod_f = prod_data.float()\n",
        "            prod_denom = prod_f.sum(1).view(length,1,a,a).expand(length,bz,a,a)\n",
        "            prod_denom = torch.clamp(prod_denom,1.,100.)\n",
        "            prod_ren = prod_f / prod_denom\n",
        "            prod_ren = (bz * prod_ren) - 1.\n",
        "            #\n",
        "            left_f = left_data.float()\n",
        "            left_denom = left_f.sum(1).view(length,1,bz,a,a).expand(length,2,bz,a,a)\n",
        "            left_denom = torch.clamp(left_denom,1.,100.)\n",
        "            left_ren = (left_f / left_denom).view(length,2*bz,a,a)\n",
        "            left_ren = left_ren - 0.5\n",
        "            #\n",
        "            right_f = right_data.float()\n",
        "            right_denom = right_f.sum(1).view(length,1,bz,a,a).expand(length,2,bz,a,a)\n",
        "            right_denom = torch.clamp(right_denom,1.,100.)\n",
        "            right_ren = (right_f / right_denom).view(length,2*bz,a,a)\n",
        "            right_ren = right_ren - 0.5\n",
        "            #\n",
        "            ternary_f = ternary_data.float()\n",
        "            ternary_denom = ternary_f.sum(1).view(length,1,a,a,a).expand(length,2,a,a,a)\n",
        "            ternary_denom = torch.clamp(ternary_denom,1.,100.)\n",
        "            ternary_ren = (ternary_f / ternary_denom).reshape(length,2*a,a,a)\n",
        "            ternary_ren = ternary_ren - 0.5\n",
        "            #\n",
        "            initial_data = torch.cat((prod_ren,left_ren,right_ren,ternary_ren),1).float()\n",
        "        #\n",
        "        assert initial_data.size() == torch.Size([length,self.channels,a,a])\n",
        "        #\n",
        "        return initial_data, prod_data\n",
        "        \n",
        "        \n",
        "    \n",
        "class OutputLayerScalar(nn.Module):  \n",
        "    \n",
        "    def __init__(self,pp,channels_in,channels_mid):\n",
        "        super(OutputLayerScalar, self).__init__()\n",
        "        self.pp = pp\n",
        "        self.a = self.pp.alpha\n",
        "        #\n",
        "        self.channels_in = channels_in\n",
        "        self.channels_mid = channels_mid\n",
        "        #\n",
        "        self.lrl = nn.LeakyReLU()\n",
        "        #\n",
        "        self.layerD1 = nn.Linear(channels_in,channels_mid,1)\n",
        "        self.layerD2 = nn.Linear(channels_mid,channels_mid,1)\n",
        "        self.layerD3 = nn.Linear(2*channels_mid,1,1)\n",
        "        \n",
        "        \n",
        "    def forward(self,layer_data):\n",
        "        #\n",
        "        length = layer_data.size()[0]\n",
        "        #\n",
        "        layer_in = layer_data.view(length,self.channels_in)\n",
        "        yD1 = self.lrl(self.layerD1(layer_in))\n",
        "        yD2 = self.lrl(self.layerD2(yD1))\n",
        "        yD3 = self.layerD3(torch.cat((yD1,yD2),1))\n",
        "        #\n",
        "        yScore = yD3.view(length)\n",
        "        #\n",
        "        return yScore\n",
        "\n",
        "class OutputLayer2d(nn.Module):  \n",
        "    \n",
        "    def __init__(self,pp,channels_array,channels_mid,channels_extra):\n",
        "        super(OutputLayer2d, self).__init__()\n",
        "        self.pp = pp\n",
        "        self.a = self.pp.alpha\n",
        "        #\n",
        "        self.channels_array = channels_array\n",
        "        self.channels_in = self.channels_array * self.a * self.a\n",
        "        self.channels_mid = channels_mid\n",
        "        self.channels_extra = channels_extra\n",
        "        #\n",
        "        self.lrl = nn.LeakyReLU()\n",
        "        #\n",
        "        self.layerD1 = nn.Linear(self.channels_in,channels_mid,1)\n",
        "        self.layerD2 = nn.Linear(channels_mid,channels_mid,1)\n",
        "        self.layerD3 = nn.Linear(2*channels_mid,channels_extra * self.a*self.a,1)\n",
        "        #\n",
        "        self.layerD4 = nn.Conv2d(self.channels_array + self.channels_extra,self.channels_mid,1)\n",
        "        self.layerD5 = nn.Conv2d(self.channels_mid,self.channels_mid,1)\n",
        "        self.layerD6 = nn.Conv2d(2*self.channels_mid + self.channels_extra,1,1)\n",
        "        \n",
        "    def forward(self,layer_data):\n",
        "        #\n",
        "        length = layer_data.size()[0]\n",
        "        #\n",
        "        layer_in = layer_data.view(length,self.channels_in)\n",
        "        yD1 = self.lrl(self.layerD1(layer_in))\n",
        "        yD2 = self.lrl(self.layerD2(yD1))\n",
        "        yD3 = self.layerD3(torch.cat((yD1,yD2),1))\n",
        "        #\n",
        "        yD3v = yD3.view(length,self.channels_extra,self.a,self.a)\n",
        "        layer_array = layer_data.view(length,self.channels_array,self.a,self.a)\n",
        "        yDcat = torch.cat((yD3v,layer_array),1)\n",
        "        yD4 = self.lrl(self.layerD4(yDcat))\n",
        "        yD5 = self.lrl(self.layerD5(yD4))\n",
        "        yD6 = self.layerD6(torch.cat((yD3v,yD4,yD5),1))\n",
        "        #\n",
        "        yScore = yD6.view(length,self.a,self.a)\n",
        "        #\n",
        "        return yScore\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "class SGNetProcess(nn.Module): \n",
        "      \n",
        "    def __init__(self,pp):\n",
        "        super(SGNetProcess, self).__init__()\n",
        "        #\n",
        "        self.pp = pp\n",
        "        self.a = self.pp.alpha\n",
        "        self.bz = self.pp.betaz\n",
        "        #\n",
        "        #\n",
        "        self.lrl = nn.LeakyReLU()\n",
        "        #\n",
        "        self.prep = PrepareInputLayer(self.pp)\n",
        "        #\n",
        "        self.channels = self.prep.channels\n",
        "        #\n",
        "        self.n = self.pp.model_n \n",
        "        n = self.n\n",
        "        #\n",
        "        self.array_channels = n\n",
        "        self.process_channels = self.array_channels*self.a*self.a\n",
        "        #\n",
        "        self.convA1 = nn.Conv2d(self.channels,8*n,1)\n",
        "        self.convA2 = nn.Conv2d(8*n,8*n,[5,1],padding = [2,0],padding_mode = 'circular',groups = n)\n",
        "        self.convA3 = nn.Conv2d(8*n,8*n,[1,5],padding = [0,2],padding_mode = 'circular',groups = n)\n",
        "        self.convA4 = nn.Conv2d(8*n,8*n,[5,1],padding = [2,0],padding_mode = 'circular',groups = n)\n",
        "        self.convA5 = nn.Conv2d(8*n,8*n,[1,5],padding = [0,2],padding_mode = 'circular',groups = n)\n",
        "        self.convA5tg = nn.Conv2d(40*n,8*n,1)\n",
        "        self.convA6 = nn.Conv2d(8*n,8*n,[5,1],padding = [2,0],padding_mode = 'circular',groups = n)\n",
        "        self.convA7 = nn.Conv2d(9*n,8*n,[1,5],padding = [0,2],padding_mode = 'circular',groups = n)\n",
        "        self.convA8 = nn.Conv2d(8*n,8*n,[5,1],padding = [2,0],padding_mode = 'circular',groups = n)\n",
        "        self.convA9 = nn.Conv2d(8*n,8*n,[1,5],padding = [0,2],padding_mode = 'circular',groups = n)\n",
        "        #\n",
        "        self.convB = nn.Conv2d(40*n,n,1)\n",
        "        #\n",
        "        self.convC1 = nn.Conv1d(8*n*self.a*self.a,8*n,1,groups=8*n)\n",
        "        self.convC2 = nn.Conv1d(8*n,n*self.a*self.a,1,groups = n)\n",
        "        #\n",
        "        ##\n",
        "\n",
        "    def forward(self,Data):\n",
        "        #\n",
        "        initial_data,prod_data = self.prep(Data)\n",
        "        length = Data['length']\n",
        "        n = self.n\n",
        "        #\n",
        "        yA1 = self.lrl(self.convA1(initial_data))\n",
        "        yA2 = self.lrl(self.convA2(yA1))\n",
        "        yA3 = self.lrl(self.convA3(yA2))\n",
        "        yA4 = self.lrl(self.convA4(yA3))\n",
        "        yA5 = self.lrl(self.convA5(yA4))\n",
        "        yA5tg = self.lrl(self.convA5tg(torch.cat((yA1,yA2,yA3,yA4,yA5),1)))\n",
        "        yA6 = self.lrl(self.convA6(yA5tg))\n",
        "        #\n",
        "        yA4side = yA4.view(length,8*n*self.a*self.a,1)\n",
        "        yC1 = self.lrl(self.convC1(yA4side))\n",
        "        yC2 = self.lrl(self.convC2(yC1)).view(length,n,self.a,self.a)\n",
        "        yA6side = torch.cat((yA6,yC2),1)\n",
        "        #\n",
        "        yA7 = self.lrl(self.convA7(yA6side))\n",
        "        yA8 = self.lrl(self.convA8(yA7))\n",
        "        yA9 = self.lrl(self.convA9(yA8))   \n",
        "        #\n",
        "        yB = self.lrl(self.convB(torch.cat((yA1,yA3,yA5tg,yA7,yA9),1)))\n",
        "        #\n",
        "        yProcessed = yB.view(length,self.process_channels)\n",
        "        #\n",
        "        return yProcessed \n",
        "        \n",
        "        \n",
        "class SGNetGlobal(nn.Module):  \n",
        "    \n",
        "    def __init__(self,pp):\n",
        "        super(SGNetGlobal, self).__init__()\n",
        "        #\n",
        "        self.pp = pp\n",
        "        self.a = self.pp.alpha\n",
        "        # \n",
        "        self.process = SGNetProcess(self.pp)\n",
        "        #\n",
        "        self.outlayer = OutputLayerScalar(self.pp,self.process.process_channels,32)\n",
        "        \n",
        "\n",
        "    def forward(self,Data):\n",
        "        #\n",
        "        # see the PrepareInputLayer() class above for the input template assertions on Data\n",
        "        #\n",
        "        yProcessed = self.process(Data)\n",
        "        #\n",
        "        yScore = self.outlayer(yProcessed)\n",
        "        #\n",
        "        #############################\n",
        "        # output template assertions:\n",
        "        length = Data['length']\n",
        "        assert yScore.dtype == torch.float\n",
        "        assert yScore.size() == torch.Size([length])\n",
        "        #############################\n",
        "        #\n",
        "        return yScore\n",
        "        \n",
        "  \n",
        "    \n",
        "\n",
        "class SGNetLocal(nn.Module):  \n",
        "    \n",
        "    def __init__(self,pp):\n",
        "        super(SGNetLocal, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        #\n",
        "        self.pp = pp\n",
        "        self.a = self.pp.alpha\n",
        "        # \n",
        "        self.process = SGNetProcess(self.pp)\n",
        "        #\n",
        "        self.outlayer = OutputLayer2d(self.pp,self.process.array_channels,32,4)\n",
        "        \n",
        "\n",
        "    def forward(self,Data):\n",
        "        #\n",
        "        # see the PrepareInputLayer() class above for the input template assertions on Data\n",
        "        #\n",
        "        yProcessed = self.process(Data)\n",
        "        #\n",
        "        yScore = self.outlayer(yProcessed)\n",
        "        #\n",
        "        #############################\n",
        "        # output template assertions:\n",
        "        length = Data['length']\n",
        "        a = self.a\n",
        "        assert yScore.dtype == torch.float\n",
        "        assert yScore.size() == torch.Size([length,a,a])\n",
        "        #\n",
        "        #############################\n",
        "        #\n",
        "        return yScore\n",
        "    \n",
        "######################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "###################################################  \n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "class SgModel :\n",
        "    def __init__(self,pp):\n",
        "        self.pp = pp\n",
        "        #\n",
        "        self.network = SGNetGlobal(self.pp).to(Dvc)\n",
        "        #\n",
        "        self.network2 = SGNetLocal(self.pp).to(Dvc)\n",
        "        #\n",
        "        self.average_local_loss = itf(1.0)\n",
        "        #\n",
        "        #print(self.network)\n",
        "        print(\"set up model network and network2\")\n",
        "        #\n",
        "        self.benchmark = False\n",
        "        #\n",
        "        self.learning_rate = 0.002  # was 0.002, then 0.003, ...\n",
        "        self.momentum = 0.95\n",
        "        #self.weight_decay = 0.0001\n",
        "        #\n",
        "        #self.optimizer = optim.SGD(self.network.parameters(), lr=self.learning_rate, momentum = self.momentum, weight_decay = self.weight_decay )\n",
        "        #self.optimizer2 = optim.SGD(self.network2.parameters(), lr=self.learning_rate, momentum = self.momentum, weight_decay = self.weight_decay )\n",
        "        self.optimizer = optim.SGD(self.network.parameters(), lr=self.learning_rate, momentum = self.momentum )\n",
        "        self.optimizer2 = optim.SGD(self.network2.parameters(), lr=self.learning_rate, momentum = self.momentum )\n",
        "        #\n",
        "        self.criterionCE = nn.CrossEntropyLoss()\n",
        "        self.criterionA = nn.L1Loss()\n",
        "        self.criterionB = nn.MSELoss()\n",
        "        #\n",
        "        self.network2_trainable = True\n",
        "        #\n",
        "        pp.global_params, pp.local_params = self.modelcount()\n",
        "        #\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        \n",
        "\n",
        "    def modelcount(self):\n",
        "        network_param = sum(p.numel() for p in self.network.parameters() if p.requires_grad)\n",
        "        print(\"network parameters\",itp(network_param))\n",
        "        network2_param = sum(p.numel() for p in self.network2.parameters() if p.requires_grad)\n",
        "        print(\"network2 parameters\",itp(network2_param))\n",
        "        return network_param, network2_param\n",
        "\n",
        "    def tweak_network(self,N,density,epsilon):\n",
        "        for p in N.parameters():\n",
        "            if p.requires_grad:\n",
        "                tirage_density = torch.rand(p.size(),device=Dvc)\n",
        "                modif = torch.rand(p.size(),device=Dvc) \n",
        "                modif *= (tirage_density < density).to(torch.float) * epsilon\n",
        "                factor = modif + 1.0\n",
        "                with torch.no_grad():\n",
        "                    p *= factor\n",
        "        return\n",
        "\n",
        "\n",
        "        \n",
        "    \n",
        "        \n",
        "    def save_model(self,filename):  # tries to save the two model state dicts and optimizer state dicts\n",
        "        # I haven't tried these but they should probably mostly work and are included for reference\n",
        "        torch.save({\n",
        "                'network_state_dict': self.network.state_dict(),\n",
        "                'network2_state_dict': self.network2.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'optimizer2_state_dict': self.optimizer2.state_dict(),\n",
        "                }, filename)\n",
        "        print(\"saved to\",filename)\n",
        "        return\n",
        "\n",
        "    def load_model(self,filename):  # tries to load from the file\n",
        "        #\n",
        "        loadedmodels = torch.load(filename)\n",
        "        network_state_dict = loadedmodels['network_state_dict']\n",
        "        network2_state_dict = loadedmodels['network2_state_dict']\n",
        "        optimizer_state_dict = loadedmodels['optimizer_state_dict']\n",
        "        optimizer2_state_dict = loadedmodels['optimizer2_state_dict']\n",
        "        #\n",
        "        self.network.load_state_dict(network_state_dict)\n",
        "        self.network2.load_state_dict(network2_state_dict)\n",
        "        self.optimizer.load_state_dict(optimizer_state_dict)\n",
        "        self.optimizer2.load_state_dict(optimizer2_state_dict)\n",
        "        #\n",
        "        self.network.train()\n",
        "        self.network2.train()\n",
        "        print(\"loaded from\",filename)\n",
        "        return \n",
        "\n",
        "\n",
        "class ProtoModel :\n",
        "    def __init__(self,pp,style):\n",
        "        self.pp = pp\n",
        "        self.style = style\n",
        "        self.a = pp.alpha\n",
        "        self.a3z = self.a*self.a*self.a + 1\n",
        "        self.b = pp.beta\n",
        "        self.bz = self.b + 1\n",
        "        #\n",
        "        self.random_order = torch.randperm(self.a*self.a)\n",
        "        self.spiral, self.spiral_mix = self.makespiral()\n",
        "        self.rays = self.makerays()\n",
        "        #\n",
        "        self.network = SGNetGlobal(self.pp).to(Dvc)\n",
        "        # there is no network2 \n",
        "        self.network2_trainable = False\n",
        "        #\n",
        "        #print(self.network)\n",
        "        print(\"set up the proto-model network---network2 is not trainable, this is for the benchmark\")\n",
        "        #\n",
        "        self.benchmark = True\n",
        "        #\n",
        "        self.learning_rate = 0.002\n",
        "        #\n",
        "        self.optimizer = optim.SGD(self.network.parameters(), lr=self.learning_rate, momentum=0.9 )\n",
        "        #\n",
        "        self.criterionA = nn.L1Loss()\n",
        "        self.criterionB = nn.MSELoss()\n",
        "        #\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def makespiral(self):\n",
        "        spiral_order = torch.zeros((self.a,self.a),dtype = torch.int64,device=Dvc)\n",
        "        spiral_order[0,0] = 0\n",
        "        count = 1\n",
        "        for x in range(1,self.a):\n",
        "            spiral_order[x,x] = count\n",
        "            count += 1\n",
        "            for y in range(x):\n",
        "                spiral_order[x,y] = count\n",
        "                count += 1\n",
        "                spiral_order[y,x] = count\n",
        "                count += 1 \n",
        "        spiral_orderf = spiral_order.to(torch.float) / itt(self.a*self.a).to(torch.float)\n",
        "        thresh = self.pp.spiral_mix_threshold\n",
        "        over_thresh = (spiral_order > thresh).view(self.a*self.a)\n",
        "        spiral_mix = spiral_order.clone().view(self.a*self.a)\n",
        "        spiral_mix[over_thresh] = arangeic(self.a*self.a)[over_thresh] + thresh \n",
        "        spiral_mixf = spiral_mix.to(torch.float) / itt(self.a*self.a).to(torch.float)\n",
        "        return spiral_orderf, spiral_mixf\n",
        "\n",
        "    def makerays(self):\n",
        "        a = self.a\n",
        "        ray_order = torch.zeros((self.a,self.a),dtype = torch.int64,device=Dvc)\n",
        "        count = 0\n",
        "        for x in range(a):\n",
        "            ray_order[x,x] = count\n",
        "            count += 1\n",
        "        for x in range(a-1):\n",
        "            for y in range(x+1,a):\n",
        "                ray_order[x,y] = count\n",
        "                count += 1\n",
        "                ray_order[y,x] = count\n",
        "                count += 1\n",
        "        ray_orderf = ray_order.to(torch.float) / itt(a*a).to(torch.float)\n",
        "        return ray_orderf\n",
        "\n",
        "    def virtual_score(self,Data):\n",
        "        length = Data['length']\n",
        "        if self.style == 'random':\n",
        "            output = torch.rand((length,self.a,self.a),device=Dvc)\n",
        "        if self.style == 'random_order':\n",
        "            output = self.random_order.view(1,self.a,self.a).expand(length,self.a,self.a)\n",
        "        if self.style == 'spiral':\n",
        "            output = self.spiral.view(1,self.a,self.a).expand(length,self.a,self.a)\n",
        "        if self.style == 'spiral_mix':\n",
        "            output = self.spiral_mix.view(1,self.a,self.a).expand(length,self.a,self.a)\n",
        "        if self.style == 'rays':\n",
        "            output = self.rays.view(1,self.a,self.a).expand(length,self.a,self.a)\n",
        "        return output\n",
        "\n",
        "    def network2(self,Data):\n",
        "        #\n",
        "        a = self.a\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        prodsum = prod.to(torch.int64).sum(3)\n",
        "        availablexyv = (prodsum > 1).view(length*a*a)\n",
        "        #\n",
        "        vsv = self.virtual_score(Data).reshape(length*a*a)\n",
        "        #\n",
        "        vsv[~availablexyv] = 100.\n",
        "        #\n",
        "        values,xyvector = torch.min(vsv.view(length,a*a),1)\n",
        "        #\n",
        "        vs = vsv.view(length,a*a)\n",
        "        return vs\n",
        " \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyem2cS9SYVq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COEYlsafwHr1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVLWfNZSwHr3"
      },
      "source": [
        "class Relations1 :   # define a series of classes so as to make some \"chapters\"\n",
        "    def __init__(self,pp):\n",
        "        #\n",
        "        self.pp = pp\n",
        "        #\n",
        "        self.alpha = self.pp.alpha\n",
        "        self.alpha2 = self.alpha * self.alpha\n",
        "        self.alpha3 = self.alpha * self.alpha * self.alpha\n",
        "        self.alpha3z = self.alpha3 + 1\n",
        "        self.beta = self.pp.beta\n",
        "        self.betaz = self.beta +1\n",
        "        #\n",
        "        #self.model = self.mm\n",
        "        #\n",
        "        self.qvalue = self.pp.qvalue\n",
        "        #\n",
        "        self.ascore_max = self.pp.ascore_max\n",
        "        #\n",
        "        self.infosize =  self.pp.infosize\n",
        "        self.pastsize = self.pp.pastsize\n",
        "        self.futuresize = self.pp.futuresize\n",
        "        #\n",
        "        #\n",
        "        self.ar1 = arangeic(self.alpha).view(self.alpha,1).expand(self.alpha,self.alpha).clone()\n",
        "        self.ar2 = arangeic(self.alpha).view(1,self.alpha).expand(self.alpha,self.alpha).clone()\n",
        "        self.ida = (self.ar1 == self.ar2)\n",
        "        #\n",
        "        self.a3r1 = arangeic(self.alpha3).view(self.alpha3,1).expand(self.alpha3,self.alpha3).clone()\n",
        "        self.a3r2 = arangeic(self.alpha3).view(1,self.alpha3).expand(self.alpha3,self.alpha3).clone()\n",
        "        self.eqa3 = (self.a3r1 == self.a3r2)\n",
        "        #\n",
        "        self.a3zr1 = arangeic(self.alpha3z).view(self.alpha3z,1).expand(self.alpha3z,self.alpha3z).clone()\n",
        "        self.a3zr2 = arangeic(self.alpha3z).view(1,self.alpha3z).expand(self.alpha3z,self.alpha3z).clone()\n",
        "        self.eqa3z = (self.a3zr1 == self.a3zr2)\n",
        "        #\n",
        "        self.iblength = 2 ** (2*self.beta)\n",
        "        self.ibarray = torch.zeros((self.iblength,2*self.beta),dtype = torch.bool,device=Dvc)\n",
        "        for z in range(self.iblength):\n",
        "            self.ibarray[z] = zbinary(2*self.beta , z)\n",
        "        #\n",
        "        self.betazsubsets = self.makebetazsubsets()  # at location j,:,: it is for size (j+1)\n",
        "        self.quantities = self.betazsubsets[:,0:self.beta].to(torch.int).sum(1)  # the size of the subset as a function of z\n",
        "        #\n",
        "        \n",
        "        \n",
        "    ##### Relations1: general manipulation of data\n",
        "    \n",
        "    def printprod(self,Data,i):\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        a = self.alpha\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        assert i < length\n",
        "        #\n",
        "        printarray = torch.zeros((a,a),dtype = torch.int,device=Dvc)\n",
        "        printarray += 9 * (10 ** bz)\n",
        "        for p in range(bz):\n",
        "            printarray += (10 ** p) * (prod[i,:,:,p].to(torch.int))\n",
        "        print(nump(printarray))\n",
        "        return\n",
        "    \n",
        "    def printrandomprods(self,Data,number):\n",
        "        length = Data['length']\n",
        "        upper = number\n",
        "        if upper > length:\n",
        "            upper = length\n",
        "        indices = torch.randperm(length,device=Dvc)\n",
        "        for i in range(upper):\n",
        "            indexi = indices[i]\n",
        "            print(\"---------------------------------------\")\n",
        "            self.printprod(Data,indexi)\n",
        "        print(\"---------------------------------------\")\n",
        "        return\n",
        "        \n",
        "        \n",
        "    def makebetazsubsets(self):\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        bpower = 2**b\n",
        "        subsets = torch.ones((bpower,bz),dtype = torch.bool,device=Dvc)\n",
        "        for z in range(bpower):\n",
        "            subsets[z,0:b] = zbinary(b,z)\n",
        "        return subsets\n",
        "    \n",
        "    \n",
        "    \n",
        "    def nulldata(self):\n",
        "        length = torch.tensor(0)\n",
        "        Output = {\n",
        "            'length': length,\n",
        "            'depth': None,\n",
        "            'prod': None,\n",
        "            'left': None,\n",
        "            'right': None,\n",
        "            'ternary': None,\n",
        "            'info': None,\n",
        "        }\n",
        "        return Output\n",
        "    \n",
        "    def copydata(self,Data):\n",
        "        if Data['length'] == 0:\n",
        "            return self.nulldata()\n",
        "        Output = {}\n",
        "        Output['length'] = itt(Data['length']).clone().detach()\n",
        "        for ky in Data.keys():\n",
        "            if ky != 'length':\n",
        "                Output[ky] = (Data[ky]).clone().detach()\n",
        "        return Output\n",
        "\n",
        "\n",
        "    def duplicatedata(self,Data):  # like copydata but it puts the same objects in place rather than clones\n",
        "        if Data['length'] == 0:\n",
        "            return self.nulldata()\n",
        "        Output = {}\n",
        "        Output['length'] = itt(Data['length']).clone().detach()\n",
        "        for ky in Data.keys():\n",
        "            if ky != 'length':\n",
        "                Output[ky] = Data[ky]\n",
        "        return Output\n",
        "\n",
        "    \n",
        "    def deletedata(self,Data):\n",
        "        #del Data['length']  # better avoid doing that\n",
        "        datakeyslist = list(Data.keys())\n",
        "        for ky in datakeyslist:\n",
        "            if ky != 'length':\n",
        "                del Data[ky]\n",
        "        del Data\n",
        "        return\n",
        "\n",
        "\n",
        "        \n",
        "    def appenddata(self,Data1,Data2): # appends Data2 to Data1 and outputs the result\n",
        "        # there is a case where Data1 == None then we just output Data2\n",
        "        assert set(Data1.keys()) == set(Data2.keys())\n",
        "        if Data1['length'] == 0:\n",
        "            return self.copydata(Data2)\n",
        "        if Data2['length'] == 0:\n",
        "            return self.copydata(Data1)\n",
        "        Output = {}\n",
        "        Output['length'] = Data1['length'] + Data2['length']\n",
        "        #\n",
        "        for ky in Data1.keys():\n",
        "            if ky != 'length':\n",
        "                Output[ky] = torch.cat((Data1[ky],Data2[ky]),0)\n",
        "        return Output\n",
        "\n",
        "\n",
        "    def indexselectdata(self,Data,indices):\n",
        "        #\n",
        "        if len(indices) == 0:\n",
        "            return self.nulldata()\n",
        "        #\n",
        "        Output = {}\n",
        "        #\n",
        "        Output['length'] = len(indices)\n",
        "        #\n",
        "        for ky in Data.keys():\n",
        "            if ky != 'length':\n",
        "                Output[ky] = (Data[ky])[indices].clone().detach()\n",
        "        #\n",
        "        return Output\n",
        "\n",
        "\n",
        "    def detectsubdata(self,Data,detection):\n",
        "        #\n",
        "        assert len(detection) == Data['length']\n",
        "        #\n",
        "        sublength = detection.to(torch.int).sum(0)\n",
        "        if sublength == 0:\n",
        "            return self.nulldata()\n",
        "        #\n",
        "        Output = {}\n",
        "        #\n",
        "        Output['length'] = sublength\n",
        "        #\n",
        "        for ky in Data.keys():\n",
        "            if ky != 'length':\n",
        "                Output[ky] = (Data[ky])[detection].detach()\n",
        "        #\n",
        "        return Output\n",
        "        \n",
        "    def insertdata(self,Data,detection,SubData): \n",
        "        #\n",
        "        assert set(Data.keys()) == set(SubData.keys())\n",
        "        #\n",
        "        sublength = SubData['length']\n",
        "        assert detection.to(torch.int).sum(0) == sublength\n",
        "        #\n",
        "        if sublength == 0:\n",
        "            return Data\n",
        "        #\n",
        "        Output = {}\n",
        "        Output['length'] = itt(Data['length'])\n",
        "        #\n",
        "        for ky in Data.keys():\n",
        "            if ky != 'length':\n",
        "                outputitem = Data[ky].clone().detach()\n",
        "                outputitem[detection] = SubData[ky]\n",
        "                Output[ky] = outputitem\n",
        "        #\n",
        "        return Output\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    #########################\n",
        "        \n",
        "        \n",
        "        \n",
        "    def filterpossible(self,Data):  # here we just filter out the impossible cases\n",
        "        # return detection of the possible ones that remain\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        prodstats = prod.to(torch.int).sum(3)\n",
        "        impossible = ((prodstats == 0).any(2)).any(1)\n",
        "        #\n",
        "        detection = ~impossible\n",
        "        return detection\n",
        "\n",
        "        \n",
        "    \n",
        "    def knowledge(self,Data):  # now it increases as we refine\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']  # mask of shape a.a.bz with boolean values\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        if length == 0:\n",
        "            zerokn = torch.zeros((1),dtype = torch.int,device=Dvc)\n",
        "            return zerokn\n",
        "        #\n",
        "        output = torch.zeros((length),dtype = torch.int64,device=Dvc)\n",
        "        output -= prod.to(torch.int64).view(length,a*a*bz).sum(1)\n",
        "        output -= left.to(torch.int64).view(length,a*bz*2).sum(1)\n",
        "        output -= right.to(torch.int64).view(length,bz*a*2).sum(1)\n",
        "        output -= ternary.to(torch.int64).view(length,a*a*a*2).sum(1)\n",
        "        return output\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    def availablexy(self,length,prod):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        prodsum = prod.to(torch.int64).sum(3)\n",
        "        possible = ((prodsum > 0).all(2)).all(1)\n",
        "        possiblexy = possible.view(length,1).expand(length,a2)\n",
        "        #\n",
        "        optionalxy = (prodsum > 1).view(length,a2)\n",
        "        #\n",
        "        available_xy = possiblexy & optionalxy\n",
        "        return available_xy\n",
        "    \n",
        "    def availablexyp(self,length,prod):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        prodsum = prod.to(torch.int64).sum(3)\n",
        "        possible = ((prodsum > 0).all(2)).all(1)\n",
        "        possiblexyp = possible.view(length,1,1,1).expand(length,a,a,bz)\n",
        "        #\n",
        "        optionalxyp = (prodsum > 1).view(length,a,a,1).expand(length,a,a,bz)\n",
        "        #\n",
        "        available_xyp = prod & possiblexyp & optionalxyp\n",
        "        return available_xyp\n",
        "    \n",
        "    ###\n",
        "\n",
        "    def upsplitting(self,Data,ivector,xvector,yvector,pvector):  # setting x.y = p\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        if len(ivector) == 0:\n",
        "            return self.rr1.nulldata()\n",
        "        #\n",
        "        UpData = self.indexselectdata(Data,ivector)\n",
        "        length = UpData['length']\n",
        "        prod = UpData['prod']\n",
        "        #\n",
        "        xrangevx = arangeic(a).view(1,a,1,1).expand(length,a,a,bz)\n",
        "        yrangevx = arangeic(a).view(1,1,a,1).expand(length,a,a,bz)\n",
        "        prangevx = arangeic(bz).view(1,1,1,bz).expand(length,a,a,bz)\n",
        "        #\n",
        "        xvectorvx = xvector.view(length,1,1,1).expand(length,a,a,bz)\n",
        "        yvectorvx = yvector.view(length,1,1,1).expand(length,a,a,bz)\n",
        "        pvectorvx = pvector.view(length,1,1,1).expand(length,a,a,bz)\n",
        "        #\n",
        "        newprod = prod & ( (xrangevx != xvectorvx) | (yrangevx != yvectorvx) | (prangevx == pvectorvx))\n",
        "        #\n",
        "        UpData['prod'] = newprod\n",
        "        UpData['depth'] += 1\n",
        "        #\n",
        "        return UpData\n",
        "    \n",
        "    \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqNckdhwwHr6"
      },
      "source": [
        "class Relations2 :   # define a series of classes so as to make some \"chapters\"\n",
        "    def __init__(self,pp):\n",
        "        #\n",
        "        self.pp = pp\n",
        "        #\n",
        "        self.rr1 = Relations1(pp)\n",
        "        #\n",
        "        self.alpha = self.pp.alpha\n",
        "        self.alpha2 = self.alpha * self.alpha\n",
        "        self.alpha2z = self.alpha2 + 1\n",
        "        self.alpha3 = self.alpha * self.alpha * self.alpha\n",
        "        self.alpha3z = self.alpha3 + 1\n",
        "        self.beta = self.pp.beta\n",
        "        self.betaz = self.beta +1\n",
        "        #\n",
        "        #\n",
        "        self.halfones_count = 0\n",
        "        self.impossible_basic_count = 0\n",
        "        #     \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    def filterpossible(self,Data):  # here we just filter out the impossible cases\n",
        "        # return detection of the possible ones that remain\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        prodstats = prod.to(torch.int).sum(3)\n",
        "        impossible = ((prodstats == 0).any(2)).any(1)\n",
        "        #\n",
        "        detection = ~impossible\n",
        "        return detection\n",
        "    \n",
        "    \n",
        "        \n",
        "    #######################################\n",
        "    ##### new steps with prod, left, right, ternary\n",
        "    \n",
        "    def modifyternaryStep(self,Data):\n",
        "        # \n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        #\n",
        "        ivx = arangeic(length).view(length,1,1,1,1).expand(length,a,a,a,bz)\n",
        "        xvx = arangeic(a).view(1,a,1,1,1).expand(length,a,a,a,bz)\n",
        "        yvx = arangeic(a).view(1,1,a,1,1).expand(length,a,a,a,bz)\n",
        "        zvx = arangeic(a).view(1,1,1,a,1).expand(length,a,a,a,bz)\n",
        "        pvx = arangeic(bz).view(1,1,1,1,bz).expand(length,a,a,a,bz)\n",
        "        #\n",
        "        nter0_left = (prod[ivx,yvx,zvx,pvx] & left[ivx,xvx,pvx,0]).any(4)\n",
        "        nter1_left = (prod[ivx,yvx,zvx,pvx] & left[ivx,xvx,pvx,1]).any(4)\n",
        "        #\n",
        "        nter0_right = (prod[ivx,xvx,yvx,pvx] & right[ivx,pvx,zvx,0]).any(4)\n",
        "        nter1_right = (prod[ivx,xvx,yvx,pvx] & right[ivx,pvx,zvx,1]).any(4)\n",
        "        #\n",
        "        nter0v = (nter0_left & nter0_right)\n",
        "        nter1v = (nter1_left & nter1_right)\n",
        "        #\n",
        "        newternary = ternary.clone()\n",
        "        newternary[:,:,:,:,0] = ternary[:,:,:,:,0] & nter0v\n",
        "        newternary[:,:,:,:,1] = ternary[:,:,:,:,1] & nter1v\n",
        "        #\n",
        "        NewData = self.rr1.copydata(Data)\n",
        "        NewData['ternary'] = newternary.detach()\n",
        "        #\n",
        "        return NewData\n",
        "    \n",
        "    def modifyleftrightStep(self,Data):\n",
        "        # \n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        prodstats = prod.to(torch.int64).sum(3)\n",
        "        unique = (prodstats == 1)\n",
        "        #\n",
        "        ivx = arangeic(length).view(length,1,1,1,1).expand(length,a,a,a,bz)\n",
        "        xvx = arangeic(a).view(1,a,1,1,1).expand(length,a,a,a,bz)\n",
        "        yvx = arangeic(a).view(1,1,a,1,1).expand(length,a,a,a,bz)\n",
        "        zvx = arangeic(a).view(1,1,1,a,1).expand(length,a,a,a,bz)\n",
        "        pvx = arangeic(bz).view(1,1,1,1,bz).expand(length,a,a,a,bz)\n",
        "        #\n",
        "        nleft0 = (( (~prod[ivx,yvx,zvx,pvx]) |  (~unique[ivx,yvx,zvx]) | ternary[ivx,xvx,yvx,zvx,0]).all(3)).all(2)\n",
        "        nleft1 = (( (~prod[ivx,yvx,zvx,pvx]) |  (~unique[ivx,yvx,zvx]) | ternary[ivx,xvx,yvx,zvx,1]).all(3)).all(2)\n",
        "        #\n",
        "        nright0 = (( (~prod[ivx,xvx,yvx,pvx]) |  (~unique[ivx,xvx,yvx]) | ternary[ivx,xvx,yvx,zvx,0]).all(2)).all(1)\n",
        "        nright1 = (( (~prod[ivx,xvx,yvx,pvx]) |  (~unique[ivx,xvx,yvx]) | ternary[ivx,xvx,yvx,zvx,1]).all(2)).all(1)\n",
        "        #\n",
        "        newleft = left.clone()\n",
        "        newright = right.clone()\n",
        "        #\n",
        "        newleft[:,:,:,0] = left[:,:,:,0] & nleft0\n",
        "        newleft[:,:,:,1] = left[:,:,:,1] & nleft1\n",
        "        newright[:,:,:,0] = right[:,:,:,0] & (nright0.permute(0,2,1))\n",
        "        newright[:,:,:,1] = right[:,:,:,1] & (nright1.permute(0,2,1))\n",
        "        #\n",
        "        NewData = self.rr1.copydata(Data)\n",
        "        NewData['left'] = newleft.detach()\n",
        "        NewData['right'] = newright.detach()\n",
        "        #\n",
        "        return NewData\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    def modifyprodStep(self,Data):\n",
        "        # \n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        lvx = arangeic(length).view(length,1,1,1,1).expand(length,a,a,a,bz)\n",
        "        xvx = arangeic(a).view(1,a,1,1,1).expand(length,a,a,a,bz)\n",
        "        yvx = arangeic(a).view(1,1,a,1,1).expand(length,a,a,a,bz)\n",
        "        zvx = arangeic(a).view(1,1,1,a,1).expand(length,a,a,a,bz)\n",
        "        pvx = arangeic(bz).view(1,1,1,1,bz).expand(length,a,a,a,bz)\n",
        "        #\n",
        "        leftbin01 = (left[lvx,xvx,pvx,0] | ternary[lvx,xvx,yvx,zvx,1]) \n",
        "        leftbin10 = (left[lvx,xvx,pvx,1] | ternary[lvx,xvx,yvx,zvx,0])\n",
        "        #\n",
        "        rightbin01 = (right[lvx,pvx,zvx,0] | ternary[lvx,xvx,yvx,zvx,1])\n",
        "        rightbin10 = (right[lvx,pvx,zvx,1] | ternary[lvx,xvx,yvx,zvx,0])\n",
        "        #\n",
        "        newprod = prod.clone()\n",
        "        newprod = newprod & ( (leftbin01 & leftbin10).all(1) )\n",
        "        newprod = newprod & ( (rightbin01 & rightbin10).all(3) )\n",
        "        #\n",
        "        NewData = self.rr1.copydata(Data)\n",
        "        NewData['prod'] = newprod.detach()\n",
        "        #\n",
        "        return NewData\n",
        "        \n",
        "        \n",
        "    def process(self,Data):\n",
        "        length = Data['length']\n",
        "        if length == 0:\n",
        "            return Data\n",
        "        #\n",
        "        #\n",
        "        OutputData = self.rr1.copydata(Data)\n",
        "        nprod = Data['prod']\n",
        "        nprodstats = nprod.to(torch.int64).sum(3)\n",
        "        subset = ((nprodstats > 0).all(2)).all(1)\n",
        "        NextData = self.rr1.detectsubdata(Data,subset)\n",
        "        if subset.to(torch.int).sum(0) == 0:\n",
        "            return OutputData\n",
        "        for i in range(1000):\n",
        "            priorknowledge = self.rr1.knowledge(NextData)\n",
        "            #\n",
        "            #\n",
        "            NextData = self.modifyternaryStep(NextData)\n",
        "            #\n",
        "            NextData = self.modifyleftrightStep(NextData)\n",
        "            #\n",
        "            NextData = self.modifyprodStep(NextData)\n",
        "            #\n",
        "            nextknowledge = self.rr1.knowledge(NextData)\n",
        "            nextdonedetect = (priorknowledge >= nextknowledge)\n",
        "            subset_nextdone = composedetections(length,subset,nextdonedetect)\n",
        "            NextDoneData = self.rr1.detectsubdata(NextData,nextdonedetect)\n",
        "            OutputData = self.rr1.insertdata(OutputData,subset_nextdone,NextDoneData)\n",
        "            #\n",
        "            subset = subset & (~subset_nextdone)\n",
        "            if subset.to(torch.int).sum(0) == 0:\n",
        "                break\n",
        "            NextData = self.rr1.detectsubdata(NextData, ~nextdonedetect )\n",
        "        return OutputData\n",
        "        \n",
        "    \n",
        "    def impossibleFilter(self,Data):\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        prodstats = prod.to(torch.int64).sum(3)\n",
        "        possible = ((prodstats > 0).all(2)).all(1)\n",
        "        #\n",
        "        leftv = left.view(length,a*bz,2)\n",
        "        rightv = right.view(length,bz*a,2)\n",
        "        ternaryv = ternary.view(length,a3,2)\n",
        "        left_possible = (leftv.any(2)).all(1)\n",
        "        right_possible = (rightv.any(2)).all(1)\n",
        "        ternary_possible = (ternaryv.any(2)).all(1)\n",
        "        #\n",
        "        detection = (~possible) | (~left_possible) | (~right_possible) | (~ternary_possible)\n",
        "        return detection\n",
        "    \n",
        "    def profileFilter(self,Data):\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        left_def = ( (left.to(torch.int64).sum(3)) == 1 )\n",
        "        right_def = ( (right.to(torch.int64).sum(3)) == 1 )\n",
        "        profile_def = ( left_def.all(1) ) &  ( right_def.all(2) )\n",
        "        left_pro = ( left[:,:,:,0] ).permute(0,2,1)\n",
        "        right_pro =  right[:,:,:,0] \n",
        "        profile = torch.cat((left_pro,right_pro),2)\n",
        "        profile_vx1 = profile.view(length,bz,1,2*a).expand(length,bz,bz,2*a)\n",
        "        profile_vx2 = profile.view(length,1,bz,2*a).expand(length,bz,bz,2*a)\n",
        "        same_profile = (profile_vx1 == profile_vx2).all(3)\n",
        "        #\n",
        "        profile_def1 = profile_def.view(length,bz,1).expand(length,bz,bz)\n",
        "        profile_def2 = profile_def.view(length,1,bz).expand(length,bz,bz)\n",
        "        #\n",
        "        same_profile_def = (profile_def1 & profile_def2 & same_profile)\n",
        "        prange1 = arangeic(bz).view(1,bz,1).expand(length,bz,bz)\n",
        "        prange2 = arangeic(bz).view(1,1,bz).expand(length,bz,bz)\n",
        "        #\n",
        "        detection = (( (prange1 != prange2) & same_profile_def ).any(2)).any(1)\n",
        "        #\n",
        "        return detection\n",
        "        \n",
        "    def doneFilter(self,Data):\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        prodstats = prod.to(torch.int64).sum(3)\n",
        "        #\n",
        "        binsum = ternary.view(length,a3,2).to(torch.int64).sum(2)\n",
        "        ternary_all = (binsum == 1).all(1)\n",
        "        #\n",
        "        #detection = ( ((prodstats == 1).all(2)).all(1) ) | ternary_all\n",
        "        detection = ( ((prodstats == 1).all(2)).all(1) ) \n",
        "        #\n",
        "        return detection\n",
        "\n",
        "    def halfonesFilter(self,Data):  # only look at cases where the number of \n",
        "        # ones on the left is >= the number on the right\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        #\n",
        "        prodstats = prod.to(torch.int64).sum(3)\n",
        "        leftstats = left.to(torch.int64).sum(3)\n",
        "        rightstats = right.to(torch.int64).sum(3)\n",
        "        #\n",
        "        assert (((leftstats <= 1).all(2)).all(1)).all(0)\n",
        "        #\n",
        "        leftones = (left[:,:,:,1].to(torch.int64).sum(2)).sum(1)\n",
        "        right_isone = (rightstats == 1) & right[:,:,:,1]\n",
        "        rightones = (right_isone.to(torch.int64).sum(2)).sum(1)\n",
        "        #\n",
        "        #\n",
        "        detection = (rightones > leftones)\n",
        "        #\n",
        "        return detection\n",
        "    \n",
        "    def filterdata(self,Data):  #\n",
        "        #\n",
        "        #\n",
        "        impossibledetect = self.impossibleFilter(Data)\n",
        "        profiledetect = self.profileFilter(Data)\n",
        "        #\n",
        "        if self.pp.profile_filter_on:\n",
        "            impossibledetect = impossibledetect | profiledetect \n",
        "        #\n",
        "        self.impossible_basic_count += impossibledetect.to(torch.int64).sum(0)\n",
        "        # experimental:\n",
        "        if self.pp.halfones_filter_on:\n",
        "            halfonesdetect = self.halfonesFilter(Data)\n",
        "            self.halfones_count += (halfonesdetect & (~impossibledetect)).to(torch.int64).sum(0)\n",
        "            #\n",
        "            impossibledetect = impossibledetect | halfonesdetect\n",
        "        #\n",
        "        donedetect = self.doneFilter(Data)\n",
        "        donedetect = donedetect & (~impossibledetect)\n",
        "        #\n",
        "        activedetect = (~impossibledetect) & ~donedetect\n",
        "        #\n",
        "        return activedetect, donedetect, impossibledetect\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssglc7SaAE0Y"
      },
      "source": [
        "class Relations3 :   \n",
        "    def __init__(self,pp):\n",
        "        #\n",
        "        self.pp = pp\n",
        "        #\n",
        "        self.rr2 = Relations2(pp)\n",
        "        self.rr1 = self.rr2.rr1\n",
        "        #\n",
        "        self.alpha = self.pp.alpha\n",
        "        self.alpha2 = self.alpha * self.alpha\n",
        "        self.alpha2z = self.alpha2 + 1\n",
        "        self.alpha3 = self.alpha * self.alpha * self.alpha\n",
        "        self.alpha3z = self.alpha3 + 1\n",
        "        self.beta = self.pp.beta\n",
        "        self.betaz = self.beta +1\n",
        "        #\n",
        "            \n",
        "        \n",
        "    def printmultiplicities(self,Data):\n",
        "        #\n",
        "        dlength = self.alpha2 + 1\n",
        "        #\n",
        "        length = Data['length']\n",
        "        depth = Data['depth']\n",
        "        if length == 0:\n",
        "            multiplicities = torch.zeros((dlength),dtype = torch.int,device=Dvc)\n",
        "            print(nump(multiplicities))\n",
        "            return\n",
        "        #\n",
        "        dmax,dindices = torch.max(depth,0)\n",
        "        if dmax +2 > dlength:\n",
        "            dlength = dmax +2\n",
        "        if dlength > 50:\n",
        "            dlength = 50\n",
        "        #\n",
        "        depthvx = depth.view(length,1).expand(length,dlength)\n",
        "        drangevx = arangeic(dlength).view(1,dlength).expand(length,dlength)\n",
        "        rectangle = (depthvx == drangevx)\n",
        "        multiplicities = rectangle.sum(0)\n",
        "        #\n",
        "        #print(\"dlength\",itp(dlength))\n",
        "        print(\"multiplicities in active pool by depth:\")\n",
        "        print(nump(multiplicities))\n",
        "        return\n",
        "    \n",
        "    def selectchunk(self,Data):  \n",
        "        #  \n",
        "        length = Data['length']\n",
        "        depth = Data['depth']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        #\n",
        "        assert length > 0\n",
        "        #\n",
        "        prodstats = prod.to(torch.int).sum(3)\n",
        "        assert (((prodstats >0).all(2)).all(1)).all(0)\n",
        "        optional = (prodstats > 1)\n",
        "        assert ((optional.any(2)).any(1)).all(0)\n",
        "        #\n",
        "        values,indices = torch.sort(depth,0,descending = True)\n",
        "        upper = length\n",
        "        if upper > self.pp.chunksize:\n",
        "            upper = self.pp.chunksize\n",
        "        indices_upper = indices[0:upper]\n",
        "        #\n",
        "        cdetection = torch.zeros((length),dtype = torch.bool,device=Dvc)\n",
        "        cdetection[indices_upper] = True\n",
        "        #\n",
        "        ChunkData = self.rr1.detectsubdata(Data,cdetection)\n",
        "        #\n",
        "        if self.pp.verbose:\n",
        "            self.printmultiplicities(Data)\n",
        "        #\n",
        "        return ChunkData, cdetection\n",
        "    \n",
        "    def network_vcuts(self,M,Data,randomize):\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        depth = Data['depth']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        assert length > 0\n",
        "        #\n",
        "        #\n",
        "        prodstats = prod.to(torch.int64).sum(3)\n",
        "        #\n",
        "        assert (((prodstats > 0).all(2)).all(1)).all(0)\n",
        "        #\n",
        "        #\n",
        "        availablexyr = self.rr1.availablexy(length,prod).reshape(length*a2)\n",
        "        #\n",
        "        networkscorer = M.network2(Data).detach().reshape(length*a2)\n",
        "        #\n",
        "        if randomize:\n",
        "            #\n",
        "            epsilon_tirage = torch.rand(length*a2,device=Dvc)\n",
        "            alll = torch.clamp(M.average_local_loss,0.,0.5)\n",
        "            epsilon_factor = M. average_local_loss * self.pp.perturbation_factor * (epsilon_tirage ** 4)\n",
        "            epsilon = torch.rand(length*a2,device=Dvc)\n",
        "            networkscorer += (epsilon_factor * epsilon)\n",
        "            #\n",
        "            phase = Data['info'][:,self.pp.phase]\n",
        "            tirage = torch.rand(length,device=Dvc)\n",
        "            detection = (phase == 1) | ( (tirage < 0.1) & (phase == 2) )\n",
        "            detectionvxr = detection.view(length,1).expand(length,a2).reshape(length*a2)\n",
        "            randomscore = torch.rand(length*a2,device=Dvc)\n",
        "            networkscorer[detectionvxr] = randomscore[detectionvxr]\n",
        "            #\n",
        "        #\n",
        "        networkscorer = torch.clamp(networkscorer, -1., 10.)\n",
        "        networkscorer[~availablexyr] = 20. \n",
        "        networkscore = networkscorer.view(length,a2)\n",
        "        #\n",
        "        values,xyvector = torch.min(networkscore,1)\n",
        "        #\n",
        "        return xyvector\n",
        "    \n",
        "    \n",
        "\n",
        "    def addvalencies(self,availablexyp,xyvector):  # adds into the HST file the valencies of these vertices\n",
        "        # also adds the passive count (just the length of the xyvector)\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a2z = self.alpha2 +1\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = len(xyvector)\n",
        "        HST.current_proof_passive_count += length\n",
        "        #\n",
        "        availablexypv = availablexyp.view(length,a2,bz)\n",
        "        lrange = arangeic(length)\n",
        "        available_cuts = availablexypv[lrange,xyvector]\n",
        "        valency = available_cuts.to(torch.int64).sum(1)\n",
        "        for v in range(bz + 1):\n",
        "            HST.current_proof_valency_frequency[v] += (valency == v).to(torch.int64).sum(0)\n",
        "        return\n",
        "    \n",
        "    def managesplit(self,M,DataToSplit,randomize):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a2z = self.alpha2 +1\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        #\n",
        "        length = DataToSplit['length']\n",
        "        prod = DataToSplit['prod']\n",
        "        #\n",
        "        availablexyp = self.rr1.availablexyp(length,prod).view(length,a2,bz)\n",
        "        #\n",
        "        xyvector = self.network_vcuts(M,DataToSplit,randomize)\n",
        "        #\n",
        "        self.addvalencies(availablexyp,xyvector)\n",
        "        #\n",
        "        lrangevxr = arangeic(length).view(length,1).expand(length,bz).reshape(length*bz)\n",
        "        xyvectorvxr = xyvector.view(length,1).expand(length,bz).reshape(length*bz)\n",
        "        bzrangevxr = arangeic(bz).view(1,bz).expand(length,bz).reshape(length*bz)\n",
        "        #\n",
        "        verticaldetect = availablexyp[lrangevxr,xyvectorvxr,bzrangevxr]\n",
        "        # \n",
        "        #\n",
        "        ivector_vert = lrangevxr[verticaldetect]\n",
        "        xyvector_vert = xyvectorvxr[verticaldetect]\n",
        "        pvector_vert = bzrangevxr[verticaldetect]\n",
        "        #\n",
        "        prx = arangeic(a).view(a,1).expand(a,a).reshape(a2)\n",
        "        pry = arangeic(a).view(1,a).expand(a,a).reshape(a2)\n",
        "        #\n",
        "        xvector_vert = prx[xyvector_vert]\n",
        "        yvector_vert = pry[xyvector_vert]\n",
        "        #\n",
        "        NewData = self.rr1.upsplitting(DataToSplit,ivector_vert,xvector_vert,yvector_vert,pvector_vert)\n",
        "        #\n",
        "        #\n",
        "        ndlength = NewData['length']\n",
        "        #\n",
        "        #\n",
        "        AssocNewData = self.rr1.nulldata()\n",
        "        detection = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        newactive = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        newdone = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        newimpossible = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        lower = 0\n",
        "        for i in range(ndlength):\n",
        "            assert lower < ndlength\n",
        "            upper = lower + 1000\n",
        "            if upper > ndlength:\n",
        "                upper = ndlength\n",
        "            detection[:] = False\n",
        "            detection[lower:upper] = True\n",
        "            NewDataSlice = self.rr1.detectsubdata(NewData,detection)\n",
        "            AssocNewDataSlice = self.rr2.process(NewDataSlice)\n",
        "            AssocNewData = self.rr1.appenddata(AssocNewData,AssocNewDataSlice)\n",
        "            newactive_s,newdone_s,newimpossible_s = self.rr2.filterdata(AssocNewDataSlice)\n",
        "            newactive[lower:upper] = newactive_s\n",
        "            newdone[lower:upper] = newdone_s\n",
        "            newimpossible[lower:upper] = newimpossible_s\n",
        "            lower = upper\n",
        "            if lower >= ndlength:\n",
        "                break\n",
        "        #\n",
        "        NewActiveData = self.rr1.detectsubdata(AssocNewData,newactive)\n",
        "        #\n",
        "        NewDoneData = self.rr1.detectsubdata(AssocNewData,newdone)\n",
        "        #\n",
        "        if NewActiveData['length'] > 0:\n",
        "            phase1 = ( NewActiveData['info'][:,self.pp.phase] == 1)\n",
        "            phase2 = ( NewActiveData['info'][:,self.pp.phase] == 2)\n",
        "            tirage = torch.rand(NewActiveData['length'], device=Dvc)\n",
        "            phasechange = phase2 & (tirage < self.pp.splitting_probability)\n",
        "            newphase = NewActiveData['info'][:,self.pp.phase].clone()\n",
        "            newphase[phase1] = 0\n",
        "            newphase[phasechange] = 1\n",
        "            NewActiveData['info'][:,self.pp.phase] = newphase\n",
        "        #\n",
        "        HST.current_proof_impossible_count += newimpossible.to(torch.int64).sum(0)\n",
        "        HST.current_proof_done_count += newdone.to(torch.int64).sum(0)\n",
        "        #\n",
        "        if self.pp.verbose:\n",
        "            print(\" >>>\")\n",
        "            print(\"DataToSplit\",itp(DataToSplit['length']))\n",
        "            print(\"NewData\",itp(NewData['length']))\n",
        "            print(\"NewActiveData\",itp(NewActiveData['length']))\n",
        "            print(\"NewDoneData\",itp(NewDoneData['length']))\n",
        "            print(\"----------------------------------\")\n",
        "        #\n",
        "        return NewActiveData, NewDoneData\n",
        "        \n",
        "    \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIRQG_PpAE0Y"
      },
      "source": [
        "class Relations4 :   # define a series of classes so as to make some \"chapters\"\n",
        "    def __init__(self,pp):\n",
        "        #\n",
        "        self.pp = pp\n",
        "        #\n",
        "        self.rr3 = Relations3(pp)\n",
        "        self.rr2 = self.rr3.rr2\n",
        "        self.rr1 = self.rr3.rr1\n",
        "        #\n",
        "        self.alpha = self.pp.alpha\n",
        "        self.alpha2 = self.alpha * self.alpha\n",
        "        self.alpha3 = self.alpha * self.alpha * self.alpha\n",
        "        self.alpha3z = self.alpha3 + 1\n",
        "        self.beta = self.pp.beta\n",
        "        self.betaz = self.beta +1\n",
        "        #\n",
        "        self.prooflooplength = self.pp.prooflooplength\n",
        "        self.done_max = 1000  # should be self.pp.done_max\n",
        "        #\n",
        "        self.sleeptime = self.pp.sleeptime\n",
        "        self.periodicity = self.pp.periodicity\n",
        "        self.stopthreshold = self.pp.stopthreshold\n",
        "        #\n",
        "        self.SamplePool = self.rr1.nulldata()\n",
        "        self.DroppedSamplePool = self.rr1.nulldata()\n",
        "        # these are by convention active (not done or impossible)\n",
        "        #\n",
        "        self.donecount = itt(0)\n",
        "        self.ECN = 0.\n",
        "        #\n",
        "        self.proofnumber = 0\n",
        "        self.allnumbers = 0\n",
        "        self.proofinstance = 0\n",
        "        self.dropoutratio = 1.\n",
        "        \n",
        "    def resetsamples(self):\n",
        "        self.SamplePool = self.rr1.nulldata()\n",
        "        self.DroppedSamplePool = self.rr1.nulldata()\n",
        "        #\n",
        "        self.rr2.impossible_basic_count = 0\n",
        "        self.rr2.halfones_count = 0\n",
        "        self.dropoutratio = 1.\n",
        "        return\n",
        "  \n",
        "    def printexamples(self,Data):\n",
        "        # \n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        depth = Data['depth']\n",
        "        prod = Data['prod']\n",
        "        ternary = Data['ternary']\n",
        "        #\n",
        "        if length == 0:\n",
        "            print(\"length 0, no examples to print\")\n",
        "            return\n",
        "        #\n",
        "        bini = ternary.to(torch.int64)\n",
        "        ternary_print = bini[:,:,0] + 2*bini[:,:,1] - 1\n",
        "        #\n",
        "        permutation = torch.randperm((length),device=Dvc)\n",
        "        upper = 5\n",
        "        if upper > length:\n",
        "            upper = length\n",
        "        for i in range(upper):\n",
        "            indexi = permutation[i]\n",
        "            print(\"---------------------------------------\")\n",
        "            self.rr1.printprod(Data,indexi)\n",
        "            print(nump(quad[indexi]))\n",
        "            print(nump(ternary_print[indexi]))\n",
        "        print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "    def transitionactive(self,ActivePool,cdetection,NewActiveData):\n",
        "        #\n",
        "        ResidualActive = self.rr1.detectsubdata(ActivePool,~cdetection)\n",
        "        NextActivePool = self.rr1.appenddata(NewActiveData,ResidualActive)\n",
        "        #\n",
        "        NextActivePoolCopy = self.rr1.copydata(NextActivePool)\n",
        "        self.rr1.deletedata(NextActivePool)\n",
        "        #\n",
        "        return NextActivePoolCopy\n",
        "    \n",
        "    def transitiondone(self,C,DonePool,DoneData,aplength):\n",
        "        #\n",
        "        idl = DoneData['length']\n",
        "        #\n",
        "        self.donecount += idl\n",
        "        #\n",
        "        #\n",
        "        if self.pp.verbose:\n",
        "            print(\"new done count is\",itp(self.donecount))\n",
        "        #\n",
        "        #\n",
        "        #NewDonePool = self.rr1.nulldata()\n",
        "        NewDonePool = self.rr1.appenddata(DonePool,DoneData)\n",
        "        ndlength = NewDonePool['length']\n",
        "        if ndlength > self.done_max:\n",
        "            #print(\"new done pool of length\",itp(ndlength),\"so we send to classifier for processing\")\n",
        "            print(\"/\",end='')\n",
        "            DataToProcess = self.rr1.copydata(NewDonePool)\n",
        "            NewDonePool = self.rr1.nulldata()\n",
        "            C.process(DataToProcess)\n",
        "            #\n",
        "        return NewDonePool\n",
        "\n",
        "      \n",
        "    def transitionsamples(self,ActivePool,DroppedPool):\n",
        "        #\n",
        "        # transitioning samples\n",
        "        #\n",
        "        slength = self.SamplePool['length']\n",
        "        aplength = ActivePool['length']\n",
        "        if aplength == 0:\n",
        "            assert DroppedPool['length'] == 0\n",
        "            return\n",
        "        apdepth = ActivePool['depth'].to(torch.int64)\n",
        "        #\n",
        "        aprectangle = ActivePool['info'][:,self.pp.sampleinfolower:self.pp.sampleinfoupper].clone()\n",
        "        #\n",
        "        aprange = arangeic(aplength)\n",
        "        #\n",
        "        # now add the new sample locations to the rectangle\n",
        "        newsloc = arangeic(aplength) + slength\n",
        "        #\n",
        "        aprectangle[aprange,apdepth] = newsloc\n",
        "        #\n",
        "        # this should modify active pool outside the present function:\n",
        "        ActivePool['info'][:,self.pp.sampleinfolower:self.pp.sampleinfoupper] = aprectangle\n",
        "        # that isn't needed for dropped pool since it doesn't get refered back to later\n",
        "        #\n",
        "        self.SamplePool = self.rr1.appenddata(self.SamplePool, ActivePool)\n",
        "        self.DroppedSamplePool = self.rr1.appenddata(self.DroppedSamplePool, DroppedPool)\n",
        "        #\n",
        "        #print(\"sample pool has size\",itp(self.SamplePool['length']))\n",
        "        #Fws.trace(\"transition samples Active Pool\",ActivePool,5,0,0,0)\n",
        "        #Fws.trace(\"transition samples Sample Pool\",self.SamplePool,5,0,0,0)\n",
        "        #self.printsampleex()\n",
        "        return \n",
        "        \n",
        "    \n",
        "    def proofloop(self,Mstrat,Mlearn,C,Input,dropoutlimit):\n",
        "        #\n",
        "        self.resetsamples()\n",
        "        #\n",
        "        if dropoutlimit > 0:\n",
        "            randomize = True\n",
        "        else:\n",
        "            randomize = False\n",
        "        #\n",
        "        InitialActiveData = self.rr2.process(Input)\n",
        "        activedetect, donedetect, impossibledetect = self.rr2.filterdata(InitialActiveData)\n",
        "        implength = impossibledetect.to(torch.int).sum(0)\n",
        "        donelength = donedetect.to(torch.int).sum(0)\n",
        "        if self.pp.verbose:\n",
        "            print(\"initial filter finds\",itp(implength),\"impossibilities and\",itp(donelength),\"instances already done\")\n",
        "        ActivePool = self.rr1.detectsubdata(InitialActiveData,activedetect)\n",
        "        #\n",
        "        napcount = 0\n",
        "        #\n",
        "        DonePool = self.rr1.detectsubdata(InitialActiveData,donedetect)\n",
        "        self.donecount = itt(0)\n",
        "        if ActivePool['length'] == 0:\n",
        "            DonePool = self.transitiondone(DonePool,self.rr1.nulldata(),ActivePool['length'])\n",
        "        #\n",
        "        #\n",
        "        self.ECN = itt(ActivePool['length']).to(torch.float).clone()\n",
        "        EDN = self.ECN.clone()\n",
        "        if self.pp.verbose:\n",
        "            print(\"starting with ECN = EDN from initial active pool\",numpr(self.ECN,1))\n",
        "        #\n",
        "        if dropoutlimit > 0:\n",
        "            ActivePool,DroppedPool,newsum,droppedsum = self.dropoutdata(Mlearn,ActivePool,dropoutlimit)         \n",
        "            if self.pp.dropout_style == 'adaptive':\n",
        "                activelengthf = itt(ActivePool['length']).clone().to(torch.float)\n",
        "                self.ECN = (activelengthf + droppedsum)\n",
        "                EDN = 0.\n",
        "        #\n",
        "        stepcount = 0\n",
        "        for i in range(self.prooflooplength):\n",
        "            stepcount += 1\n",
        "            prooflength = i\n",
        "            if ActivePool['length'] > 0:\n",
        "                #\n",
        "                PreAPL = itt(ActivePool['length']).clone()\n",
        "                #\n",
        "                if self.pp.verbose:\n",
        "                    print(\"= = = = = =  loop\",i,\"= = = = =\",end=' ')\n",
        "                    print(itp(self.proofnumber),\"/\",itp(self.allnumbers),\"<\",itp(self.proofinstance),\"> = = = =\")\n",
        "                else:\n",
        "                    print(\".\",end = '')\n",
        "                    if (i%50) == 49:\n",
        "                        print(\" \")\n",
        "                    if (i%100) == 0:\n",
        "                        print(i)\n",
        "                napcount += 1\n",
        "                #\n",
        "                ChunkData, cdetection = self.rr3.selectchunk(ActivePool)\n",
        "                #\n",
        "                #\n",
        "                CurrentData, DoneData = self.rr3.managesplit(Mstrat,ChunkData,randomize)\n",
        "                #\n",
        "                #\n",
        "                ActivePool = self.transitionactive(ActivePool,cdetection,CurrentData)\n",
        "                # do the following before dropout    \n",
        "                if dropoutlimit == 0:\n",
        "                    EDN = itt(ActivePool['length']).clone().to(torch.float)\n",
        "                    self.ECN += itt(CurrentData['length']).clone().to(torch.float)\n",
        "                    if self.ECN > HST.proof_nodes_max:\n",
        "                        print(\"break after maximum proof nodes\")\n",
        "                        break\n",
        "                #\n",
        "                if dropoutlimit > 0:\n",
        "                    if self.pp.dropout_style == 'regular' or self.pp.dropout_style == 'uniform':\n",
        "                        PostAPL = itt(ActivePool['length']).clone().to(torch.float)\n",
        "                        ratio = PostAPL / PreAPL\n",
        "                        EDN *= ratio\n",
        "                        self.ECN += EDN\n",
        "                    ActivePool,DroppedPool,newsum, droppedsum = self.dropoutdata(Mlearn,ActivePool,dropoutlimit)\n",
        "                    #\n",
        "                    self.transitionsamples(ActivePool,DroppedPool)\n",
        "                    #\n",
        "                    if self.pp.dropout_style == 'adaptive':\n",
        "                        activelengthf = itt(ActivePool['length']).clone().to(torch.float)\n",
        "                        self.ECN += (activelengthf + droppedsum)\n",
        "                        EDN = 0.\n",
        "                    #\n",
        "                    #\n",
        "                    #\n",
        "                    if self.pp.verbose:\n",
        "                        self.printexamples(ActivePool)\n",
        "                #\n",
        "                #\n",
        "                if self.pp.verbose:\n",
        "                    print(\"Active Pool has length\",itp(ActivePool['length']),end = ' ')\n",
        "                    #\n",
        "                    print(\"treated Chunk Data of length\",itp(ChunkData['length']))\n",
        "                    print(\"yielding Current Data of length\",itp(CurrentData['length']))\n",
        "                    print(\"net active data gained\",itp(CurrentData['length'] - ChunkData['length']))\n",
        "                    #\n",
        "                    print(\"Active Pool has length\",itp(ActivePool['length']),end = ' ')\n",
        "                DonePool = self.transitiondone(C,DonePool,DoneData,ActivePool['length'])\n",
        "                #\n",
        "                gcc = gc.collect()\n",
        "                if self.pp.verbose:\n",
        "                    memReport('mg')\n",
        "                    #\n",
        "                    # \n",
        "                    if 0 < dropoutlimit <= self.pp.chunksize:\n",
        "                        print(\"Estimated nodes at this depth\",numpr(EDN,1),\"estimated cumulative nodes\",numpr(self.ECN,1))\n",
        "                    if dropoutlimit == 0:\n",
        "                        print(\"current nodes\",numpr(EDN,1),\"cumulative nodes\",numpr(self.ECN,1))\n",
        "                #\n",
        "                if ActivePool['length'] == 0: \n",
        "                    break\n",
        "                if ActivePool['length'] > self.stopthreshold: \n",
        "                    print(\"over threshold --------->>>>>>>>>>>>>>>>> stopping\")\n",
        "                    break\n",
        "                #\n",
        "                #\n",
        "            if ActivePool['length'] == 0: \n",
        "                break\n",
        "            if ActivePool['length'] > self.stopthreshold: \n",
        "                print(\"over threshold --------->>>>>>>>>>>>>>>>> stopping\")\n",
        "                break\n",
        "            #\n",
        "            #if (napcount%self.periodicity) == 0:\n",
        "                #siesta(self.sleeptime)\n",
        "            #\n",
        "        #\n",
        "        print(\"|||\")\n",
        "        activelength = ActivePool['length']\n",
        "        donelength = DonePool['length']\n",
        "        if donelength > 0:\n",
        "            C.process(DonePool)\n",
        "            DonePool = self.rr1.nulldata()\n",
        "        #\n",
        "        if dropoutlimit == 0:\n",
        "            cumulative_nodes = torch.round(self.ECN).to(torch.int64)\n",
        "            HST.record_full_proof(Mstrat,stepcount,cumulative_nodes,self.donecount)\n",
        "        else:\n",
        "            HST.record_dropout_proof(self.pp.dropout_style,dropoutlimit,stepcount,self.ECN)\n",
        "        #\n",
        "        if self.pp.verbose:\n",
        "            if activelength > 0:\n",
        "                print(\"there remained\",itp(activelength),\"active locations\", end=' ')\n",
        "            else:\n",
        "                print(\"no further active locations\", end=' ')\n",
        "            print(\"done pool has length\",itp(donelength))\n",
        "            print(\"Estimated Cumulative Nodes at end of proof\",numpr(self.ECN,1) )\n",
        "            print(\"done count is\",itp(self.donecount))\n",
        "            print(\"impossible basic count is\",itp(self.rr2.impossible_basic_count))\n",
        "            print(\"half ones count is\",itp(self.rr2.halfones_count))\n",
        "        return True, ActivePool, DonePool, prooflength\n",
        "\n",
        "    def dropoutdata(self,M,Data,dropoutlimit):\n",
        "        if self.pp.dropout_style == 'regular':\n",
        "            NewData,DroppedData = self.dropoutdataRegular(Data,dropoutlimit)\n",
        "            newsum = 0.\n",
        "            droppedsum = 0.\n",
        "        if self.pp.dropout_style == 'adaptive':\n",
        "            NewData,DroppedData,newsum,droppedsum = self.dropoutdataAdaptive(M,Data,dropoutlimit)\n",
        "        if self.pp.dropout_style == 'uniform':\n",
        "            NewData,DroppedData,newsum,droppedsum = self.dropoutdataUniform(M,Data,dropoutlimit)\n",
        "        #\n",
        "        return NewData,DroppedData, newsum,droppedsum\n",
        "        \n",
        "\n",
        "    def dropoutdataRegular(self,Data,dropoutlimit):\n",
        "        length = Data['length']\n",
        "        if length == 0:\n",
        "            return Data, self.rr1.nulldata()\n",
        "        permutation = torch.randperm(length,device = Dvc)\n",
        "        upper = dropoutlimit\n",
        "        if upper > length:\n",
        "            NewData = self.rr1.copydata(Data)\n",
        "            DroppedData = self.rr1.nulldata()\n",
        "        else:\n",
        "            indices = permutation[0:upper]\n",
        "            indices_dropped = permutation[upper:length]\n",
        "            NewData = self.rr1.indexselectdata(Data,indices)\n",
        "            DroppedData = self.rr1.indexselectdata(Data,indices_dropped)\n",
        "        return NewData,DroppedData\n",
        "\n",
        "    def extent_sliced(self,M,Data):\n",
        "        #\n",
        "        length = Data['length']\n",
        "        if length <= 1000:\n",
        "            extent_log = M.network(Data).detach()\n",
        "            extent_log = torch.clamp(extent_log,0.,8.)\n",
        "            extent = 10**extent_log\n",
        "            return extent\n",
        "        extent = torch.zeros((length),dtype = torch.float,device=Dvc)\n",
        "        lrange = arangeic(length)\n",
        "        #\n",
        "        lower = 0\n",
        "        for i in range(length):\n",
        "            upper = lower + 1000\n",
        "            if upper > length:\n",
        "                upper = length\n",
        "            indices = lrange[lower:upper]\n",
        "            DataSlice = self.rr1.indexselectdata(Data,indices)\n",
        "            extent_log = M.network(DataSlice).detach()\n",
        "            extent_log = torch.clamp(extent_log,0.,8.)\n",
        "            extent[lower:upper] = 10**extent_log\n",
        "            lower = upper\n",
        "            if upper >= length:\n",
        "                break\n",
        "        return extent\n",
        "\n",
        "    def dropoutdataAdaptive(self,M,Data,dropoutlimit):\n",
        "        length = Data['length']\n",
        "        if length == 0:\n",
        "            return Data, self.rr1.nulldata(),0.,0.\n",
        "        #\n",
        "        extent = self.extent_sliced(M,Data)\n",
        "        denom = extent.sum(0)\n",
        "        proba = (dropoutlimit * extent)/denom\n",
        "        tirage = torch.rand(length,device=Dvc)\n",
        "        detection = (tirage < proba)\n",
        "        indices1 = arangeic(length)[detection]\n",
        "        dlength = detection.to(torch.int64).sum(0)\n",
        "        if dlength > dropoutlimit:\n",
        "            indices1 = arangeic(length)[detection]\n",
        "            permutation = torch.randperm(dlength)\n",
        "            indices2 = permutation[dropoutlimit:dlength]\n",
        "            indices3 = indices1[indices2]\n",
        "            detection[indices3] = False\n",
        "        if  length <= dropoutlimit:\n",
        "            NewData = self.rr1.copydata(Data)\n",
        "            DroppedData = self.rr1.nulldata()\n",
        "            newsum = extent.sum(0)\n",
        "            droppedsum = 0.\n",
        "        else:\n",
        "            NewData = self.rr1.detectsubdata(Data,detection)\n",
        "            DroppedData = self.rr1.detectsubdata(Data,(~detection))\n",
        "            newsum = (extent[detection]).sum(0)\n",
        "            droppedsum = (extent[~detection]).sum(0)\n",
        "        return NewData,DroppedData, newsum, droppedsum\n",
        "\n",
        "\n",
        "    def dropoutdataUniform(self,M,Data,dropoutlimit):\n",
        "        length = Data['length']\n",
        "        if length == 0:\n",
        "            return Data, self.rr1.nulldata(),0.,0.\n",
        "        #\n",
        "        extent = self.extent_sliced(M,Data)\n",
        "        #\n",
        "        values,sort_indices = torch.sort(extent,0)\n",
        "        fraction = itf(length) / itf(dropoutlimit)\n",
        "        epsilon_multiplier = itf(length - dropoutlimit) / itf(length)\n",
        "        epsilon_multiplier = torch.clamp(epsilon_multiplier,0.,1.)\n",
        "        drange = arangeic(dropoutlimit).to(torch.float)\n",
        "        tirage = torch.rand(dropoutlimit,device=Dvc)\n",
        "        tirage2 = torch.rand(dropoutlimit,device=Dvc) - 0.5\n",
        "        tirage2vx = tirage2.view(dropoutlimit,1).expand(dropoutlimit,dropoutlimit)\n",
        "        irange = arangeic(dropoutlimit).view(dropoutlimit,1).expand(dropoutlimit,dropoutlimit)\n",
        "        jrange = arangeic(dropoutlimit).view(1,dropoutlimit).expand(dropoutlimit,dropoutlimit)\n",
        "        tirage2_triangle = tirage2vx * ( (irange > jrange).to(torch.float) )\n",
        "        tirage2_integral = tirage2_triangle.sum(0)\n",
        "        epsilon = tirage * epsilon_multiplier\n",
        "        drange_mod = drange + epsilon + (0.05 * tirage2_integral)\n",
        "        #\n",
        "        float_indices = drange_mod * fraction\n",
        "        round_indices = torch.round(float_indices).to(torch.int64)\n",
        "        round_indices = torch.clamp(round_indices,0,length-1)\n",
        "        combined_indices = sort_indices[round_indices]\n",
        "        detection = torch.zeros((length),dtype = torch.bool,device=Dvc)\n",
        "        detection[combined_indices] = True\n",
        "        #\n",
        "        if  length <= dropoutlimit:\n",
        "            NewData = self.rr1.copydata(Data)\n",
        "            DroppedData = self.rr1.nulldata()\n",
        "            newsum = extent.sum(0)\n",
        "            droppedsum = 0.\n",
        "        else:\n",
        "            NewData = self.rr1.detectsubdata(Data,detection)\n",
        "            DroppedData = self.rr1.detectsubdata(Data,(~detection))\n",
        "            newsum = (extent[detection]).sum(0)\n",
        "            droppedsum = (extent[~detection]).sum(0)\n",
        "        return NewData,DroppedData, newsum, droppedsum\n",
        "    \n",
        "\n",
        "    \n",
        "    def printsampleex(self):\n",
        "        samplelength = self.SamplePool['length']\n",
        "        if samplelength == 0:\n",
        "            return\n",
        "        upper = 20\n",
        "        if upper > samplelength:\n",
        "            upper = samplelength\n",
        "        permutation = torch.randperm(samplelength,device=Dvc)\n",
        "        depth = self.SamplePool['depth']\n",
        "        points = self.SamplePool['info'][:,self.pp.samplepoints]\n",
        "        for i in range(upper):\n",
        "            ip = permutation[i]\n",
        "            print(\"number\",ip,\"depth\",itp(depth[ip]),\"points\",itp(points[ip]))\n",
        "        return\n",
        "            \n",
        "    \n",
        "    \n",
        "    \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEDc1aEg_kSE"
      },
      "source": [
        "class Classifier :    # \n",
        "    def __init__(self,P):\n",
        "        #\n",
        "        #\n",
        "        self.Pp = P\n",
        "        self.rr4 = Relations4(self.Pp)\n",
        "        self.rr3 = self.rr4.rr2\n",
        "        self.rr2 = self.rr4.rr2\n",
        "        self.rr1 = self.rr4.rr1\n",
        "        self.alpha = self.Pp.alpha\n",
        "        self.alpha2 = self.Pp.alpha2\n",
        "        self.alpha3 = self.Pp.alpha3\n",
        "        self.alpha3z = self.Pp.alpha3z\n",
        "        self.beta = self.Pp.beta\n",
        "        self.betaz = self.Pp.betaz\n",
        "        #\n",
        "        self.sga = SymmetricGroup(self.alpha)\n",
        "        #\n",
        "        self.eqlength = 0\n",
        "        self.eqlist = None\n",
        "        #\n",
        "        #self.iota = 24*24*4\n",
        "        self.matrix = self.choosematrix()\n",
        "        self.ilength = 10\n",
        "        self.indices1,self.indices2,self.indices3,self.indices4,self.vector=self.choosestuff(self.ilength)\n",
        "\n",
        "\n",
        "    def initialize(self):\n",
        "        self.eqlength = 0\n",
        "        self.eqlist = None\n",
        "        return\n",
        "\n",
        "    def choosestuff(self,ilength):\n",
        "        assert ilength <= 48\n",
        "        permutation1 = torch.randperm(48,device = Dvc)\n",
        "        permutation2 = torch.randperm(48,device = Dvc)\n",
        "        permutation3 = torch.randperm(48,device = Dvc)\n",
        "        permutation4 = torch.randperm(48,device = Dvc)\n",
        "        #\n",
        "        indices1 = permutation1[0:ilength]\n",
        "        indices2 = permutation2[0:ilength]\n",
        "        indices3 = permutation3[0:ilength]\n",
        "        indices4 = permutation4[0:ilength]\n",
        "        #\n",
        "        permutationV = torch.randperm(101,device = Dvc)\n",
        "        vector = permutationV[0:ilength] - 50\n",
        "        return indices1,indices2,indices3,indices4,vector\n",
        "\n",
        "    def choosematrix(self):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        #\n",
        "        #thematrix = torch.zeros((a,a,a,a),dtype = torch.int64,device=Dvc)\n",
        "        for i in range(10):\n",
        "            permutation = torch.randperm((a*a*a*a),device=Dvc)\n",
        "            psquared = permutation*permutation\n",
        "            thematrix = psquared.view(a,a,a,a)\n",
        "        return thematrix\n",
        "\n",
        "    def geteq(self,Data):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a4 = self.alpha2 * self.alpha2\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        prodsum = prod.to(torch.int64).sum(3)\n",
        "        #\n",
        "        assert (((prodsum == 1).all(2)).all(1)).all(0)\n",
        "        #\n",
        "        values,table = torch.max(prod.to(torch.int64),3)\n",
        "        #\n",
        "        table1vx = table.view(length,a,a,1,1).expand(length,a,a,a,a)\n",
        "        table2vx = table.view(length,1,1,a,a).expand(length,a,a,a,a)\n",
        "        eq = (table1vx == table2vx)\n",
        "        iz = (table == b).view(length,a,a)\n",
        "        return eq,iz\n",
        "        \n",
        "\n",
        "\n",
        "    def orderinvariantSlice(self,Data):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a4 = self.alpha2 * self.alpha2\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        #print(\"at start of orderinvariantSlice, length is\",length)\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        eq,iz = self.geteq(Data)\n",
        "        #\n",
        "        eqview = eq.view(1,length,a,a,a,a)\n",
        "        izview = iz.view(1,length,a,a,1,1).expand(1,length,a,a,a,a)\n",
        "        eqiz = torch.cat((eqview,izview),0)\n",
        "        eqiz_p2 = eqiz.permute(0,1,2,3,5,4)\n",
        "        eqiz_p3 = eqiz.permute(0,1,2,4,3,5)\n",
        "        eqiz_p4 = eqiz.permute(0,1,2,4,5,3)\n",
        "        eqiz_p5 = eqiz.permute(0,1,2,5,3,4)\n",
        "        eqiz_p6 = eqiz.permute(0,1,2,5,4,3)\n",
        "        #\n",
        "        eqiz_c1 = torch.cat((eqiz,eqiz_p2,eqiz_p3,eqiz_p4,eqiz_p5,eqiz_p6),0)\n",
        "        eqiz_c2 = eqiz_c1.permute(0,1,3,4,5,2)\n",
        "        eqiz_c3 = eqiz_c1.permute(0,1,4,5,2,3)\n",
        "        eqiz_c4 = eqiz_c1.permute(0,1,5,2,3,4)\n",
        "        #\n",
        "        eqiz_cat = torch.cat((eqiz_c1,eqiz_c2,eqiz_c3,eqiz_c4),0)  # size 48.length.a.a.a.a\n",
        "        #\n",
        "        eqiz_cat1 = eqiz_cat[self.indices1]\n",
        "        eqiz_cat2 = eqiz_cat[self.indices2]\n",
        "        eqiz_cat3 = eqiz_cat[self.indices3]\n",
        "        eqiz_cat4 = eqiz_cat[self.indices4]\n",
        "        #\n",
        "        eqiz_andor = (eqiz_cat1 | eqiz_cat2)  & (eqiz_cat3 | eqiz_cat4)\n",
        "        #\n",
        "        eqiz_sum = ((eqiz_andor.to(torch.int64).sum(5)).sum(4)).sum(3)\n",
        "        #\n",
        "        vectorvx = self.vector.view(self.ilength,1,1).expand(self.ilength,length,a)\n",
        "        invariant = (eqiz_sum * vectorvx).sum(0)\n",
        "        #\n",
        "        return invariant\n",
        "\n",
        "    def orderinvariant(self,Data):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        #\n",
        "        length = Data['length']\n",
        "        #\n",
        "        lrange = arangeic(length)\n",
        "        #\n",
        "        invariant = torch.zeros((length,a),dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        lower = 0\n",
        "        for i in range(length):\n",
        "            upper = lower + 100\n",
        "            if upper > length:\n",
        "                upper = length\n",
        "            indices = lrange[lower:upper]\n",
        "            DataSlice = self.rr1.indexselectdata(Data,indices)\n",
        "            invariant[lower:upper] = self.orderinvariantSlice(DataSlice)\n",
        "            lower = upper\n",
        "            if lower >= length:\n",
        "                break\n",
        "        return invariant\n",
        "\n",
        "\n",
        "        \n",
        "    def printtestinvariant(self,length,invariant):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        #\n",
        "        assert length > 0\n",
        "        assert a > 1\n",
        "        #\n",
        "        invariant_sorted,indices = torch.sort(invariant,1)\n",
        "        #\n",
        "        invariant_pre = invariant_sorted[:,0:a-1]\n",
        "        invariant_post = invariant_sorted[:,1:a]\n",
        "        invariant_delta = invariant_post - invariant_pre\n",
        "        #\n",
        "        assert ((invariant_delta >= 0).all(1)).all(0)\n",
        "        delta_avg = (invariant_delta.sum(1)).sum(0).to(torch.float) / (itf(length) * itf(a-1))\n",
        "        #\n",
        "        delta_sum = (invariant_delta == 0).to(torch.int64).sum(1)\n",
        "        print(\"invariant delta average is\",numpr(delta_avg,1),\"for length\",itp(length))\n",
        "        for k in range(a-1):\n",
        "            locnumber = (delta_sum == k).to(torch.int64).sum(0)\n",
        "            print(\"number of locations with delta sum\",itp(k),\"is\",itp(locnumber))\n",
        "        return\n",
        "\n",
        "\n",
        "    def matrixinvariant(self,Data,ivector,gvector):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        #\n",
        "        length = Data['length']\n",
        "        #\n",
        "        eq,iz = self.geteq(Data)\n",
        "        #\n",
        "        vlength = len(ivector)\n",
        "        assert len(gvector) == vlength\n",
        "        #\n",
        "        irangevx = ivector.view(vlength,1,1,1,1).expand(vlength,a,a,a,a)\n",
        "        grangevx = gvector.view(vlength,1,1,1,1).expand(vlength,a,a,a,a)\n",
        "        xrangevx = arangeic(a).view(1,a,1,1,1).expand(vlength,a,a,a,a)\n",
        "        yrangevx = arangeic(a).view(1,1,a,1,1).expand(vlength,a,a,a,a)\n",
        "        zrangevx = arangeic(a).view(1,1,1,a,1).expand(vlength,a,a,a,a)\n",
        "        wrangevx = arangeic(a).view(1,1,1,1,a).expand(vlength,a,a,a,a)\n",
        "        #\n",
        "        xtransform = self.sga.grouptable[grangevx,xrangevx]\n",
        "        ytransform = self.sga.grouptable[grangevx,yrangevx]\n",
        "        ztransform = self.sga.grouptable[grangevx,zrangevx]\n",
        "        wtransform = self.sga.grouptable[grangevx,wrangevx]\n",
        "        #\n",
        "        eq_transform = eq[irangevx,xtransform,ytransform,ztransform,wtransform]\n",
        "        #\n",
        "        matrixvx = self.matrix.view(1,a,a,a,a).expand(vlength,a,a,a,a)\n",
        "        matrix_invariant = ((eq_transform.to(torch.int64)) * matrixvx).view(vlength,a*a*a*a).sum(1)\n",
        "        #\n",
        "        return matrix_invariant\n",
        "\n",
        "    def to_eqfunction(self,length,eq,iz):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = a*a\n",
        "        #\n",
        "        eqview = eq.view(length*a2,a2)\n",
        "        izview = iz.view(length*a2)\n",
        "        #\n",
        "        numerical = arangeic(a2).view(1,a2).expand(length*a2,a2) + 1\n",
        "        numerical_eq = numerical * (eqview.to(torch.int64))\n",
        "        #\n",
        "        values,eqfunctionv = torch.max(numerical_eq,1)\n",
        "        eqfunctionv[izview] = a2\n",
        "        eqfunction = eqfunctionv.view(length,a2)\n",
        "        return eqfunction\n",
        "\n",
        "    def from_eqfunction(self,length,eqfunction):\n",
        "        #\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = a*a\n",
        "        #\n",
        "        eqf1vx = eqfunction.view(length,a2,1).expand(length,a2,a2)\n",
        "        eqf2vx = eqfunction.view(length,1,a2).expand(length,a2,a2)\n",
        "        #\n",
        "        eq = (eqf1vx == eqf2vx)\n",
        "        #\n",
        "        iz = (eqfunction.view(length,a2) == a2)\n",
        "        #\n",
        "        return eq,iz\n",
        "\n",
        "    def transform_eqfunction(self,length,eq,iz,gvector):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = a*a\n",
        "        #\n",
        "        assert len(gvector)==length\n",
        "        #\n",
        "        eqv = eq.view(length,a,a,a,a)\n",
        "        izv = iz.view(length,a,a)\n",
        "        #\n",
        "        irangevx = arangeic(length).view(length,1,1,1,1).expand(length,a,a,a,a)\n",
        "        grangevx = gvector.view(length,1,1,1,1).expand(length,a,a,a,a)\n",
        "        xrangevx = arangeic(a).view(1,a,1,1,1).expand(length,a,a,a,a)\n",
        "        yrangevx = arangeic(a).view(1,1,a,1,1).expand(length,a,a,a,a)\n",
        "        zrangevx = arangeic(a).view(1,1,1,a,1).expand(length,a,a,a,a)\n",
        "        wrangevx = arangeic(a).view(1,1,1,1,a).expand(length,a,a,a,a)\n",
        "        #\n",
        "        xtransform = self.sga.grouptable[grangevx,xrangevx]\n",
        "        ytransform = self.sga.grouptable[grangevx,yrangevx]\n",
        "        ztransform = self.sga.grouptable[grangevx,zrangevx]\n",
        "        wtransform = self.sga.grouptable[grangevx,wrangevx]\n",
        "        #\n",
        "        eq_transform = eqv[irangevx,xtransform,ytransform,ztransform,wtransform].view(length,a2,a2)\n",
        "        #\n",
        "        irangeZvx = arangeic(length).view(length,1,1).expand(length,a,a)\n",
        "        grangeZvx = gvector.view(length,1,1).expand(length,a,a)\n",
        "        xrangeZvx = arangeic(a).view(1,a,1).expand(length,a,a)\n",
        "        yrangeZvx = arangeic(a).view(1,1,a).expand(length,a,a)\n",
        "        #\n",
        "        xtransformZ = self.sga.grouptable[grangeZvx,xrangeZvx]\n",
        "        ytransformZ = self.sga.grouptable[grangeZvx,yrangeZvx]\n",
        "        #\n",
        "        iz_transform = izv[irangeZvx,xtransformZ,ytransformZ].view(length,a2)\n",
        "        #\n",
        "        eqfunction_transform = self.to_eqfunction(length,eq_transform,iz_transform)\n",
        "        return eqfunction_transform\n",
        "        \n",
        "\n",
        "\n",
        "    def data_eqfunction_transform(self,Data,ivector,gvector):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = a*a\n",
        "        #\n",
        "        DataVector = self.rr1.indexselectdata(Data,ivector)\n",
        "        length = DataVector['length']\n",
        "        #\n",
        "        eq,iz = self.geteq(DataVector)\n",
        "        #\n",
        "        assert len(gvector) == length\n",
        "        #\n",
        "        #eqfunction = self.to_eqfunction(length,eq,iz)\n",
        "        #\n",
        "        eqfunction_transform = self.transform_eqfunction(length,eq,iz,gvector)\n",
        "        #\n",
        "        return eqfunction_transform\n",
        "\n",
        "    def uniqueinstances(self,length,eq_function):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2= a*a\n",
        "        #\n",
        "        assert length > 0\n",
        "        #\n",
        "        eqf1vx = eq_function.view(length,1,a2).expand(length,length,a2)\n",
        "        eqf2vx = eq_function.view(1,length,a2).expand(length,length,a2)\n",
        "        #\n",
        "        equivalent = (eqf1vx == eqf2vx).all(2) \n",
        "        #\n",
        "        first = arangeic(length).view(length,1).expand(length,length)\n",
        "        second = arangeic(length).view(1,length).expand(length,length)\n",
        "        #\n",
        "        isrep = ((~equivalent) | (first <= second)).all(1)\n",
        "        #\n",
        "        unique_length = isrep.to(torch.int64).sum(0)\n",
        "        eq_unique = eq_function[isrep]\n",
        "        #\n",
        "        assert unique_length > 0\n",
        "        #\n",
        "        return unique_length,eq_unique\n",
        "\n",
        "\n",
        "\n",
        "    def addSlice(self,length,eq_function):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2= a*a\n",
        "        #\n",
        "        ulength,eq_unique = self.uniqueinstances(length,eq_function)\n",
        "        #\n",
        "        alength = self.eqlength\n",
        "        #\n",
        "        if alength == 0:\n",
        "            self.eqlist = eq_unique\n",
        "            self.eqlength = ulength\n",
        "            return\n",
        "        alreadyvx = self.eqlist.view(1,alength,a2).expand(ulength,alength,a2)\n",
        "        newvx = eq_unique.view(ulength,1,a2).expand(ulength,alength,a2)\n",
        "        #\n",
        "        already_known = ( (alreadyvx == newvx).all(2) ).any(1)\n",
        "        #\n",
        "        detection = ~already_known\n",
        "        detected_length = detection.to(torch.int64).sum(0)\n",
        "        eq_detected = eq_unique[detection]\n",
        "        #\n",
        "        #\n",
        "        self.eqlist = torch.cat((self.eqlist,eq_detected),0)\n",
        "        #\n",
        "        self.eqlength += detected_length\n",
        "        #\n",
        "        return\n",
        "\n",
        "    def addinstances(self,length,eq_function):\n",
        "        #\n",
        "        if length == 0:\n",
        "            #print(\"no instances to add\")\n",
        "            return\n",
        "        #\n",
        "        lower = 0\n",
        "        for i in range(length):\n",
        "            upper = lower + 500\n",
        "            if upper > length:\n",
        "                upper = length\n",
        "            length_slice = upper - lower\n",
        "            eq_slice = eq_function[lower:upper]\n",
        "            self.addSlice(length_slice,eq_slice)\n",
        "            lower = upper\n",
        "            if lower >= length:\n",
        "                break\n",
        "        return\n",
        "\n",
        "    def processBasic(self,Data):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        assert a > 1\n",
        "        #\n",
        "        length = Data['length']\n",
        "        print(\"current eqlength\",itp(self.eqlength),\"processing data of length\",itp(length))\n",
        "        #\n",
        "        eq,iz = self.geteq(Data)\n",
        "        #\n",
        "        eq_function = self.to_eqfunction(length,eq,iz)\n",
        "        #\n",
        "        self.addinstances(length,eq_function)\n",
        "        #\n",
        "        return\n",
        "\n",
        "    def process(self,Data):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        assert a > 1\n",
        "        #\n",
        "        length = Data['length']\n",
        "        gl = self.sga.gtlength\n",
        "        #\n",
        "        invariant = self.orderinvariant(Data)\n",
        "        #\n",
        "        lrangevxr = arangeic(length).view(length,1,1).expand(length,gl,a).reshape(length*gl,a)\n",
        "        gtvxr =self.sga.grouptable.view(1,gl,a).expand(length,gl,a).reshape(length*gl,a)\n",
        "        #\n",
        "        invariant_gt = invariant[lrangevxr,gtvxr]\n",
        "        detection = ((invariant_gt[:,0:a-1]) <= (invariant_gt[:,1:a])).all(1)\n",
        "        dlength = detection.to(torch.int64).sum(0)\n",
        "        ivector = (arangeic(length).view(length,1).expand(length,gl).reshape(length*gl))[detection]\n",
        "        gvector = (arangeic(gl).view(1,gl).expand(length,gl).reshape(length*gl))[detection]\n",
        "        #\n",
        "        verification = torch.zeros((length),dtype = torch.bool,device=Dvc)\n",
        "        verification[ivector] = True\n",
        "        #\n",
        "        assert verification.all(0)\n",
        "        #\n",
        "        eq_function = self.data_eqfunction_transform(Data,ivector,gvector)\n",
        "        #\n",
        "        self.addinstances(dlength,eq_function)\n",
        "        #\n",
        "        return\n",
        "\n",
        "    def checklocationSlice(self,length,eq_function):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = a*a\n",
        "        #\n",
        "        location = torch.zeros((length),dtype = torch.int64,device=Dvc)\n",
        "        location[:] = -1\n",
        "        #\n",
        "        alength = self.eqlength\n",
        "        #\n",
        "        predetect_already = (self.eqlist.view(alength,a2)).sum(1)\n",
        "        predetect_current = (eq_function.view(length,a2)).sum(1)\n",
        "        predetect_already_vxr = predetect_already.view(alength,1).expand(alength,length).reshape(alength*length)\n",
        "        predetect_current_vxr = predetect_current.view(1,length).expand(alength,length).reshape(alength*length)\n",
        "        #\n",
        "        predetection = (predetect_already_vxr == predetect_current_vxr)\n",
        "        #\n",
        "        alrangevxr = arangeic(alength).view(alength,1).expand(alength,length).reshape(alength*length)\n",
        "        lrangevxr = arangeic(length).view(1,length).expand(alength,length).reshape(alength*length)\n",
        "        #\n",
        "        alrvector = alrangevxr[predetection]\n",
        "        currvector = lrangevxr[predetection]\n",
        "        #\n",
        "        eq_already = (self.eqlist.view(alength,a2))[alrvector]\n",
        "        eq_current = (eq_function.view(length,a2))[currvector]\n",
        "        #\n",
        "        detection = (eq_already == eq_current).all(1)\n",
        "        #\n",
        "        already_detected = alrvector[detection]\n",
        "        current_detected = currvector[detection]\n",
        "        #\n",
        "        location[current_detected] = already_detected\n",
        "        #\n",
        "        return location\n",
        "\n",
        "    def checklocation(self,length,eq_function):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = a*a\n",
        "        #\n",
        "        location = torch.zeros((length),dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        lower = 0\n",
        "        for i in range(length):\n",
        "            upper = lower + 100\n",
        "            if upper > length:\n",
        "                upper = length\n",
        "            length_slice = upper - lower\n",
        "            eq_slice = eq_function[lower:upper]\n",
        "            location[lower:upper] = self.checklocationSlice(length_slice,eq_slice)\n",
        "            lower = upper\n",
        "            if lower >= length:\n",
        "                break\n",
        "        return location\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    def highestlocation(self,length,eq_function):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = a*a\n",
        "        #\n",
        "        gl = self.sga.gtlength\n",
        "        #\n",
        "        eq_vxr = eq_function.view(length,1,a2).expand(length,gl,a2).reshape(length*gl,a2)\n",
        "        gvector = arangeic(gl).view(1,gl).expand(length,gl).reshape(length*gl)\n",
        "        grouplength = length*gl\n",
        "        #\n",
        "        eq,iz = self.from_eqfunction(grouplength,eq_vxr)\n",
        "        eq_transform = self.transform_eqfunction(grouplength,eq,iz,gvector)\n",
        "        #\n",
        "        location = self.checklocation(grouplength,eq_transform)\n",
        "        locationv = location.view(length,gl)\n",
        "        highest,indices = torch.max(locationv,1)\n",
        "        return highest\n",
        "\n",
        "    def sieve(self):\n",
        "        #\n",
        "        alength = self.eqlength\n",
        "        print(\"doing sieve on\",itp(alength),\"locations\")\n",
        "        #\n",
        "        highest_location = torch.zeros((alength),dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        lower = 0\n",
        "        for i in range(alength):\n",
        "            print(\"---\",lower)\n",
        "            upper = lower + 100\n",
        "            if upper > alength:\n",
        "                upper = alength\n",
        "            eq_slice = self.eqlist[lower:upper]\n",
        "            length_slice = upper - lower\n",
        "            #\n",
        "            highest_location[lower:upper] = self.highestlocation(length_slice,eq_slice)\n",
        "            lower = upper\n",
        "            if lower >= alength:\n",
        "                break\n",
        "        #\n",
        "        negative_detect = (highest_location < 0)\n",
        "        negative_count = negative_detect.to(torch.int64).sum(0)\n",
        "        print(\"unfortunate locations with negative values :\",itp(negative_count))\n",
        "        alrange = arangeic(alength)\n",
        "        good_detect = (highest_location <= alrange)\n",
        "        good_count = good_detect.to(torch.int64).sum(0)\n",
        "        print(\"detected\",itp(good_count),\"good locations\")\n",
        "        print(\"finished sieve\")\n",
        "        return\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pT12z4VtFy2"
      },
      "source": [
        "class Learner :    # \n",
        "    def __init__(self,Rr4):\n",
        "        #\n",
        "        #\n",
        "        self.rr4 = Rr4\n",
        "        self.pp = self.rr4.pp\n",
        "        self.rr3 = self.rr4.rr3\n",
        "        self.rr2 = self.rr4.rr2\n",
        "        self.rr1 = self.rr4.rr1\n",
        "        self.alpha = self.pp.alpha\n",
        "        self.alpha2 = self.pp.alpha2\n",
        "        self.alpha3 = self.pp.alpha3\n",
        "        self.alpha3z = self.pp.alpha3z\n",
        "        self.beta = self.pp.beta\n",
        "        self.betaz = self.pp.betaz\n",
        "        #\n",
        "        #self.mm = Mm\n",
        "        self.explore_max = self.pp.explore_max\n",
        "        self.examples_max = self.pp.examples_max\n",
        "        self.new_examples_max = self.pp.new_examples_max\n",
        "        self.new_explore_max = self.pp.new_explore_max\n",
        "        self.outlier_max = self.pp.outlier_max\n",
        "        self.new_outliers_max = self.pp.new_outliers_max\n",
        "        #\n",
        "        self.trainingprint = True\n",
        "        #\n",
        "        # all the examples are by convention active (not done or impossible)\n",
        "        self.OutlierPrePool = self.rr1.nulldata()\n",
        "        self.ExplorePrePool = self.rr1.nulldata()\n",
        "        self.ExamplesPrePool = self.rr1.nulldata()\n",
        "        self.Examples = self.rr1.nulldata()\n",
        "        self.extent_log = None  # log10 of the number of nodes below that node, including that one, note everything is active\n",
        "        # so this number of nodes is never 0\n",
        "        self.localscores = None  # at (x,y) it is log10 of the sum score over (x,y,p), plus 1 for the upper node itself\n",
        "        #\n",
        "        #self.sga = SymmetricGroup(self.alpha)\n",
        "        #self.sgb = SymmetricGroup(self.beta)\n",
        "        #\n",
        "        self.globalL1level = 1.0\n",
        "        \n",
        "\n",
        "    def pruneExamples(self,M): # remove the last ones, those were the earliest added\n",
        "        #\n",
        "        elength = self.Examples['length']\n",
        "        epplength = self.ExamplesPrePool['length']\n",
        "        xlength = self.ExplorePrePool['length']\n",
        "        olength = self.OutlierPrePool['length']\n",
        "        #\n",
        "        if elength > self.examples_max:\n",
        "            #self.specialPruneExamples(M)\n",
        "            permutation = torch.randperm(elength,device=Dvc)\n",
        "            indices = permutation[0:self.examples_max]\n",
        "            self.Examples = self.rr1.indexselectdata(self.Examples,indices)\n",
        "            self.extent_log = self.extent_log[indices]\n",
        "            self.localscores = self.localscores[indices] \n",
        "        if epplength > self.examples_max:\n",
        "            permutation = torch.randperm(epplength,device=Dvc)\n",
        "            indices = permutation[0:self.examples_max]\n",
        "            self.ExamplesPrePool = self.rr1.indexselectdata(self.ExamplesPrePool,indices)\n",
        "        if xlength > self.explore_max:\n",
        "            permutation = torch.randperm(xlength,device=Dvc)\n",
        "            indices = permutation[0:self.explore_max]\n",
        "            self.ExplorePrePool = self.rr1.indexselectdata(self.ExplorePrePool,indices)\n",
        "        if olength > self.outlier_max:\n",
        "            permutation = torch.randperm(olength,device=Dvc)\n",
        "            indices = permutation[0:self.outlier_max]\n",
        "            self.OutlierPrePool = self.rr1.indexselectdata(self.OutlierPrePool,indices)\n",
        "        return\n",
        "\n",
        "\n",
        "    def check_availability(self,Data,comment):\n",
        "        #\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        available = (((prod.any(3)).all(2)).all(1)).all(0)\n",
        "        if not available:\n",
        "            print(\"with comment\",comment)\n",
        "            raise CoherenceError(\"availability problem in data that should be active\") \n",
        "        return\n",
        "\n",
        "\n",
        "        \n",
        "    def prepoolSamples(self,M,Data,new_examples): # prepend new ones so that the earliest ones were the last ones\n",
        "        #\n",
        "        #\n",
        "        splength = Data['length']\n",
        "        #\n",
        "        if splength == 0:\n",
        "            return\n",
        "        permutation = torch.randperm(splength,device=Dvc)\n",
        "        upper = new_examples\n",
        "        if upper > splength:\n",
        "            upper = splength\n",
        "        indices = permutation[0:upper]\n",
        "        NewExamples = self.rr1.indexselectdata(Data,indices)\n",
        "        self.check_availability(NewExamples,\"prepoolSamples\")\n",
        "        self.ExamplesPrePool = self.rr1.appenddata(NewExamples,self.ExamplesPrePool)\n",
        "        #\n",
        "        self.pruneExamples(M)\n",
        "        return\n",
        "\n",
        "    def prepoolExplore(self,M,Data,new_examples): # prepend new ones so that the earliest ones were the last ones\n",
        "        #\n",
        "        #\n",
        "        splength = Data['length']\n",
        "        #\n",
        "        if splength == 0:\n",
        "            return\n",
        "        ppe_length = torch.round(itf(new_examples)/M.average_local_loss).to(torch.int64)\n",
        "        rectangle = arangeic(splength).view(splength,1).expand(splength,ppe_length).reshape(splength*ppe_length)\n",
        "        permutation = torch.randperm(splength*ppe_length,device=Dvc)\n",
        "        upper = ppe_length\n",
        "        #\n",
        "        indices = rectangle[permutation[0:upper]]\n",
        "        NewExamples = self.rr1.indexselectdata(Data,indices)\n",
        "        self.check_availability(NewExamples,\"prepoolExplore\")\n",
        "        self.ExplorePrePool = self.rr1.appenddata(NewExamples,self.ExplorePrePool)\n",
        "        #\n",
        "        self.pruneExamples(M)\n",
        "        return\n",
        "\n",
        "    def transferexamples(self,M,TransferData,extent,localscore,epp_throw_detection):\n",
        "        # detection tells us the ones to remove from the epp\n",
        "        if self.Examples['length'] == 0:\n",
        "            self.extent_log = extent\n",
        "            self.localscores = localscore\n",
        "        else:\n",
        "            self.extent_log = torch.cat((extent,self.extent_log),0)\n",
        "            self.localscores = torch.cat((localscore,self.localscores),0)\n",
        "        self.Examples = self.rr1.appenddata(TransferData,self.Examples)\n",
        "        #\n",
        "        keep_detection = ~epp_throw_detection\n",
        "        self.ExamplesPrePool = self.rr1.detectsubdata(self.ExamplesPrePool,keep_detection)\n",
        "        #\n",
        "        delta = self.abs_diff_sup(M,TransferData,localscore)\n",
        "        seuil = self.pp.outlier_threshold * M.average_local_loss\n",
        "        NewOutliers = self.rr1.detectsubdata(TransferData,(delta > seuil))\n",
        "        self.OutlierPrePool = self.rr1.appenddata(NewOutliers,self.OutlierPrePool)\n",
        "        #\n",
        "        self.pruneExamples(M)\n",
        "        return \n",
        "\n",
        "    def scoreExamples(self,M):\n",
        "        epplength = self.ExamplesPrePool['length']\n",
        "        if epplength == 0:\n",
        "            print(\"no examples to locally score\")\n",
        "            return\n",
        "        #\n",
        "        permutation = torch.randperm(epplength,device=Dvc)\n",
        "        upper = self.new_examples_max\n",
        "        if upper > epplength:\n",
        "            upper = epplength\n",
        "        indices = permutation[0:upper]\n",
        "        #\n",
        "        epp_throw_detection = torch.zeros((epplength),dtype = torch.bool,device=Dvc)\n",
        "        epp_throw_detection[indices] = True\n",
        "        #\n",
        "        TransferData = self.rr1.indexselectdata(self.ExamplesPrePool,indices)\n",
        "        #\n",
        "        print(\"transfering data of length\",itp(TransferData['length']))\n",
        "        #\n",
        "        xyscore_log, xyscore_min, LocalExamples = self.calculatescoresLocal(M,TransferData)\n",
        "        #\n",
        "        self.transferexamples(M,TransferData,xyscore_min,xyscore_log,epp_throw_detection)\n",
        "        #\n",
        "        self.check_availability(LocalExamples,\"LocalExamples in scoreExamples\")\n",
        "        self.prepoolExplore(M,LocalExamples,self.new_examples_max)\n",
        "        #\n",
        "        print(\"examples has size\",itp(self.Examples['length']))\n",
        "        print(\"example pre pool has size\",itp(self.ExamplesPrePool['length']))\n",
        "        return\n",
        "\n",
        "    def scoreExplore(self,M):\n",
        "        xlength = self.ExplorePrePool['length']\n",
        "        epplength = self.ExamplesPrePool['length']\n",
        "        if xlength == 0:\n",
        "            print(\"no examples to locally score\")\n",
        "            return\n",
        "        #\n",
        "        permutation = torch.randperm(xlength,device=Dvc)\n",
        "        current_explore = 2*self.new_examples_max\n",
        "        if current_explore > self.new_explore_max:\n",
        "            current_explore = self.new_explore_max\n",
        "        upper = current_explore\n",
        "        if upper > xlength:\n",
        "            upper = xlength\n",
        "        indices = permutation[0:upper]\n",
        "        #\n",
        "        epp_throw_detection = torch.zeros((epplength),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        TransferData = self.rr1.indexselectdata(self.ExplorePrePool,indices)\n",
        "        #\n",
        "        print(\"transfering explore data of length\",itp(TransferData['length']))\n",
        "        #\n",
        "        xyscore_log, xyscore_min, LocalExamples = self.calculatescoresLocal(M,TransferData)\n",
        "        #\n",
        "        self.transferexamples(M,TransferData,xyscore_min,xyscore_log,epp_throw_detection)\n",
        "        #\n",
        "        self.check_availability(LocalExamples,\"LocalExamples in scoreExplore\")\n",
        "        self.prepoolExplore(M,LocalExamples,current_explore)\n",
        "        #\n",
        "        print(\"examples has size\",itp(self.Examples['length']))\n",
        "        print(\"explore pre pool has size\",itp(self.ExplorePrePool['length']))\n",
        "        return\n",
        "\n",
        "    def scoreOutlier(self,M):\n",
        "        xlength = self.OutlierPrePool['length']\n",
        "        epplength = self.ExamplesPrePool['length']\n",
        "        if xlength == 0:\n",
        "            print(\"no outliers to locally score\")\n",
        "            return\n",
        "        #\n",
        "        permutation = torch.randperm(xlength,device=Dvc)\n",
        "        #\n",
        "        upper = self.new_outliers_max\n",
        "        if upper > xlength:\n",
        "            upper = xlength\n",
        "        indices = permutation[0:upper]\n",
        "        #\n",
        "        epp_throw_detection = torch.zeros((epplength),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        TransferData = self.rr1.indexselectdata(self.OutlierPrePool,indices)\n",
        "        #\n",
        "        print(\"transfering outlier data of length\",itp(TransferData['length']))\n",
        "        #\n",
        "        xyscore_log, xyscore_min, LocalExamples = self.calculatescoresLocal(M,TransferData)\n",
        "        #\n",
        "        delta = self.abs_diff_sup(M,TransferData,xyscore_log)\n",
        "        seuil = self.pp.outlier_threshold * M.average_local_loss\n",
        "        indices_throw = indices[(delta < seuil)]\n",
        "        throw_detection = torch.zeros((xlength),dtype = torch.bool,device=Dvc)\n",
        "        throw_detection[indices_throw] = True\n",
        "        self.OutlierPrePool = self.rr1.detectsubdata(self.OutlierPrePool,(~throw_detection))\n",
        "        #\n",
        "        self.transferexamples(M,TransferData,xyscore_min,xyscore_log,epp_throw_detection)\n",
        "        #\n",
        "        print(\"outlier pre pool has size\",itp(self.OutlierPrePool['length']))\n",
        "        return\n",
        "\n",
        "    def abs_diff_sup(self,M,Data,xyscore_log):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        availablexyf = self.rr1.availablexy(length,prod).view(length,a,a)\n",
        "        net2 = M.network2(Data)\n",
        "        predicted = net2.view(length,a,a).detach()\n",
        "        delta_abs = torch.abs((availablexyf * (xyscore_log - predicted))).view(length,a2)\n",
        "        delta_sup,indices = torch.max(delta_abs,1)\n",
        "        return delta_sup\n",
        "    \n",
        "    def addexamplesPrev(self,Data,logextent):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        datalength = Data['length']\n",
        "        if datalength == 0:\n",
        "            return\n",
        "        lsk_data = torch.zeros((datalength),dtype = torch.bool,device=Dvc)\n",
        "        ls_data = torch.zeros((datalength,a,a),dtype = torch.float,device=Dvc)\n",
        "        #\n",
        "        NewExamples = self.rr1.appenddata(self.Examples,Data)\n",
        "        if self.Examples['length'] == 0:\n",
        "            newextent = logextent\n",
        "            newlsk = lsk_data\n",
        "            newls = ls_data\n",
        "        else:\n",
        "            newextent = torch.cat((self.extent_log,logextent),0)\n",
        "            newlsk = torch.cat((self.localscores_known,lsk_data),0)\n",
        "            newls = torch.cat((self.localscores,ls_data),0)\n",
        "        #\n",
        "        newlength = NewExamples['length']\n",
        "        if newlength == 0:\n",
        "            return\n",
        "        assert len(newextent) == newlength\n",
        "        #\n",
        "        permutation = torch.randperm(newlength,device=Dvc)\n",
        "        upper = newlength\n",
        "        if upper > self.examples_max:\n",
        "            upper = self.examples_max\n",
        "        indices = permutation[0:upper]\n",
        "        self.Examples = self.rr1.indexselectdata(NewExamples,indices)\n",
        "        self.extent_log = newextent[indices]\n",
        "        self.localscores_known = newlsk[indices]\n",
        "        self.localscores = newls[indices]\n",
        "        return \n",
        "        \n",
        "        \n",
        "        \n",
        "    def addscoredexamples(self,M):\n",
        "        SamplePool = self.rr4.SamplePool\n",
        "        DroppedPool = self.rr4.DroppedSamplePool\n",
        "        #\n",
        "        splength = SamplePool['length']\n",
        "        dplength = DroppedPool['length']\n",
        "        #\n",
        "        sprectangle = SamplePool['info'][:,self.pp.sampleinfolower:self.pp.sampleinfoupper].clone()\n",
        "        #\n",
        "        splrangevxr = arangeic(splength).view(splength,1).expand(splength,200).reshape(splength*200)\n",
        "        sprectangler = sprectangle.reshape(splength*200)\n",
        "        sp_detection = (sprectangler >= 0) & (sprectangler < splrangevxr)\n",
        "        #\n",
        "        ivector = splrangevxr[sp_detection]\n",
        "        jvector = sprectangler[sp_detection]\n",
        "        # note that jvector < ivector because of the second condition in sp_detection\n",
        "        # jvector is any previous location strictly above the ivector location in the proof tree\n",
        "        #\n",
        "        incidence = torch.zeros((splength,splength),dtype = torch.float,device=Dvc)\n",
        "        #\n",
        "        incidence[ivector,jvector] = 1.\n",
        "        #\n",
        "        #\n",
        "        spnodes = incidence.sum(0) + 1. # this should be the number of nodes below that location, including that location\n",
        "        #\n",
        "        if dplength > 0:\n",
        "            dprectangle = DroppedPool['info'][:,self.pp.sampleinfolower:self.pp.sampleinfoupper].clone()\n",
        "            #\n",
        "            dincidence = torch.zeros((dplength,splength),dtype = torch.float,device=Dvc)\n",
        "            #\n",
        "            dplrangevxr = arangeic(dplength).view(dplength,1).expand(dplength,200).reshape(dplength*200)\n",
        "            dprectangler = dprectangle.reshape(dplength*200)\n",
        "            dp_detection = (dprectangler >= 0) \n",
        "            #\n",
        "            idvector = dplrangevxr[dp_detection]\n",
        "            jdvector = dprectangler[dp_detection]\n",
        "            #\n",
        "            dincidence[idvector,jdvector] = 1. \n",
        "            #\n",
        "            #\n",
        "            dpextent_log = M.network(DroppedPool).detach()\n",
        "            # this approximates log10 of (the number of nodes at or below a dropped location)\n",
        "            dpextent_log = torch.clamp(dpextent_log,0.,9.)\n",
        "            dpextent = (10**dpextent_log) \n",
        "            dpextentv = dpextent.view(1,dplength)\n",
        "            extent_transfer = (torch.matmul(dpextentv,dincidence)).view(splength)\n",
        "            #\n",
        "            extent = spnodes + extent_transfer\n",
        "        else:\n",
        "            extent = spnodes\n",
        "        #\n",
        "        logextent = torch.log10(extent)\n",
        "        #\n",
        "        #\n",
        "        permutation = torch.randperm(splength,device=Dvc)\n",
        "        upper = self.new_examples_max\n",
        "        if upper > splength:\n",
        "            upper = splength\n",
        "        indices = permutation[0:upper]\n",
        "        TransferData = self.rr1.indexselectdata(SamplePool,indices)\n",
        "        transfer_extent = logextent[indices] \n",
        "        #\n",
        "        epp_throw_detection = torch.zeros((self.ExamplesPrePool['length']),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        xyscore_log, xyscore_min, LocalExamples = self.calculatescoresLocal(M,TransferData)\n",
        "        #\n",
        "        positivephase = ( TransferData['info'][:,self.pp.phase] > 0 ) \n",
        "        phased_extent = transfer_extent.clone()\n",
        "        phased_extent[positivephase] = xyscore_min[positivephase]\n",
        "        #\n",
        "        self.transferexamples(M,TransferData,phased_extent,xyscore_log,epp_throw_detection)\n",
        "        #\n",
        "        return\n",
        "\n",
        "    \n",
        "\n",
        "    def noisetensor(self,thetensor):\n",
        "        #\n",
        "        length = len(thetensor)\n",
        "        tirage = torch.rand(length,device=Dvc)\n",
        "        noiselevel = HST.noiselevel(self.pp,torch.tensor(HST.training_counter,device=Dvc)).item()\n",
        "        bruit = (tirage < noiselevel)\n",
        "        ntensor = ((~bruit) & thetensor) | (bruit & (~thetensor))\n",
        "        return ntensor\n",
        " \n",
        "\n",
        "    def noise(self,Data):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prodv = Data['prod'].view(length*a*a*bz)\n",
        "        leftv = Data['left'].view(length*a*bz*2)\n",
        "        rightv = Data['right'].view(length*bz*a*2)\n",
        "        ternaryv = Data['ternary'].view(length*a*a*a*2)\n",
        "        #\n",
        "        nprod = self.noisetensor(prodv).view(length,a,a,bz)\n",
        "        nleft = self.noisetensor(leftv).view(length,a,bz,2)\n",
        "        nright = self.noisetensor(rightv).view(length,bz,a,2)\n",
        "        nternary = self.noisetensor(ternaryv).view(length,a,a,a,2)\n",
        "        #\n",
        "        NoiseData = self.rr1.copydata(Data)\n",
        "        NoiseData['prod'] = nprod\n",
        "        NoiseData['left'] = nleft\n",
        "        NoiseData['right'] = nright\n",
        "        NoiseData['ternary'] = nternary\n",
        "        #\n",
        "        return NoiseData\n",
        "    \n",
        "    \n",
        "    def printexamplescores(self,number):\n",
        "        ExamplePool = self.Examples\n",
        "        xplength = ExamplePool['length']\n",
        "        xpdepth = ExamplePool['depth']\n",
        "        if xplength == 0:\n",
        "            print(\"no examples to print\")\n",
        "            return\n",
        "        #\n",
        "        print(\"example pool has\",itp(xplength),\"elements\")\n",
        "        #\n",
        "        permutation = torch.randperm(xplength,device=Dvc)\n",
        "        #\n",
        "        upper = number\n",
        "        if upper > xplength:\n",
        "            upper = xplength\n",
        "        for i in range(upper):\n",
        "            ip = permutation[i]\n",
        "            idepth = xpdepth[ip]\n",
        "            iextent = self.extent_log[ip]\n",
        "            print(\"sample number\",itp(ip),\"depth\",itp(idepth),\"log extent\",numpr(iextent,2))\n",
        "        return\n",
        "    \n",
        "    def selectminibatch(self,minibatchsize):\n",
        "        ExamplePool = self.Examples\n",
        "        xplength = ExamplePool['length']\n",
        "        score = self.extent_log\n",
        "        if xplength < 10:\n",
        "            print(\"not enough examples to train on\")\n",
        "            return False,None,None\n",
        "        #\n",
        "        permutation = torch.randperm(xplength,device=Dvc)\n",
        "        upper = minibatchsize\n",
        "        if upper > xplength:\n",
        "            upper = xplength\n",
        "        indices = permutation[0:upper]\n",
        "        DataBatch = self.rr1.indexselectdata(ExamplePool,indices)\n",
        "        scorebatch = score[indices]\n",
        "        return True,DataBatch,scorebatch\n",
        "        \n",
        "        \n",
        "        \n",
        "    def trainingGlobal(self,M,numberofbatches,iterationsperbatch,style,minibatchsize,partname):\n",
        "        #\n",
        "        if style != 'score-A' and style != 'score-B' and style != 'score-C':\n",
        "            raise CoherenceError(\"only allowed styles are score-A or score-B or score-C\")\n",
        "        #\n",
        "        if self.trainingprint:\n",
        "            print(\"/\",style,numberofbatches,iterationsperbatch,partname,\"/\",end = ' ')\n",
        "        for s in range(numberofbatches):\n",
        "            smb,DataBatch, scorebatch = self.selectminibatch(minibatchsize)\n",
        "            if not smb:\n",
        "                print(\"exit training\")\n",
        "                return\n",
        "            #\n",
        "            for i in range(iterationsperbatch):\n",
        "                #\n",
        "                M.optimizer.zero_grad()\n",
        "                #\n",
        "                NoiseData = self.noise(DataBatch)\n",
        "                #\n",
        "                predictedscore = M.network(NoiseData)\n",
        "                #\n",
        "                if style == 'score-A':\n",
        "                    loss = M.criterionA(predictedscore,scorebatch)\n",
        "                if style == 'score-B':\n",
        "                    loss = M.criterionB(predictedscore,scorebatch)\n",
        "                if style == 'score-C':\n",
        "                    lossA = M.criterionA(predictedscore,scorebatch)\n",
        "                    lossB = M.criterionB(predictedscore,scorebatch)\n",
        "                    loss = (lossA + lossB)/2\n",
        "                loss.backward()\n",
        "                M.optimizer.step()\n",
        "            #\n",
        "        #\n",
        "        print(\"-\",end = ' ')\n",
        "        #\n",
        "        return\n",
        "    \n",
        "    def printlossaftertrainingGlobal(self,M,minibatchsize,topicture):\n",
        "        #\n",
        "        smb,DataBatch,scorebatch = self.selectminibatch(minibatchsize)\n",
        "        if not smb:\n",
        "            print(\"data too small\")\n",
        "            return\n",
        "        #\n",
        "        mblength = DataBatch['length']\n",
        "        predictedscore = M.network(DataBatch)\n",
        "        lossa= M.criterionA(predictedscore,scorebatch)\n",
        "        lra = numpr(lossa,3)\n",
        "        lossb = M.criterionB(predictedscore,scorebatch)\n",
        "        lrb = numpr(lossb,3)\n",
        "        #\n",
        "        with torch.no_grad():\n",
        "            self.globalL1level += lossa\n",
        "            self.globalL1level *= 0.5\n",
        "            self.globalL1level = torch.clamp(self.globalL1level,0.005,1.0)\n",
        "        #\n",
        "        print(\"on \",itp(mblength),\"values network -- L1 loss\",lra,\"MSE loss\",lrb)\n",
        "        #\n",
        "        if topicture:\n",
        "            #\n",
        "            print(\"global L1 loss level\",numpr(self.globalL1level,4))\n",
        "            #\n",
        "            HST.record_loss('global',lossa,lossb)\n",
        "            #\n",
        "            dotsize = torch.zeros((mblength),dtype = torch.int,device=Dvc) \n",
        "            dotsize[:]=2\n",
        "            dotsize_np = nump(dotsize)\n",
        "            #\n",
        "            calcscore_npr = numpr(scorebatch,3)\n",
        "            predscore_npr = numpr(predictedscore,3)\n",
        "            #\n",
        "            scoremax,index = torch.max(scorebatch,0)\n",
        "            linelimit = numpr(scoremax,1)\n",
        "            #\n",
        "            #\n",
        "            plt.clf() \n",
        "            plt.scatter(calcscore_npr,predscore_npr,dotsize_np) \n",
        "            #\n",
        "            plt.plot([0.0,linelimit],[0.0,0.0],'g-',lw=1) \n",
        "            plt.plot([0.0,0.0],[0.0,linelimit],'g-',lw=1) \n",
        "            plt.plot([0.0,linelimit],[0.0,linelimit],'r-',lw=1) \n",
        "            plt.show()\n",
        "        return\n",
        "        \n",
        "    def learningGlobal(self,M,globaliterations):\n",
        "        #\n",
        "        self.printlossaftertrainingGlobal(M,500,True)\n",
        "        #\n",
        "        tweak_cursor = HST.global_tweak_cursor\n",
        "        tdensity = self.pp.tweak_density * (self.pp.tweak_decay ** tweak_cursor)\n",
        "        tepsilon = self.pp.tweak_epsilon * (self.pp.tweak_decay ** tweak_cursor)\n",
        "        HST.global_tweak_cursor += 1\n",
        "        print(\"tweaking global network at cursor\",itp(tweak_cursor),\"with density\",numpr(tdensity,4),\"and epsilon\",numpr(tepsilon,4))\n",
        "        M.tweak_network(M.network,tdensity,tepsilon)\n",
        "        self.printlossaftertrainingGlobal(M,500,True)\n",
        "        print(\"training\",end = ' ')\n",
        "        #\n",
        "        explore_pre_length = self.ExplorePrePool['length']\n",
        "        example_pre_length = self.ExamplesPrePool['length']\n",
        "        example_length = self.Examples['length']\n",
        "        HST.record_training('global',globaliterations,explore_pre_length,example_pre_length,example_length)\n",
        "        #\n",
        "        for g in range(globaliterations):\n",
        "            print(\"/\",end=' ')\n",
        "            self.trainingGlobal(M,2,20,'score-C',20,'mb20')\n",
        "            self.trainingGlobal(M,1,30,'score-C',30,'mb30')\n",
        "            self.trainingGlobal(M,2,10,'score-C',40,'mb40')\n",
        "            self.trainingGlobal(M,5,10,'score-C',60,'mb60')\n",
        "            self.trainingGlobal(M,3,8,'score-C',20,'mb20')\n",
        "            self.trainingGlobal(M,3,5,'score-C',40,'mb40')\n",
        "            self.trainingGlobal(M,5,3,'score-C',30,'mb30')\n",
        "            #\n",
        "            print(\" \")\n",
        "            self.printlossaftertrainingGlobal(M,300,False)\n",
        "            print(\"  \")\n",
        "        self.printlossaftertrainingGlobal(M,500,True)\n",
        "        print(\"=================    end score training   =================\")\n",
        "        return\n",
        "        \n",
        "    ######## Local stuff\n",
        "    \n",
        "    def calculatescoresLocal(self,M,Data):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        xypscore_exp = torch.zeros((length,a,a,bz),dtype = torch.float,device=Dvc)\n",
        "        #\n",
        "        availablexyp = self.rr1.availablexyp(length,prod)\n",
        "        availablexypr = availablexyp.reshape(length*a*a*bz)\n",
        "        #\n",
        "        lrangevxr = arangeic(length).view(length,1,1,1).expand(length,a,a,bz).reshape(length*a*a*bz)\n",
        "        xrangevxr = arangeic(a).view(1,a,1,1).expand(length,a,a,bz).reshape(length*a*a*bz)\n",
        "        yrangevxr = arangeic(a).view(1,1,a,1).expand(length,a,a,bz).reshape(length*a*a*bz)\n",
        "        prangevxr = arangeic(bz).view(1,1,1,bz).expand(length,a,a,bz).reshape(length*a*a*bz)\n",
        "        #\n",
        "        ivector = lrangevxr[availablexypr]\n",
        "        xvector = xrangevxr[availablexypr]\n",
        "        yvector = yrangevxr[availablexypr]\n",
        "        pvector = prangevxr[availablexypr]\n",
        "        #\n",
        "        NewData = self.rr1.upsplitting(Data,ivector,xvector,yvector,pvector)\n",
        "        #\n",
        "        #\n",
        "        ndlength = NewData['length']\n",
        "        #\n",
        "        #\n",
        "        LocalExamples = self.rr1.nulldata()\n",
        "        detection = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        newextent_exp = torch.zeros((ndlength),dtype = torch.float,device=Dvc)\n",
        "        # that should be the (approximation of) the number of nodes below and including that node resulting from (i,x,y,p)\n",
        "        newactive = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        lower = 0\n",
        "        for i in range(ndlength):\n",
        "            assert lower < ndlength\n",
        "            upper = lower + 1000\n",
        "            if upper > ndlength:\n",
        "                upper = ndlength\n",
        "            detection[:] = False\n",
        "            detection[lower:upper] = True\n",
        "            NewDataSlice = self.rr1.detectsubdata(NewData,detection)\n",
        "            AssocNewDataSlice = self.rr2.process(NewDataSlice)\n",
        "            newactive_s,newdone_s,newimpossible_s = self.rr2.filterdata(AssocNewDataSlice)\n",
        "            #\n",
        "            ActiveNewDataSlice = self.rr1.detectsubdata(AssocNewDataSlice,newactive_s)\n",
        "            LocalExamples = self.rr1.appenddata(ActiveNewDataSlice,LocalExamples)\n",
        "            #\n",
        "            predictedscore_s = M.network(AssocNewDataSlice).detach()\n",
        "            if torch.isnan(predictedscore_s).any(0):\n",
        "                raise CoherenceException(\"predicted score nan\")\n",
        "            # recall that approximates log10 of (the number of nodes below and including that node)\n",
        "            predictedscore_s_clamp = torch.clamp(predictedscore_s,0.,8.)\n",
        "            predictedscore_s_exp = 10.**predictedscore_s_clamp\n",
        "            predictedscore_s_exp[newdone_s] = 0.0 \n",
        "            predictedscore_s_exp[newimpossible_s] = 0.1\n",
        "            newextent_exp[lower:upper] = predictedscore_s_exp\n",
        "            newactive[lower:upper] = newactive_s\n",
        "            lower = upper\n",
        "            if lower >= ndlength:\n",
        "                break\n",
        "        #\n",
        "        xypscore_exp[ivector,xvector,yvector,pvector] = newextent_exp\n",
        "        #\n",
        "        xyscore_sum = xypscore_exp.sum(3)\n",
        "        #\n",
        "        xyscore_log = torch.log10(xyscore_sum + 1.)  # here the +1. is for the upper node itself. \n",
        "        #\n",
        "        # now replace the global scores by these ones too: here output the min of availables\n",
        "        availablexyr = self.rr1.availablexy(length,prod).view(length*a2)\n",
        "        xyscorer = xyscore_log.view(length*a2)\n",
        "        xyscorer_mod = torch.clamp(xyscorer,0.,8.)\n",
        "        xyscorer_mod[~availablexyr] = 20.\n",
        "        xyscore_min,xysm_indices = torch.min(xyscorer_mod.view(length,a2),1)\n",
        "        assert (xyscore_min < 10.).all(0)\n",
        "        #\n",
        "        return xyscore_log, xyscore_min, LocalExamples\n",
        "    \n",
        "        \n",
        "    def printsomelocalscores(self,howmany):\n",
        "        elength = self.Examples['length']\n",
        "        if elength == 0:\n",
        "            print(\"no examples\")\n",
        "            return\n",
        "        lsk = self.localscores_known\n",
        "        detection = lsk\n",
        "        detection_length = detection.to(torch.int64).sum(0)\n",
        "        if detection_length == 0:\n",
        "            print(\"no known score locations to print\")\n",
        "            return\n",
        "        ivector = arangeic(elength)[detection]\n",
        "        permutation = torch.randperm(detection_length,device=Dvc)\n",
        "        upper = howmany\n",
        "        if upper > detection_length:\n",
        "            upper = detection_length\n",
        "        indices_pre = permutation[0:upper]\n",
        "        indices = ivector[indices_pre]\n",
        "        #\n",
        "        assert detection[indices].all(0)\n",
        "        #\n",
        "        for i in range(upper):\n",
        "            indexi = indices[i]\n",
        "            lsi = self.localscores[indexi]\n",
        "            print(numpr(lsi,2))\n",
        "        return \n",
        "    \n",
        "    def predictedscoreLocal(self,M,ivector,xvector,yvector):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        #\n",
        "        length = self.Examples['length']\n",
        "        prod = self.Examples['prod']\n",
        "        #\n",
        "        availablexy = self.rr1.availablexy(length,prod).view(length,a,a)\n",
        "        #\n",
        "        assert availablexy[ivector,xvector,yvector].all(0)\n",
        "        #\n",
        "        Data = self.rr1.indexselectdata(self.Examples,ivector)\n",
        "        dlength = Data['length']\n",
        "        dlrange = arangeic(dlength)\n",
        "        #\n",
        "        NoiseData = self.noise(Data)\n",
        "        #\n",
        "        pre_score = M.network2(NoiseData)\n",
        "        #\n",
        "        dlrangevxa = dlrange.view(dlength,1).expand(dlength,a)\n",
        "        arangevx = arangeic(a).view(1,a).expand(dlength,a)\n",
        "        #\n",
        "        predictedscore = pre_score[dlrange,xvector,yvector]\n",
        "        return predictedscore\n",
        "\n",
        "\n",
        "    def adapt_local_scores(self,ivector,xvector,yvector):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        ExamplePool = self.Examples\n",
        "        #\n",
        "        length = len(ivector)\n",
        "        prod = (ExamplePool['prod'])[ivector]\n",
        "        availablexyv = self.rr1.availablexy(length,prod).reshape(length*a*a)\n",
        "        #\n",
        "        available_count = availablexyv.view(length,a*a).to(torch.int64).sum(1)\n",
        "        available_count_xf = available_count.view(length,1).expand(length,a*a).to(torch.float)\n",
        "        available_count_xf -= 1.\n",
        "        available_count_xf = torch.clamp(available_count_xf,1.,100.)\n",
        "        #\n",
        "        score = self.localscores[ivector].reshape(length*a*a)\n",
        "        score[~availablexyv] = 100. \n",
        "        values,indices = torch.sort(score.view(length,a*a),1)\n",
        "        #\n",
        "        position = torch.zeros((length,a*a),dtype = torch.float,device=Dvc)\n",
        "        #\n",
        "        lrangevx = arangeic(length).view(length,1).expand(length,a*a)\n",
        "        a2rangevx = arangeic(a*a).view(1,a*a).expand(length,a*a)\n",
        "        position[lrangevx,indices] = a2rangevx.to(torch.float) / available_count_xf\n",
        "        #\n",
        "        position_score = position + score.view(length,a*a)\n",
        "        #\n",
        "        adapted_score = position_score.view(length,a,a)[arangeic(length),xvector,yvector]\n",
        "        return adapted_score\n",
        "        \n",
        "    def selectminibatchLocal(self,minibatchsize):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        ExamplePool = self.Examples\n",
        "        xplength = ExamplePool['length']\n",
        "        if xplength == 0:\n",
        "            return False,None,None,None,None\n",
        "        xpprod = ExamplePool['prod']\n",
        "        xp_availablexy = self.rr1.availablexy(xplength,xpprod).view(xplength,a,a)\n",
        "        #\n",
        "        irangevxr = arangeic(xplength).view(xplength,1,1).expand(xplength,a,a).reshape(xplength*a*a)\n",
        "        xrangevxr = arangeic(a).view(1,a,1).expand(xplength,a,a).reshape(xplength*a*a)\n",
        "        yrangevxr = arangeic(a).view(1,1,a).expand(xplength,a,a).reshape(xplength*a*a)\n",
        "        #\n",
        "        avdetect = xp_availablexy[irangevxr,xrangevxr,yrangevxr]\n",
        "        #\n",
        "        ivector_all = irangevxr[avdetect]\n",
        "        xvector_all = xrangevxr[avdetect]\n",
        "        yvector_all = yrangevxr[avdetect]\n",
        "        #\n",
        "        avlength = avdetect.to(torch.int64).sum(0)\n",
        "        #\n",
        "        permutation = torch.randperm(avlength,device=Dvc)\n",
        "        mblength = minibatchsize\n",
        "        if mblength > avlength:\n",
        "            mblength = avlength\n",
        "        indices = permutation[0:mblength]\n",
        "        #\n",
        "        ivector = ivector_all[indices]\n",
        "        xvector = xvector_all[indices]\n",
        "        yvector = yvector_all[indices]\n",
        "        #\n",
        "        scorebatch = self.adapt_local_scores(ivector,xvector,yvector)\n",
        "        #\n",
        "        return True,ivector,xvector,yvector,scorebatch\n",
        "        \n",
        "    def trainingLocal(self,M,numberofbatches,iterationsperbatch,style,minibatchsize,partname):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        #\n",
        "        if style != 'score-A' and style != 'score-B' and style != 'score-C':\n",
        "            raise CoherenceError(\"only allowed styles are score-A or score-B or score-C\")\n",
        "        #\n",
        "        if self.trainingprint:\n",
        "            print(\"/\",style,numberofbatches,iterationsperbatch,partname,\"/\",end = ' ')\n",
        "        for s in range(numberofbatches):\n",
        "            smb,ivector,xvector,yvector,scorebatch = self.selectminibatchLocal(minibatchsize)\n",
        "            if not smb:\n",
        "                print(\"exit training\")\n",
        "                return\n",
        "            #\n",
        "            for i in range(iterationsperbatch):\n",
        "                #\n",
        "                M.optimizer2.zero_grad()\n",
        "                #\n",
        "                predictedscore = self.predictedscoreLocal(M,ivector,xvector,yvector)\n",
        "                #\n",
        "                if style == 'score-C':\n",
        "                    lossA = M.criterionA(predictedscore,scorebatch)\n",
        "                    lossB = M.criterionB(predictedscore,scorebatch)\n",
        "                    loss = (lossA + lossB) / 2.\n",
        "                loss.backward()\n",
        "                M.optimizer2.step()\n",
        "            #\n",
        "        #\n",
        "        print(\"-\",end = ' ')\n",
        "        #\n",
        "        return\n",
        "    \n",
        "    def printlossaftertrainingLocal(self,M,minibatchsize,topicture):\n",
        "        #\n",
        "        a=M.pp.alpha\n",
        "        #\n",
        "        smb,ivector,xvector,yvector,scorebatch = self.selectminibatchLocal(minibatchsize)\n",
        "        if not smb:\n",
        "            print(\"data too small\")\n",
        "            return\n",
        "        #\n",
        "        mblength = len(ivector)\n",
        "        #\n",
        "        predictedscore = self.predictedscoreLocal(M,ivector,xvector,yvector)\n",
        "        #\n",
        "        lossa= M.criterionA(predictedscore,scorebatch)\n",
        "        lra = numpr(lossa,3)\n",
        "        #\n",
        "        lossb = M.criterionB(predictedscore,scorebatch)\n",
        "        lrb = numpr(lossb,3)\n",
        "        #\n",
        "        lossa_detach = lossa.detach()\n",
        "        M.average_local_loss = (0.9 * M.average_local_loss + 0.1 * lossa_detach)  # change the variable name later!\n",
        "        #\n",
        "        print(\"on \",itp(mblength),\"values network -- L1 loss\",lra,\"MSE loss\",lrb)\n",
        "        #\n",
        "        if topicture:\n",
        "            #\n",
        "            HST.record_loss('local',lossa,lossb)\n",
        "            #\n",
        "            print(\"average local loss\",numpr(M.average_local_loss,4))\n",
        "            #\n",
        "            dotsize = torch.zeros((mblength),dtype = torch.int,device=Dvc) \n",
        "            dotsize[:]=2\n",
        "            dotsize_np = nump(dotsize)\n",
        "            #\n",
        "            calcscore_npr = numpr(scorebatch,3)\n",
        "            predscore_npr = numpr(predictedscore,3)\n",
        "            #\n",
        "            scoremax,index = torch.max(scorebatch,0)\n",
        "            linelimit = numpr(scoremax,1)\n",
        "            #\n",
        "            plt.clf() \n",
        "            plt.scatter(calcscore_npr,predscore_npr,dotsize_np) \n",
        "            #\n",
        "            #linelimit = 1.0\n",
        "            plt.plot([0.0,linelimit],[0.0,0.0],'g-',lw=1) \n",
        "            plt.plot([0.0,0.0],[0.0,linelimit],'g-',lw=1) \n",
        "            plt.plot([0.0,linelimit],[0.0,linelimit],'r-',lw=1) \n",
        "            #\n",
        "            plt.show()\n",
        "        return\n",
        "        \n",
        "    def learningLocal(self,M,globaliterations):\n",
        "        #\n",
        "        self.printlossaftertrainingLocal(M,500,True)\n",
        "        #\n",
        "        tweak_cursor = HST.local_tweak_cursor\n",
        "        tdensity = self.pp.tweak_density * (self.pp.tweak_decay ** tweak_cursor)\n",
        "        tepsilon = self.pp.tweak_epsilon * (self.pp.tweak_decay ** tweak_cursor)\n",
        "        HST.local_tweak_cursor += 1\n",
        "        print(\"tweaking local network at cursor\",itp(tweak_cursor),\"with density\",numpr(tdensity,4),\"and epsilon\",numpr(tepsilon,4))\n",
        "        M.tweak_network(M.network2,tdensity,tepsilon)\n",
        "        self.printlossaftertrainingLocal(M,500,True)\n",
        "        print(\"training\",end = ' ')\n",
        "        #\n",
        "        explore_pre_length = self.ExplorePrePool['length']\n",
        "        example_pre_length = self.ExamplesPrePool['length']\n",
        "        example_length = self.Examples['length']\n",
        "        HST.record_training('local',globaliterations,explore_pre_length,example_pre_length,example_length)\n",
        "        #\n",
        "        for g in range(globaliterations):\n",
        "            print(\"/\",end=' ')\n",
        "            self.trainingLocal(M,3,20,'score-C',20,'mb20')\n",
        "            self.trainingLocal(M,1,15,'score-C',30,'mb30')\n",
        "            self.trainingLocal(M,3,4,'score-C',60,'mb60')\n",
        "            self.trainingLocal(M,3,10,'score-C',40,'mb40')\n",
        "            self.trainingLocal(M,3,3,'score-C',20,'mb20')\n",
        "            self.trainingLocal(M,3,2,'score-C',40,'mb40')\n",
        "            self.trainingLocal(M,3,1,'score-C',30,'mb30')\n",
        "            #\n",
        "            print(\" \")\n",
        "            self.printlossaftertrainingLocal(M,300,False)\n",
        "            print(\"  \")\n",
        "        self.printlossaftertrainingLocal(M,500,True)\n",
        "        print(\"=================    end score training   =================\")\n",
        "        return\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS3g8Wi5SYV2"
      },
      "source": [
        "class Driver :    # this becomes the first element of the relations datatype\n",
        "    def __init__(self,P):\n",
        "        #\n",
        "        #\n",
        "        self.Pp = P\n",
        "        self.rr4 = Relations4(self.Pp)\n",
        "        self.rr3 = self.rr4.rr2\n",
        "        self.rr2 = self.rr4.rr2\n",
        "        self.rr1 = self.rr4.rr1\n",
        "        self.alpha = self.Pp.alpha\n",
        "        self.alpha2 = self.Pp.alpha2\n",
        "        self.alpha3 = self.Pp.alpha3\n",
        "        self.alpha3z = self.Pp.alpha3z\n",
        "        self.beta = self.Pp.beta\n",
        "        self.betaz = self.Pp.betaz\n",
        "        #\n",
        "        self.sga = SymmetricGroup(self.alpha)\n",
        "        #\n",
        "        self.zbinatable = self.makezbinatable()\n",
        "        #\n",
        "        self.init_length,self.init_left_table = self.make_init_left_table()\n",
        "        #\n",
        "        self.donecount_collection = 0\n",
        "        self.ECN_collection = 0.\n",
        "        self.ECN_average = 0. \n",
        "        #\n",
        "        self.Cc = Classifier(self.Pp)\n",
        "        #\n",
        "        self.Ll = Learner(self.rr4)\n",
        "        #\n",
        "        HST.record_driver(self.alpha,self.beta)\n",
        "        \n",
        "            \n",
        "            \n",
        "    def printprod(self,prod,loc):\n",
        "        #\n",
        "        a=self.alpha\n",
        "        a2=a*a\n",
        "        a3=a*a*a\n",
        "        a3z = a3+1\n",
        "        b=self.beta\n",
        "        bz = b+1\n",
        "        #\n",
        "        prodi = prod[loc]\n",
        "        prarray = torch.zeros((a,a),dtype = torch.int64,device=Dvc)\n",
        "        for x in range(a):\n",
        "            for y in range(a):\n",
        "                column = prod[loc,x,y]\n",
        "                prarray[x,y] = self.printcolumn(column)\n",
        "        print(nump(prarray))\n",
        "        return\n",
        "    \n",
        "    def printcolumn(self,column):\n",
        "        #\n",
        "        a=self.alpha\n",
        "        a2=a*a\n",
        "        a3=a*a*a\n",
        "        a3z = a3+1\n",
        "        b=self.beta\n",
        "        bz = b+1\n",
        "        #\n",
        "        assert len(column) == bz\n",
        "        #\n",
        "        column_sum = column.to(torch.int64).sum(0)\n",
        "        #\n",
        "        if column_sum == 0:\n",
        "            return -8\n",
        "        #\n",
        "        if column_sum == 1:\n",
        "            value,prvalue = torch.max(column.to(torch.int64),0)\n",
        "            return prvalue\n",
        "        if column_sum == bz:\n",
        "            return -1\n",
        "        exponents = arangeic(bz)\n",
        "        powers = 10**exponents\n",
        "        prvalue = (column.to(torch.int64) * powers).sum(0)\n",
        "        prvalue += 3 * (10 ** bz)\n",
        "        return prvalue\n",
        "\n",
        "    def printcolumn2(self,column):\n",
        "        #\n",
        "        assert len(column) == 2\n",
        "        #\n",
        "        column_sum = column.to(torch.int64).sum(0)\n",
        "        #\n",
        "        if column_sum == 0:\n",
        "            return -8\n",
        "        if column_sum == 2:\n",
        "            return 3\n",
        "        #\n",
        "        return column[1]\n",
        "\n",
        "    def printleft(self,left,loc):\n",
        "        #\n",
        "        a=self.alpha\n",
        "        a2=a*a\n",
        "        a3=a*a*a\n",
        "        a3z = a3+1\n",
        "        b=self.beta\n",
        "        bz = b+1\n",
        "        #\n",
        "        prarray = torch.zeros((a,bz),dtype = torch.int64,device=Dvc)\n",
        "        for x in range(a):\n",
        "            for y in range(bz):\n",
        "                column = left[loc,x,y]\n",
        "                prarray[x,y] = self.printcolumn2(column)\n",
        "        print(nump(prarray))\n",
        "        return\n",
        "\n",
        "    def printright(self,right,loc):\n",
        "        #\n",
        "        a=self.alpha\n",
        "        a2=a*a\n",
        "        a3=a*a*a\n",
        "        a3z = a3+1\n",
        "        b=self.beta\n",
        "        bz = b+1\n",
        "        #\n",
        "        prarray = torch.zeros((bz,a),dtype = torch.int64,device=Dvc)\n",
        "        for x in range(bz):\n",
        "            for y in range(a):\n",
        "                column = right[loc,x,y]\n",
        "                prarray[x,y] = self.printcolumn2(column)\n",
        "        print(nump(prarray))\n",
        "        return\n",
        "\n",
        "    def print_prod_left_right(self,Data,i):\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        #\n",
        "        print(\"---------------------------------------------------\")\n",
        "        print(\"at location\",itp(i),\"the prod, left and right are respectively\")\n",
        "        self.printprod(prod,i)\n",
        "        self.printleft(left,i)\n",
        "        self.printright(right,i)\n",
        "        print(\"---------------------------------------------------\")\n",
        "        return\n",
        "\n",
        "        \n",
        "    ##########################################################\n",
        "    \n",
        "    def makezbinatable(self):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        #\n",
        "        power = 2**a\n",
        "        #\n",
        "        zbatable = torch.zeros((power,a),dtype = torch.bool,device=Dvc)\n",
        "        for z in range(power):\n",
        "            zbatable[z,:] = zbinary(a,z)\n",
        "        return zbatable\n",
        "    \n",
        "    def collection(self,m):  # this doesn't work perfectly, it needs to be re-sieved afterwards\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        power = 2**a\n",
        "        gl = self.sga.gtlength\n",
        "        #\n",
        "        gtbin = self.sga.gtbinary\n",
        "        #\n",
        "        if m <= 0:\n",
        "            print(\"m <= 0 not allowed\")\n",
        "            raise CoherenceError(\"exiting\")\n",
        "        if m == 1:\n",
        "            zrangevx = arangeic(power).view(1,power).expand(gl,power)\n",
        "            detection = (gtbin >= zrangevx).all(0)\n",
        "            clength = detection.to(torch.int64).sum(0)\n",
        "            collec = arangeic(power)[detection].view(clength,m)\n",
        "            #\n",
        "            zbatablev = self.zbinatable.view(power,1,a)\n",
        "            collec_bin = zbatablev[detection]\n",
        "            #\n",
        "            previous_possibilities = torch.ones((clength,power),dtype = torch.bool,device=Dvc)\n",
        "            #\n",
        "            previous_subgroup = torch.ones((clength,gl),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        else:\n",
        "            cp,collec_prev,collec_bin_prev,poss_prev,subgroup_prev = self.collection(m-1)\n",
        "            #\n",
        "            detection_prev = poss_prev.view(cp*power)\n",
        "            #\n",
        "            subgroup_prev_vxr = subgroup_prev.view(cp,1,gl).expand(cp,power,gl).reshape(cp*power,gl)\n",
        "            z_current = arangeic(power).view(1,power,1).expand(cp,power,gl).reshape(cp*power,gl)\n",
        "            g_current = arangeic(gl).view(1,gl).expand(cp*power,gl)\n",
        "            detection_current = ((~subgroup_prev_vxr) | ( gtbin[g_current,z_current] >= z_current ) ).all(1)\n",
        "            #\n",
        "            detection = detection_prev & detection_current\n",
        "            #\n",
        "            clength = detection.to(torch.int64).sum(0)\n",
        "            collec_prev_next = collec_prev.view(cp,1,m-1).expand(cp,power,m-1).reshape(cp*power,m-1)\n",
        "            zrangevxr = arangeic(power).view(1,power,1).expand(cp,power,1).reshape(cp*power,1)\n",
        "            new_prev = collec_prev_next[detection]\n",
        "            zvector = zrangevxr[detection]\n",
        "            collec = torch.cat((new_prev,zvector),1)\n",
        "            #\n",
        "            collec_bin_prev_next = collec_bin_prev.view(cp,1,m-1,a).expand(cp,power,m-1,a).reshape(cp*power,m-1,a)\n",
        "            new_bin_prev = collec_bin_prev_next[detection]\n",
        "            zbatablevxr = self.zbinatable.view(1,power,1,a).expand(cp,power,1,a).reshape(cp*power,1,a)\n",
        "            zbavector = zbatablevxr[detection]\n",
        "            collec_bin = torch.cat((new_bin_prev,zbavector),1)\n",
        "            #\n",
        "            poss_prev_vxr = poss_prev.view(cp,1,power).expand(cp,power,power).reshape(cp*power,power)\n",
        "            previous_possibilities = poss_prev_vxr[detection]\n",
        "            #\n",
        "            subgroup_prev_vxr = subgroup_prev.view(cp,1,gl).expand(cp,power,gl).reshape(cp*power,gl)\n",
        "            previous_subgroup = subgroup_prev_vxr[detection]\n",
        "        #\n",
        "        grange_vx = arangeic(gl).view(1,gl,1).expand(clength,gl,m)\n",
        "        collec_vx = collec.view(clength,1,m).expand(clength,gl,m)\n",
        "        transform = gtbin[grange_vx,collec_vx]\n",
        "        transform_sort,t_indices = torch.sort(transform,2)\n",
        "        #\n",
        "        subgroup = (collec_vx == transform_sort).all(2)\n",
        "        # \n",
        "        #\n",
        "        next_transform = gtbin.view(1,gl,power).expand(clength,gl,power)\n",
        "        prev_bound = (collec[:,m-1]).view(clength,1,1).expand(clength,gl,power)\n",
        "        prev_subgroupvx = previous_subgroup.view(clength,gl,1).expand(clength,gl,power)\n",
        "        #\n",
        "        next_possib_prev = ( (~prev_subgroupvx) | (prev_bound <= next_transform) ).all(1)\n",
        "        possibilities = next_possib_prev & previous_possibilities\n",
        "        #\n",
        "        return clength,collec,collec_bin,possibilities,subgroup\n",
        "        \n",
        "    def collectiontest(self,amount):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        #\n",
        "        clength,collec,collec_bin,possibilities,subgroup = self.collection(b)\n",
        "        print(\"collection has length\",itp(clength))\n",
        "        upper = 20\n",
        "        if upper > clength:\n",
        "            upper = clength\n",
        "        print(\"first\",itp(upper),\"elements are as follows\")\n",
        "        #\n",
        "        collec_sum = (collec_bin.to(torch.int64).sum(2)).sum(1)\n",
        "        collec_sum1 = collec_bin.to(torch.int64).sum(1)\n",
        "        collec_sum2 = collec_bin.to(torch.int64).sum(2)\n",
        "        for q in range(a*b+1):\n",
        "            freq = (collec_sum == q).to(torch.int64).sum(0)\n",
        "            print(\"for amount\",itp(q),\"frequency\",itp(freq))\n",
        "        amount_detect = (collec_sum == amount)\n",
        "        #\n",
        "        ad_freq = amount_detect.to(torch.int64).sum(0)\n",
        "        collec_detect = collec_bin[amount_detect]\n",
        "        print(\"ad freq\",itp(ad_freq))\n",
        "        for i in range(ad_freq):\n",
        "            print(\"--------------------------\")\n",
        "            print(numpi(collec_detect[i]))\n",
        "            print(\"collec_sum1\",nump((collec_sum1[amount_detect])[i]))\n",
        "            print(\"collec_sum2\",nump((collec_sum2[amount_detect])[i]))\n",
        "        return\n",
        "    \n",
        "    def lex_lt(self,width,z1,z2):\n",
        "        #\n",
        "        assert width > 0\n",
        "        #\n",
        "        if width == 1:\n",
        "            z1new = z1[:,0]\n",
        "            z2new = z2[:,0]\n",
        "            lt = (z1new < z2new)\n",
        "            return lt\n",
        "        z1prev = z1[:,0:width-1]\n",
        "        z2prev = z2[:,0:width-1]\n",
        "        #\n",
        "        lt_prev = self.lex_lt(width-1,z1prev,z2prev)\n",
        "        #\n",
        "        eq_prev = (z1prev == z2prev).all(1)\n",
        "        #\n",
        "        z1new = z1[:,width-1]\n",
        "        z2new = z2[:,width-1]\n",
        "        lt = ( lt_prev | (eq_prev & (z1new < z2new)) )\n",
        "        return lt\n",
        "            \n",
        "    \n",
        "    def collection_sieve(self):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        power = 2**a\n",
        "        gl = self.sga.gtlength\n",
        "        #\n",
        "        gtbin = self.sga.gtbinary\n",
        "        #\n",
        "        clength,collec,collec_bin,possibilities,subgroup = self.collection(b)\n",
        "        #\n",
        "        collecvxr = collec.view(clength,1,b).expand(clength,gl,b).reshape(clength*gl,b)\n",
        "        grangevxr = arangeic(gl).view(1,gl,1).expand(clength,gl,b).reshape(clength*gl,b)\n",
        "        #\n",
        "        transform = gtbin[grangevxr,collecvxr]\n",
        "        transform_sort, t_indices = torch.sort(transform,1)\n",
        "        #\n",
        "        transform_ltv = self.lex_lt(b,transform_sort,collecvxr)\n",
        "        transform_lt = transform_ltv.view(clength,gl)\n",
        "        #\n",
        "        throw = transform_lt.any(1)\n",
        "        #\n",
        "        detection = (~throw)\n",
        "        sieved_length = detection.to(torch.int64).sum(0)\n",
        "        sieved_collection = collec[detection]\n",
        "        sieved_collection_bin = collec_bin[detection]\n",
        "        #\n",
        "        return sieved_length, sieved_collection, sieved_collection_bin\n",
        "    \n",
        "    def sieve_test(self):\n",
        "        sieved_length, sieved_collection, sieved_collection_bin = self.collection_sieve()\n",
        "        print(\"sieved collection has length\",itp(sieved_length))\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def make_init_left_table(self):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length, sieved_collection, sieved_collection_bin = self.collection_sieve()\n",
        "        #\n",
        "        init_left_table = torch.zeros((length,a,bz,2),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        left_value = sieved_collection_bin.permute(0,2,1)\n",
        "        init_left_table[:,:,0:b,1] = left_value\n",
        "        init_left_table[:,:,0:b,0] = ~left_value\n",
        "        init_left_table[:,:,b,0] = True\n",
        "        #\n",
        "        print(\" \")\n",
        "        print(\"available left table instances are 0 <= sigma <\",itp(length))\n",
        "        print(\"   ---> these should be noted as the possible values of sigma for the proof instances\")\n",
        "        #\n",
        "        zerocolumn = (~left_value).all(1)\n",
        "        zerocolumn_number = zerocolumn.to(torch.int64).sum(1)\n",
        "        overhalf = (2*zerocolumn_number > b)\n",
        "        overhalf_indices = arangeic(length)[overhalf]\n",
        "        print(\"   \")\n",
        "        print(\"locations with > half zero columns are\",nump(overhalf_indices))\n",
        "        print(\"   ---> it is suggested not to use the sigma instances in this list, specially for larger cases \")\n",
        "        print(\"   \")\n",
        "        #\n",
        "        return length,init_left_table\n",
        "    \n",
        "    ##########################################################\n",
        "    \n",
        "      \n",
        "        \n",
        "    def initialdata(self,instancevector,dropoutlimit):  \n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = len(instancevector)\n",
        "        prod = torch.ones((length,a,a,bz),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        left = self.init_left_table[instancevector]\n",
        "        right = torch.ones((length,bz,a,2),dtype = torch.bool,device=Dvc)\n",
        "        right[:,b,:,1] = False\n",
        "        #\n",
        "        if self.Pp.verbose:\n",
        "            print(\"initial prod at 0\")\n",
        "            self.printprod(prod,0)\n",
        "            print(\"initial left at 0\")\n",
        "            print(numpi(left[0,:,:,1]))\n",
        "            print(\"initial right at 0\")\n",
        "            print(numpi(right[0,:,:,1]))\n",
        "        #\n",
        "        depth = torch.zeros((length),dtype = torch.int,device=Dvc)\n",
        "        #\n",
        "        ternary = torch.ones((length,a,a,a,2),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        info = torch.zeros((length,self.Pp.infosize),dtype = torch.int64,device=Dvc)\n",
        "        info[:,self.Pp.sampleinfolower:self.Pp.sampleinfoupper] = -1\n",
        "        #\n",
        "        #\n",
        "        RawData = {\n",
        "            'length': length,\n",
        "            'depth': depth,\n",
        "            'prod': prod,\n",
        "            'left': left,\n",
        "            'right': right,\n",
        "            'ternary': ternary,\n",
        "            'info': info,\n",
        "        }\n",
        "        #\n",
        "        if dropoutlimit > 0:\n",
        "            rectangle = arangeic(length).view(length,1).expand(length,dropoutlimit).reshape(length*dropoutlimit)\n",
        "            index_slice = (torch.randperm(length*dropoutlimit,device=Dvc))[0:dropoutlimit]\n",
        "            indices = rectangle[index_slice]\n",
        "            AugmentedData = self.rr1.indexselectdata(RawData,indices)\n",
        "            # set phase\n",
        "            tirage = torch.rand((dropoutlimit),device = Dvc)\n",
        "            phase1_upper = 0.8 * self.Pp.splitting_probability\n",
        "            phase12_upper = 0.8\n",
        "            phase1 = (tirage < phase1_upper) \n",
        "            phase2 = (tirage < phase12_upper) & (~phase1)\n",
        "            AugmentedData['info'][:,self.Pp.phase][phase2] = 2\n",
        "            AugmentedData['info'][:,self.Pp.phase][phase1] = 1\n",
        "            return AugmentedData\n",
        "        return RawData\n",
        "         \n",
        "    def print_initial_data(self):\n",
        "        #\n",
        "        instancevector, proof_title = self.InAll()\n",
        "        InitialData = self.initialdata(instancevector,0)\n",
        "        length = InitialData['length']\n",
        "        print(proof_title)\n",
        "        print(\"initial data from this driver has length\",itp(length))\n",
        "        for i in range(length):\n",
        "            self.print_prod_left_right(InitialData,i)\n",
        "        print(\"- - - - - - - - - - - - - - - - - - -\")\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def classificationproof(self,Mstrat,Mlearn,dropoutlimit,proving_instances,title_text):  # dropoutlimit =-1 for no dropout\n",
        "        print(\"---   ---   ---   ---   ---   ---   ---   ---   ---\")\n",
        "        print(\"                classification proof\")\n",
        "        print(\"---   ---   ---   ---   ---   ---   ---   ---   ---\")\n",
        "        #\n",
        "        HST.title_text_sigma_proof = title_text\n",
        "        #\n",
        "        self.rr4.proofnumber = 0\n",
        "        self.rr4.proofinstance = 0\n",
        "        self.rr4.allnumbers = 1\n",
        "        #\n",
        "        if dropoutlimit == 0:\n",
        "            HST.reset_current_proof()\n",
        "        #\n",
        "        self.Cc.initialize()\n",
        "        #\n",
        "        InitialData = self.initialdata(proving_instances,dropoutlimit)\n",
        "        #\n",
        "        if dropoutlimit > 0:\n",
        "            AssocInitialData = self.rr2.process(InitialData)\n",
        "            activedetect,donedetect,impossibledetect = self.rr2.filterdata(AssocInitialData)\n",
        "            ActiveInitialData = self.rr1.detectsubdata(AssocInitialData,activedetect)\n",
        "            #\n",
        "            explore_upper = self.Pp.root_injection\n",
        "            if explore_upper > dropoutlimit:\n",
        "                explore_upper = dropoutlimit\n",
        "            self.Ll.prepoolExplore(Mlearn,ActiveInitialData,explore_upper)\n",
        "        #\n",
        "        pl,ActivePool,DonePool,prooflength = self.rr4.proofloop(Mstrat,Mlearn,self.Cc,InitialData,dropoutlimit)\n",
        "        #\n",
        "        print(\"---   ---   ---   ---   ---   ---   ---   ---   ---\")\n",
        "        print(\"this proof was treating alpha =\",itp(self.alpha),\"beta =\",itp(self.beta))\n",
        "        print(\"proof ended after step number\",itp(prooflength))\n",
        "        if ActivePool['length'] > 0:\n",
        "            print(\"proof outputs Active Data of length\",itp(ActivePool['length']))\n",
        "        print(\"proof has done count\",itp(self.rr4.donecount),end=' ')\n",
        "        print(\"and estimated cumulative nodes\",numpr(self.rr4.ECN,1))\n",
        "        self.donecount_collection += self.rr4.donecount\n",
        "        self.ECN_collection += self.rr4.ECN\n",
        "        print(\"classifier eq pool has length\",itp(self.Cc.eqlength))\n",
        "        #\n",
        "        if dropoutlimit == 0:\n",
        "            HST.record_current_proof()\n",
        "        #\n",
        "        print(\"---   ---   ---   ---   ---   ---   ---   ---   ---\")\n",
        "        print(\"          classification proof done\")\n",
        "        print(\"---   ---   ---   ---   ---   ---   ---   ---   ---\")\n",
        "        print(\"===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===\")\n",
        "        #\n",
        "        return\n",
        "\n",
        "    #### mini programs for creation of the instancevector_title object (it is really a pair)\n",
        "\n",
        "    def InAll(self):\n",
        "        instance_vector = arangeic(self.init_length)\n",
        "        #\n",
        "        title_text = F'for all sigma instances'\n",
        "        #\n",
        "        return instance_vector, instance_vector, title_text\n",
        "    \n",
        "    def InOne(self,instance):\n",
        "        assert 0 <= instance < self.init_length\n",
        "        instance_vector = torch.zeros((1),dtype = torch.int64,device=Dvc)\n",
        "        instance_vector[0] = instance\n",
        "        #\n",
        "        title_text = F'for sigma instance {instance}'\n",
        "        #\n",
        "        return instance_vector, instance_vector, title_text\n",
        "    \n",
        "    def InSeg(self,lower,upper):\n",
        "        assert 0 <= lower < self.init_length\n",
        "        assert lower < upper\n",
        "        if upper > self.init_length:\n",
        "            print(\"retracting the upper value to\",self.init_length)\n",
        "        upper_mod =upper\n",
        "        if upper_mod > self.init_length:\n",
        "            upper_mod = self.init_length\n",
        "        instance_vector = arangeic(self.init_length)[lower:upper]\n",
        "        #\n",
        "        title_text = F'for sigma instances in range {lower}:{upper_mod}'\n",
        "        #\n",
        "        return instance_vector, instance_vector, title_text\n",
        "\n",
        "    def InSkip(self,skip,lower,upper):\n",
        "        assert 0 <= lower < self.init_length\n",
        "        assert lower < upper\n",
        "        upper_mod =upper\n",
        "        if upper_mod > self.init_length:\n",
        "            upper_mod = self.init_length\n",
        "        detection = torch.zeros((self.init_length),dtype = torch.bool,device=Dvc)\n",
        "        detection[lower:upper] = True\n",
        "        detection[skip] = False\n",
        "        training_vector = arangeic(self.init_length)[detection]\n",
        "        #\n",
        "        proving_vector = torch.zeros((1),dtype = torch.int64,device=Dvc)\n",
        "        proving_vector[0] = skip\n",
        "        #\n",
        "        title_text = F'training for sigma in range {lower}:{upper} skipping and proving {skip}'\n",
        "        #\n",
        "        return proving_vector, training_vector, title_text\n",
        "    \n",
        "    def InList(self,proving_list,training_list):\n",
        "        proving_vector = torch.tensor(proving_list,dtype = torch.int64,device=Dvc)\n",
        "        training_vector = torch.tensor(training_list,dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        title_text = F'training for sigma instances in list {training_list} and proving {proving_list}'\n",
        "        #\n",
        "        return proving_vector, training_vector, title_text\n",
        "\n",
        "\n",
        "    \n",
        "    def basicloop(self,Mstrat,Mlearn,training_instances,title_text):\n",
        "        #\n",
        "        dropout2 = 300\n",
        "        #\n",
        "        HST.title_text_sigma_train = title_text\n",
        "        #\n",
        "        for i in range(self.Pp.basicloop_iterations):\n",
        "            print(\"------      ------      basic loop\",itp(i),\"------      ------\")\n",
        "            #\n",
        "            #\n",
        "            self.Pp.dropout_style = 'regular'\n",
        "            self.classificationproof(Mstrat,Mlearn,dropout2,training_instances,title_text)\n",
        "            self.Ll.prepoolSamples(Mlearn,self.rr4.SamplePool,self.Ll.new_examples_max)\n",
        "            #\n",
        "            self.Ll.scoreExamples(Mlearn)\n",
        "            #\n",
        "            self.Pp.dropout_style = 'adaptive'\n",
        "            #\n",
        "            self.classificationproof(Mstrat,Mlearn,dropout2,training_instances,title_text)\n",
        "            self.Ll.prepoolSamples(Mlearn,self.rr4.SamplePool,self.Ll.new_examples_max)\n",
        "            self.classificationproof(Mstrat,Mlearn,100,training_instances,title_text)\n",
        "            self.Ll.addscoredexamples(Mlearn)\n",
        "            self.Ll.prepoolSamples(Mlearn,self.rr4.SamplePool,self.Ll.new_examples_max)\n",
        "            #\n",
        "            self.Ll.scoreExamples(Mlearn)\n",
        "            self.Ll.scoreExplore(Mlearn)\n",
        "            self.Ll.scoreOutlier(Mlearn)\n",
        "            print(\"------  global learning ---   ----\",itp(i),\"------      ------\")\n",
        "            self.Ll.learningGlobal(Mlearn,self.Pp.basicloop_training_iterations)\n",
        "            if Mlearn.network2_trainable:\n",
        "                print(\"------  local learning  ---   ----\",itp(i),\"------      ------\")\n",
        "                self.Ll.learningLocal(Mlearn,self.Pp.basicloop_training_iterations)\n",
        "            else:\n",
        "                print(\"network 2 is not trainable\")\n",
        "            print(\"======      ======      ======      ======      ======      ======\")\n",
        "            gcc = gc.collect()\n",
        "            memReport('mg')\n",
        "            print(\"======      ======      ======      ======      ======      ======\")\n",
        "        print(\"end of\",itp(self.Pp.basicloop_iterations),\"iterations of the basic loop\")\n",
        "        print(\"======      ======      ======      ======      ======      ======\")\n",
        "        return\n",
        "    \n",
        "    def basicloop_classificationproof(self,Mstrat,Mlearn,proving_instances,training_instances,title_text):\n",
        "        #\n",
        "        print(\"suggested number of proof cycles: between 20 and 50\")\n",
        "        runs = int(input(\"input the number of proof cycles to do : \"))\n",
        "        for s in range(runs):\n",
        "            Dd.basicloop(Mstrat,Mlearn,training_instances,title_text)\n",
        "            print(\">>>\",s+1,\"   (out of \",runs,\" )\")\n",
        "            Dd.classificationproof(Mstrat,Mlearn,0,proving_instances,title_text)\n",
        "            HST.graph_history(self.Pp,'big')\n",
        "        return\n",
        "    \n",
        "    #### the following function automates the process of choosing a collection of instances to do\n",
        "\n",
        "    def instance_chooser(self):\n",
        "        print(\"choose instances, this chooser allows : all, one, seg, skip  (do by hand for list input---see optional cells below)\")\n",
        "        instance_type = input(\"input type : \")\n",
        "        if instance_type != \"all\" and instance_type != \"one\" and instance_type != \"seg\" and instance_type != \"skip\":\n",
        "            print(\"please use one of : all one seg skip\")\n",
        "            raise CoherenceError(\"exiting\")\n",
        "        if instance_type == \"all\":\n",
        "            print(F\"this will do all sigma instances in the existing range 0 <= sigma < {self.init_length}\")\n",
        "            proving_instances, training_instances, title_text = self.InAll()\n",
        "        if instance_type == \"one\":\n",
        "            print(F\"to do a single sigma instance, choose in range 0 <= sigma < {self.init_length}\")\n",
        "            sigma_instance = int(input(\"input sigma instance : \"))\n",
        "            proving_instances, training_instances, title_text = self.InOne(sigma_instance)\n",
        "        if instance_type == \"seg\":\n",
        "            print(\"to do sigma instances in a segment lower <= sigma < upper\")\n",
        "            segment_lower = int(input(\"input lower : \"))\n",
        "            segment_upper = int(input(\"input upper : \"))\n",
        "            proving_instances, training_instances, title_text = self.InSeg(segment_lower,segment_upper)\n",
        "        if instance_type == \"skip\":\n",
        "            print(\"to train on a segment skipping a single value, and do proofs of that value\")\n",
        "            segment_lower = int(input(\"input training segment lower : \"))\n",
        "            segment_upper = int(input(\"input training segment upper : \"))\n",
        "            skip = int(input(\"input instance to prove, and to skip during training : \"))\n",
        "            proving_instances, training_instances, title_text = self.InSkip(skip,segment_lower,segment_upper)\n",
        "        return proving_instances, training_instances, title_text\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0TwEIghPY-b"
      },
      "source": [
        "class Parameters :    # this becomes the first element of the relations datatype\n",
        "    def __init__(self):\n",
        "        #\n",
        "        print(\"please enter alpha, beta and model_n\")\n",
        "        print(\"alpha = | A - A^2 | and beta = | A^2 - A^3 |, we are doing |A^3 - A^4 | = | A^4 | = 1 and A is an associated-graded\")\n",
        "        print(\"ranges 2 <= alpha, beta <= 6 and alpha + beta <= 10, GPU needed for values bigger than around 3 or 4\")\n",
        "        print(\"model_n governs the size of the neural networks\")\n",
        "        print(\"suggested value n=4, can go to n=8 for more difficult cases on GPU\")\n",
        "        alpha = int(input(\"input alpha : \"))\n",
        "        beta = int(input(\"input beta : \"))\n",
        "        model_n = int(input(\"input model_n : \"))\n",
        "        #\n",
        "        #\n",
        "        if alpha < 2 or beta < 2 or alpha > 6 or beta > 6 or alpha + beta > 10:\n",
        "            print(\"suggested range is alpha,beta in [2,...,6] and alpha + beta <= 10\")\n",
        "            raise CoherenceError(\"exiting\")\n",
        "        if model_n < 1 or model_n > 12:\n",
        "            print(\"suggested range for model_n is [2,...,12]\")\n",
        "            raise CoherenceError(\"exiting\")\n",
        "        #\n",
        "        self.model_n = model_n  # could go up to 10 (that was what we did before)\n",
        "        #\n",
        "        self.alpha = alpha\n",
        "        self.alpha2 = alpha*alpha\n",
        "        self.alpha3 = alpha*alpha*alpha\n",
        "        self.alpha3z = self.alpha3+1\n",
        "        self.beta = beta\n",
        "        self.betaz = self.beta +1\n",
        "        #\n",
        "        HST.record_parameters(self.alpha,self.beta)\n",
        "        #\n",
        "        self.verbose = False\n",
        "        #\n",
        "        #\n",
        "        self.rdetect_max = 1000\n",
        "        #\n",
        "        self.global_params = 0\n",
        "        self.local_params = 0\n",
        "        #\n",
        "        # for rr1:\n",
        "        #\n",
        "        self.qvalue = 0.9\n",
        "        #\n",
        "        self.ascore_max = 7 # it is going to be a sum of a 0,1,2 and a 0,2,4\n",
        "        #\n",
        "        self.pastsize = 100\n",
        "        self.futuresize = 1000\n",
        "        #\n",
        "        # for rr2:\n",
        "        # \n",
        "        self.profile_filter_on = True\n",
        "        self.halfones_filter_on = True\n",
        "        #\n",
        "        # for rr3:\n",
        "        self.chunksize = 1000  # reduced this: for the 5,5 case 1000 led to memory overflow\n",
        "        self.chunksize_extra = 10\n",
        "        self.exponent = 0.9\n",
        "        self.chunkconstant = 300\n",
        "        self.chunkextent = 10\n",
        "        #\n",
        "        self.ukseuil = 0.5\n",
        "        #\n",
        "        self.newratio = 0.8  # how much we use the model rather than benchmark version to choose x,y\n",
        "        self.search_epsilon = 0.3  # the proportion of randomized strategy choices\n",
        "        #\n",
        "        # for rr4:\n",
        "        self.prooflooplength = 4000\n",
        "        self.done_max = 30000\n",
        "        #\n",
        "        #self.sleeptime = 120  # use this on a laptop\n",
        "        self.sleeptime = 0 # was 5\n",
        "        self.periodicity = 5\n",
        "        self.stopthreshold = 100000  # too big for a notebook utilisation\n",
        "        if torch.cuda.is_available(): self.trainingiterations = 4  # was 8  \n",
        "        else: self.trainingiterations = 1\n",
        "        #\n",
        "        self.root_injection = 100\n",
        "        self.randomize_q = 0.7\n",
        "        self.randomize_factor = 0.1\n",
        "        self.perturbation_factor = 0.3\n",
        "        #\n",
        "        self.spiral_mix_threshold = 10\n",
        "        self.dropout_style = 'regular'\n",
        "        #\n",
        "        #\n",
        "        # info ranges   ###################\n",
        "        #\n",
        "        self.infosize =  300   # currently we just need up to 250 for sample info stuff\n",
        "        #\n",
        "        self.samplepoints = 15\n",
        "        self.sampleinfolower = 50\n",
        "        self.sampleinfoupper = 250 # on ecrase vilower\n",
        "        #\n",
        "        self.fulldata_location = 25\n",
        "        self.phase = 12\n",
        "        #\n",
        "        ###################################\n",
        "        #\n",
        "        self.basicloop_iterations = 3\n",
        "        self.basicloop_training_iterations = 3\n",
        "        #\n",
        "        self.tweak_start = 200\n",
        "        self.tweak_step = 4\n",
        "        self.tweak_density = 0.05\n",
        "        self.tweak_epsilon = 0.02\n",
        "        self.tweak_decay = 0.9\n",
        "        #\n",
        "        self.splitting_probability = 1. - ( 10**(   -2. / ( (itt(self.alpha).to(torch.float))**2 )   ) )  \n",
        "        print(\"splitting probability\",numpr(self.splitting_probability,4) )\n",
        "        #\n",
        "        self.outlier_threshold = 2.5\n",
        "        #\n",
        "        ##### learner parameters\n",
        "        #\n",
        "        self.explore_max = 30000\n",
        "        self.examples_max = 5000\n",
        "        self.new_examples_max = 300\n",
        "        self.new_explore_max = 400\n",
        "        self.outlier_max = 2000\n",
        "        self.new_outliers_max = 300\n",
        "        #\n",
        "        self.noise_level = 0.05\n",
        "        self.noise_period = 50.\n",
        "        self.noise_decay = 0.5  # decay per period\n",
        "        #\n",
        "        \n",
        "        \n",
        "    \n",
        "\n",
        "      \n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUFF2aDbtFy3"
      },
      "source": [
        "###### end of basic program cells ###############\n",
        "#def stopcompile():  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZgeg0PIfvVC"
      },
      "source": [
        "##### to set up the environment: please run this cell and enter the desired alpha,beta and model_n\n",
        "#####    ( first suggested values are alpha = 3, beta = 2 and model_n = 4 )\n",
        "\n",
        "HST = Historical(10000)\n",
        "HST.proof_nodes_max = 100000  #  can be used to set the upper limit for proof lengths\n",
        "#\n",
        "Pp = Parameters()  # this will ask for alpha, beta and model_n\n",
        "#\n",
        "Pp.basicloop_iterations = 3  # default is 3\n",
        "Pp.basicloop_training_iterations = 3  # default is 3\n",
        "#\n",
        "Dd = Driver(Pp)\n",
        "#\n",
        "# the following could be set to False to turn off those additional filters\n",
        "Pp.profile_filter_on = True  # the default is True\n",
        "Pp.halfones_filter_on = True  # the default is True\n",
        "#\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr9KEh7DYsoH"
      },
      "source": [
        "#####  please run this cell to (re)initialize the model\n",
        "#\n",
        "Mm = SgModel(Pp)\n",
        "Pp.spiral_mix_threshold = 4\n",
        "Mmr = ProtoModel(Pp,'spiral_mix')\n",
        "#\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiwnPgGRxWSL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bedQJ04sdtZI"
      },
      "source": [
        "def stopcompile():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FekIZRBhbRtN"
      },
      "source": [
        "######################################################################################\n",
        "######## \n",
        "######## do a classificationproof with Mmr to set benchmark, then one with Mm for the first data point\n",
        "######## then do basicloop_classificationproof  for example for 50 iterations\n",
        "######## the basicloop_classificationproof can be repeated. Cumulative results are printed in the output\n",
        "########\n",
        "######## (a first suggested value for (alpha,beta)=(3,2) could be sigma = 3)\n",
        "######## \n",
        "######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzGARagnxrpW"
      },
      "source": [
        "##### \n",
        "#\n",
        "proving_instances, training_instances, title_text = Dd.instance_chooser()\n",
        "#\n",
        "HST.reset()\n",
        "Dd.classificationproof(Mmr,Mm,0,proving_instances,title_text)  # sets the benchmark value into HST, using ProtoModel Mmr\n",
        "Dd.classificationproof(Mm,Mm,0,proving_instances,title_text)   # to record proof 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ctvt4B8xraB"
      },
      "source": [
        "Dd.basicloop_classificationproof(Mm,Mm,proving_instances,training_instances,title_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9XMogiCc0oU"
      },
      "source": [
        "#######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBq1EwLndp7o"
      },
      "source": [
        "##### this is the end of the main notebook part, further optional cells are included below\n",
        "##### also note that the previous basicloop_classificationproof cell can be repeated, cumulating the proof history\n",
        "def stopcompile():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi87m2OyRe5F"
      },
      "source": [
        "#######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L231YRLotFy3"
      },
      "source": [
        "###### InList use case (this isn't covered by the instance chooser)\n",
        "###### \n",
        "## modify the following as desired: proving list, training list \n",
        "proving_instances, training_instances, title_text = Dd.InList([6,7],[5,8,9,10,11])  \n",
        "#\n",
        "HST.reset()\n",
        "Dd.classificationproof(Mmr,Mm,0,proving_instances,title_text)\n",
        "Dd.classificationproof(Mm,Mm,0,proving_instances,title_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_jlzU5pPL04"
      },
      "source": [
        "Dd.basicloop_classificationproof(Mm,Mm,proving_instances,training_instances,title_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT0FxTM8doPi"
      },
      "source": [
        "#######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH5_q7IS3_vZ"
      },
      "source": [
        "def stopcompile():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h76YmPMLSUtR"
      },
      "source": [
        "#######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKKOdSJ_SYan"
      },
      "source": [
        "#######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioRCb3G-r_gT"
      },
      "source": [
        "#### do the proofs in a range\n",
        "\n",
        "for i in range(13):\n",
        "    print(\"instance\",i)\n",
        "    proving_instances, training_instances, title_text = Dd.InOne(i)\n",
        "    Dd.classificationproof(Mm,Mm,0,proving_instances,title_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xHfmzsUSsui"
      },
      "source": [
        "#######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPJAn6E5aNP8"
      },
      "source": [
        "def stopcompile():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrUUis_cUwHL"
      },
      "source": [
        "#######################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdB5yuF7mUor"
      },
      "source": [
        "#### next: the class used to prove the theoretical minimum (this is for alpha,beta = 3,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyXUdRkWAE0b"
      },
      "source": [
        "class Minimizer :    # this becomes the first element of the relations datatype\n",
        "    def __init__(self,model,sigma,cutx,cuty,cutp):\n",
        "        #\n",
        "        #\n",
        "        self.Mm = model\n",
        "        self.Pp = self.Mm.pp\n",
        "        self.Dd = Driver(self.Pp)\n",
        "        assert self.Pp.alpha == 3\n",
        "        assert self.Pp.beta == 2\n",
        "        #\n",
        "        self.sigma = sigma\n",
        "        #\n",
        "        print(\"Minimizer for sigma =\",sigma)\n",
        "        #\n",
        "        self.rr4 = Relations4(self.Pp)\n",
        "        self.rr3 = self.rr4.rr3\n",
        "        self.rr2 = self.rr4.rr2\n",
        "        self.rr1 = self.rr4.rr1\n",
        "        self.alpha = self.Pp.alpha\n",
        "        self.alpha2 = self.Pp.alpha2\n",
        "        self.alpha3 = self.Pp.alpha3\n",
        "        self.alpha3z = self.Pp.alpha3z\n",
        "        self.beta = self.Pp.beta\n",
        "        self.betaz = self.Pp.betaz\n",
        "        #\n",
        "        self.length_max = 500000\n",
        "        #\n",
        "        instancevector, proof_title = self.Dd.InOne(self.sigma)\n",
        "        self.InitialData = self.Dd.initialdata(instancevector,0)\n",
        "        #\n",
        "        self.CurrentData = self.rr1.nulldata()\n",
        "        self.FullData = self.rr1.nulldata()\n",
        "        self.FullDoneData = self.rr1.nulldata()\n",
        "        #\n",
        "        self.up = torch.zeros((self.length_max),dtype = torch.int64,device=Dvc)\n",
        "        self.xvalue = torch.zeros((self.length_max),dtype = torch.int64,device=Dvc)\n",
        "        self.yvalue = torch.zeros((self.length_max),dtype = torch.int64,device=Dvc)\n",
        "        self.pvalue = torch.zeros((self.length_max),dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        self.lowerbound = torch.zeros((self.length_max),dtype = torch.int64,device=Dvc)\n",
        "        self.upperbound = torch.zeros((self.length_max),dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        self.down = torch.zeros((self.length_max,self.alpha,self.alpha,self.betaz),dtype = torch.int64,device=Dvc)\n",
        "        self.down[:,:,:,:] = -2\n",
        "        #\n",
        "        self.split = torch.zeros((self.length_max),dtype = torch.bool,device=Dvc)\n",
        "        self.inplay = torch.zeros((self.length_max),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        self.availablexyp = torch.zeros((self.length_max,self.alpha,self.alpha,self.betaz),dtype = torch.bool,device=Dvc)\n",
        "        self.availablexy = torch.zeros((self.length_max,self.alpha,self.alpha),dtype = torch.bool,device=Dvc)\n",
        "        #\n",
        "        self.cutx = cutx\n",
        "        self.cuty = cuty\n",
        "        self.cutp = cutp\n",
        "        print(\"cut to check is x y p =\",cutx,cuty,cutp)\n",
        "        #\n",
        "        self.combo_all()\n",
        "        \n",
        "\n",
        "    def next_stage_data(self,DataToSplit):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a2z = self.alpha2 +1\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        #\n",
        "        length = DataToSplit['length']\n",
        "        prod = DataToSplit['prod']\n",
        "        #\n",
        "        availablexyp = self.rr1.availablexyp(length,prod).view(length,a2,bz)\n",
        "        #\n",
        "        avxyp_amount = availablexyp.to(torch.float).sum(2).sum(1).sum(0)\n",
        "        avxyp_denom = itt(length).to(torch.float)\n",
        "        if avxyp_denom < 0.1:\n",
        "            avxyp_denom = 1.\n",
        "        availablexyp_average = avxyp_amount / avxyp_denom\n",
        "        ##print(\"average available xyp is\",numpr(availablexyp_average,2))\n",
        "        #\n",
        "        lrangevxr = arangeic(length).view(length,1,1).expand(length,a2,bz).reshape(length*a2*bz)\n",
        "        xyvectorvxr = arangeic(a2).view(1,a2,1).expand(length,a2,bz).reshape(length*a2*bz)\n",
        "        bzrangevxr = arangeic(bz).view(1,1,bz).expand(length,a2,bz).reshape(length*a2*bz)\n",
        "        #\n",
        "        verticaldetect = availablexyp[lrangevxr,xyvectorvxr,bzrangevxr]\n",
        "        # \n",
        "        #\n",
        "        ivector_vert = lrangevxr[verticaldetect]\n",
        "        xyvector_vert = xyvectorvxr[verticaldetect]\n",
        "        pvector_vert = bzrangevxr[verticaldetect]\n",
        "        #\n",
        "        prx = arangeic(a).view(a,1).expand(a,a).reshape(a2)\n",
        "        pry = arangeic(a).view(1,a).expand(a,a).reshape(a2)\n",
        "        #\n",
        "        xvector_vert = prx[xyvector_vert]\n",
        "        yvector_vert = pry[xyvector_vert]\n",
        "        #\n",
        "        #\n",
        "        NewData = self.rr1.upsplitting(DataToSplit,ivector_vert,xvector_vert,yvector_vert,pvector_vert)\n",
        "        #\n",
        "        #\n",
        "        ndlength = NewData['length']\n",
        "        #\n",
        "        #\n",
        "        AssocNewData = self.rr1.nulldata()\n",
        "        detection = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        newactive = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        newdone = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        newimpossible = torch.zeros((ndlength),dtype = torch.bool,device=Dvc)\n",
        "        lower = 0\n",
        "        for i in range(ndlength):\n",
        "            assert lower < ndlength\n",
        "            upper = lower + 1000\n",
        "            if upper > ndlength:\n",
        "                upper = ndlength\n",
        "            detection[:] = False\n",
        "            detection[lower:upper] = True\n",
        "            NewDataSlice = self.rr1.detectsubdata(NewData,detection)\n",
        "            AssocNewDataSlice = self.rr2.process(NewDataSlice)\n",
        "            AssocNewData = self.rr1.appenddata(AssocNewData,AssocNewDataSlice)\n",
        "            newactive_s,newdone_s,newimpossible_s = self.rr2.filterdata(AssocNewDataSlice)\n",
        "            newactive[lower:upper] = newactive_s\n",
        "            newdone[lower:upper] = newdone_s\n",
        "            newimpossible[lower:upper] = newimpossible_s\n",
        "            lower = upper\n",
        "            if lower >= ndlength:\n",
        "                break\n",
        "        #\n",
        "        NewActiveData = self.rr1.detectsubdata(AssocNewData,newactive)\n",
        "        #\n",
        "        NewDoneData = self.rr1.detectsubdata(AssocNewData,newdone)\n",
        "        #\n",
        "        assert len(newactive) == len(ivector_vert)\n",
        "        assert newactive.to(torch.int64).sum(0) == NewActiveData['length']\n",
        "        #\n",
        "        return ivector_vert,xvector_vert,yvector_vert,pvector_vert,newactive,newdone,newimpossible,NewActiveData, NewDoneData\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "    def manage_initial_stage(self):\n",
        "        #\n",
        "        self.FullData = self.rr1.copydata(self.InitialData)\n",
        "        #\n",
        "        fdlength = self.FullData['length']\n",
        "        assert fdlength == 1\n",
        "        #\n",
        "        #\n",
        "        self.up[0] = -1\n",
        "        self.xvalue[0] = -1\n",
        "        self.yvalue[0] = -1\n",
        "        self.pvalue[0] = -1\n",
        "        #\n",
        "        self.lowerbound[0] = 1\n",
        "        self.upperbound[0] = 1000\n",
        "        #\n",
        "        #\n",
        "        self.split[0] = False\n",
        "        self.inplay[0] = True\n",
        "        #\n",
        "        newactive_length = self.InitialData['length']\n",
        "        newactive_prod = self.InitialData['prod'] \n",
        "        #\n",
        "        new_availablexyp = self.rr1.availablexyp(newactive_length,newactive_prod).view(newactive_length,self.alpha,self.alpha,self.betaz)\n",
        "        new_availablexy = new_availablexyp.any(3)\n",
        "        #\n",
        "        self.availablexyp[0] = new_availablexyp\n",
        "        self.availablexy[0] = new_availablexy\n",
        "        #\n",
        "        new_fdlength = self.FullData['length']\n",
        "        #\n",
        "        self.FullData['info'][:,self.Pp.fulldata_location] = arangeic(new_fdlength)\n",
        "        #\n",
        "        ##print(\"initialized full data that now has length\",itp(new_fdlength))\n",
        "        return\n",
        "        \n",
        "\n",
        "    def manage_next_stage(self):\n",
        "        #\n",
        "        fdlength = self.FullData['length']\n",
        "        if fdlength == 0:\n",
        "            self.manage_initial_stage()\n",
        "            return\n",
        "        #\n",
        "        fd_split = self.split[0:fdlength] \n",
        "        fd_inplay = self.inplay[0:fdlength]\n",
        "        #\n",
        "        fd_current = fd_inplay & ~fd_split\n",
        "        DataToSplit = self.rr1.detectsubdata(self.FullData,fd_current)\n",
        "        #\n",
        "        fd_index = arangeic(fdlength)[fd_current]\n",
        "        #\n",
        "        ivector_vert,xvector_vert,yvector_vert,pvector_vert,newactive,newdone,newimpossible,NewActiveData, NewDoneData = self.next_stage_data(DataToSplit)\n",
        "        #\n",
        "        inew = fd_index[ivector_vert[newactive]]\n",
        "        xnew = xvector_vert[newactive]\n",
        "        ynew = yvector_vert[newactive]\n",
        "        pnew = pvector_vert[newactive]\n",
        "        #\n",
        "        newactive_length = NewActiveData['length']\n",
        "        newactive_prod = NewActiveData['prod']\n",
        "        #\n",
        "        lower = itt(fdlength).clone()\n",
        "        upper = fdlength + newactive_length\n",
        "        if upper > self.length_max:\n",
        "            print(\"upper is\",itp(upper),\"whereas max length is\",itp(self.length_max))\n",
        "            raise CoherenceError(\"length overflow\")\n",
        "        self.FullData = self.rr1.appenddata(self.FullData,NewActiveData)\n",
        "        self.FullDoneData = self.rr1.appenddata(self.FullDoneData,NewDoneData)\n",
        "        #\n",
        "        self.up[lower:upper] = inew\n",
        "        self.xvalue[lower:upper] = xnew\n",
        "        self.yvalue[lower:upper] = ynew\n",
        "        self.pvalue[lower:upper] = pnew\n",
        "        #\n",
        "        self.lowerbound[lower:upper] = 1\n",
        "        self.upperbound[lower:upper] = 1000\n",
        "        #\n",
        "        assert (~self.split[inew]).all(0)\n",
        "        assert self.inplay[inew].all(0) \n",
        "        #\n",
        "        self.split[inew] = True\n",
        "        self.split[lower:upper] = False\n",
        "        self.inplay[lower:upper] = True\n",
        "        #\n",
        "        self.down[inew,xnew,ynew,pnew] = arangeic(newactive_length) + lower\n",
        "        #\n",
        "        newimpdone = newimpossible | newdone\n",
        "        inew_id = fd_index[ivector_vert[newimpdone]]\n",
        "        xnew_id = xvector_vert[newimpdone]\n",
        "        ynew_id = yvector_vert[newimpdone]\n",
        "        pnew_id = pvector_vert[newimpdone]\n",
        "        #\n",
        "        self.down[inew_id,xnew_id,ynew_id,pnew_id] = -1\n",
        "        #\n",
        "        new_availablexyp = self.rr1.availablexyp(newactive_length,newactive_prod).view(newactive_length,self.alpha,self.alpha,self.betaz)\n",
        "        new_availablexy = new_availablexyp.any(3)\n",
        "        #\n",
        "        self.availablexyp[lower:upper] = new_availablexyp\n",
        "        self.availablexy[lower:upper] = new_availablexy\n",
        "        #\n",
        "        #\n",
        "        new_fdlength = self.FullData['length']\n",
        "        #\n",
        "        self.FullData['info'][:,self.Pp.fulldata_location] = arangeic(new_fdlength)\n",
        "        #\n",
        "        ##print(\"added\",itp(newactive_length),\"new instances to full data that now has length\",itp(new_fdlength))\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def fd_location(self,Data):\n",
        "        length = Data['length']\n",
        "        assert length > 0\n",
        "        return Data['info'][:,self.Pp.fulldata_location]\n",
        "\n",
        "\n",
        "\n",
        "    def bounding_proofloop(self,Mstrat,fd_instances):\n",
        "        #\n",
        "        self.upperbound[fd_instances] = 1\n",
        "        #\n",
        "        Input = self.rr1.indexselectdata(self.FullData, fd_instances)\n",
        "        #\n",
        "        InitialActiveData = self.rr2.process(Input)\n",
        "        activedetect, donedetect, impossibledetect = self.rr2.filterdata(InitialActiveData)\n",
        "        #\n",
        "        ActivePool = self.rr1.detectsubdata(InitialActiveData,activedetect)\n",
        "        #\n",
        "        stepcount = 0\n",
        "        #\n",
        "        ##print(\"at step\",itp(stepcount),\"currently proof nodes for fd instances in question are:\")\n",
        "        ##print(nump(self.upperbound[fd_instances]))\n",
        "        #\n",
        "        for i in range(self.rr4.prooflooplength):\n",
        "            stepcount += 1\n",
        "            prooflength = i\n",
        "            if ActivePool['length'] > 0:\n",
        "                #\n",
        "                print(\".\",end = '')\n",
        "                if (i%50) == 49:\n",
        "                    print(\" \")\n",
        "                if (i%100) == 0:\n",
        "                    print(i)\n",
        "                #\n",
        "                #\n",
        "                ChunkData, cdetection = self.rr3.selectchunk(ActivePool)\n",
        "                #\n",
        "                #\n",
        "                ProofCurrentData, DoneData = self.rr3.managesplit(Mstrat,ChunkData,False)\n",
        "                #\n",
        "                #\n",
        "                ActivePool = self.rr4.transitionactive(ActivePool,cdetection,ProofCurrentData)\n",
        "                # do the following before dropout    \n",
        "                #if dropoutlimit == 0:\n",
        "                    #EDN = itt(ActivePool['length']).clone().to(torch.float)\n",
        "                    #self.ECN += itt(CurrentData['length']).clone().to(torch.float)\n",
        "                    # this from proofloop indicates that we should add to our node counts the nodes in current data\n",
        "                #\n",
        "                if ProofCurrentData['length'] > 0:\n",
        "                    current_fd_location = self.fd_location(ProofCurrentData)\n",
        "                    fd_loc_list, fd_loc_counts = torch.unique(current_fd_location,return_counts = True)\n",
        "                    self.upperbound[fd_loc_list] = self.upperbound[fd_loc_list] + fd_loc_counts\n",
        "                #\n",
        "                ##print(\"at step\",itp(stepcount),\"currently proof nodes for fd instances in question are:\")\n",
        "                ##print(nump(self.upperbound[fd_instances]))\n",
        "                #\n",
        "                gcc = gc.collect()\n",
        "                #\n",
        "                if ActivePool['length'] == 0: \n",
        "                    break\n",
        "                if ActivePool['length'] > self.rr4.stopthreshold: \n",
        "                    print(\"over threshold --------->>>>>>>>>>>>>>>>> stopping\")\n",
        "                    break\n",
        "                #\n",
        "                #\n",
        "            if ActivePool['length'] == 0: \n",
        "                break\n",
        "            if ActivePool['length'] > self.rr4.stopthreshold: \n",
        "                print(\"over threshold --------->>>>>>>>>>>>>>>>> stopping\")\n",
        "                break\n",
        "            #\n",
        "            #\n",
        "        #\n",
        "        print(\"|||\")\n",
        "        #\n",
        "        ##print(\"proofs finished, resulting in proof nodes for fd instances in question as follows:\")\n",
        "        ##print(nump(self.upperbound[fd_instances]))\n",
        "        #\n",
        "        return \n",
        "\n",
        "    def calculate_current_upperbound(self):\n",
        "        fdlength = self.FullData['length']\n",
        "        if fdlength == 0:\n",
        "            print(\"no upper bounds to calculate\")\n",
        "            return\n",
        "        #\n",
        "        fd_split = self.split[0:fdlength] \n",
        "        fd_inplay = self.inplay[0:fdlength]\n",
        "        #\n",
        "        fd_current = fd_inplay & ~fd_split\n",
        "        #\n",
        "        fd_instances = arangeic(fdlength)[fd_current]\n",
        "        #\n",
        "        self.bounding_proofloop(self.Mm,fd_instances)\n",
        "        return\n",
        "\n",
        "    def recursive_bound_step(self):\n",
        "        #\n",
        "        fdlength = self.FullData['length']\n",
        "        assert fdlength > 0\n",
        "        #\n",
        "        fd_up  = self.up[0:fdlength] \n",
        "        fd_xvalue  = self.xvalue[0:fdlength] \n",
        "        fd_yvalue  = self.yvalue[0:fdlength] \n",
        "        fd_pvalue  = self.pvalue[0:fdlength] \n",
        "        #\n",
        "        fd_lowerbound  = self.lowerbound[0:fdlength] \n",
        "        fd_upperbound  = self.upperbound[0:fdlength] \n",
        "        #\n",
        "        fd_down  = self.down[0:fdlength] \n",
        "        #\n",
        "        fd_split  = self.split[0:fdlength] \n",
        "        fd_inplay  = self.inplay[0:fdlength]\n",
        "        #\n",
        "        fd_current = fd_inplay & ~fd_split \n",
        "        fd_lookat = fd_inplay & fd_split\n",
        "        fd_uppable = (fd_up >= 0)\n",
        "        #\n",
        "        fd_availablexyp  = self.availablexyp[0:fdlength] \n",
        "        fd_availablexy  = self.availablexy[0:fdlength] \n",
        "        #\n",
        "        #fd_impdone_xyp = (fd_down == -1)\n",
        "        #\n",
        "        fd_lowerbound_xyp = torch.zeros((fdlength,self.alpha,self.alpha,self.betaz),dtype = torch.int64,device=Dvc)\n",
        "        fd_upperbound_xyp = torch.zeros((fdlength,self.alpha,self.alpha,self.betaz),dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        ivalue_uppable = fd_up[fd_uppable]\n",
        "        xvalue_uppable = fd_xvalue[fd_uppable]\n",
        "        yvalue_uppable = fd_yvalue[fd_uppable]\n",
        "        pvalue_uppable = fd_pvalue[fd_uppable]\n",
        "        #\n",
        "        fd_lowerbound_xyp[ivalue_uppable,xvalue_uppable,yvalue_uppable,pvalue_uppable] = fd_lowerbound[fd_uppable]\n",
        "        fd_upperbound_xyp[ivalue_uppable,xvalue_uppable,yvalue_uppable,pvalue_uppable] = fd_upperbound[fd_uppable]\n",
        "        #\n",
        "        fd_lowerbound_xy_r = fd_lowerbound_xyp.sum(3).reshape(fdlength*self.alpha*self.alpha) + 1\n",
        "        fd_upperbound_xy_r = fd_upperbound_xyp.sum(3).reshape(fdlength*self.alpha*self.alpha) + 1\n",
        "        fd_availablexy_r = fd_availablexy.reshape(fdlength*self.alpha*self.alpha)\n",
        "        #\n",
        "        fd_lowerbound_xy_r[~fd_availablexy_r] = 10000\n",
        "        fd_upperbound_xy_r[~fd_availablexy_r] = 10000\n",
        "        #\n",
        "        fd_lowerbound_xy_rv = fd_lowerbound_xy_r.view(fdlength,self.alpha*self.alpha)\n",
        "        fd_upperbound_xy_rv = fd_upperbound_xy_r.view(fdlength,self.alpha*self.alpha)\n",
        "        #\n",
        "        fd_lowerbound_min,fdlbmindices = torch.min(fd_lowerbound_xy_rv,1)\n",
        "        fd_upperbound_min,fdubmindices = torch.min(fd_upperbound_xy_rv,1)\n",
        "        #\n",
        "        fd_lowerbound_new = fd_lowerbound.clone()\n",
        "        fd_upperbound_new = fd_upperbound.clone()\n",
        "        fd_lowerbound_new[fd_lookat] = fd_lowerbound_min[fd_lookat]\n",
        "        fd_upperbound_new[fd_lookat] = fd_upperbound_min[fd_lookat]\n",
        "        #\n",
        "        ##print(\"new lower bound\")\n",
        "        ##print(nump(fd_lowerbound_new))\n",
        "        ##print(\"new upper bound\")\n",
        "        ##print(nump(fd_upperbound_new))\n",
        "        #\n",
        "        self.lowerbound[0:fdlength] = fd_lowerbound_new\n",
        "        self.upperbound[0:fdlength] = fd_upperbound_new\n",
        "        return\n",
        "\n",
        "    def recursive_bound(self,iterations):\n",
        "        #\n",
        "        fdlength = self.FullData['length']\n",
        "        assert fdlength > 0\n",
        "        #\n",
        "        lower_all = self.lowerbound[0:fdlength].sum(0)\n",
        "        upper_all = self.upperbound[0:fdlength].sum(0)\n",
        "        ##print(\"init with lower sum\",itp(lower_all),\"upper sum\",itp(upper_all))\n",
        "        for i in range(iterations):\n",
        "            self.recursive_bound_step()\n",
        "            lower_new = self.lowerbound[0:fdlength].sum(0)\n",
        "            upper_new = self.upperbound[0:fdlength].sum(0)\n",
        "            ##print(\"iteration\",i,\"with lower sum\",itp(lower_new),\"upper sum\",itp(upper_new))\n",
        "            #\n",
        "            if lower_new == lower_all and upper_new == upper_all:\n",
        "                ##print(\"stabilizes\")\n",
        "                break\n",
        "            else:\n",
        "                lower_all = lower_new\n",
        "                upper_all = upper_new\n",
        "        ##print(\"done with recursive bound steps\")\n",
        "        return\n",
        "            \n",
        "\n",
        "\n",
        "    def remove_from_play(self,subset):\n",
        "        #\n",
        "        #\n",
        "        fdlength = self.FullData['length']\n",
        "        #\n",
        "        fd_up  = self.up[0:fdlength] \n",
        "        fd_up_mod = torch.clamp(fd_up,0,fdlength)\n",
        "        #\n",
        "        inplay_prev = self.inplay[0:fdlength].to(torch.int64).sum(0)\n",
        "        ##print(\"previous in play count is\",itp(inplay_prev))\n",
        "        ##print(\"removing\",itp(subset.to(torch.int64).sum(0)),\"locations from play\")\n",
        "        #\n",
        "        self.inplay[0:fdlength] = self.inplay[0:fdlength] & (~subset)\n",
        "        #\n",
        "        for i in range(100):\n",
        "            inplay_count = self.inplay[0:fdlength].to(torch.int64).sum(0)\n",
        "            inplay_up = self.inplay[fd_up_mod]\n",
        "            self.inplay[0:fdlength] = self.inplay[0:fdlength] & inplay_up\n",
        "            if self.inplay[0:fdlength].to(torch.int64).sum(0) == inplay_count:\n",
        "                break\n",
        "        inplay_new = self.inplay[0:fdlength].to(torch.int64).sum(0)\n",
        "        ##print(\"new in play count is\",itp(inplay_new))\n",
        "        #\n",
        "        return\n",
        "\n",
        "    def random_remove_from_play(self,threshold):\n",
        "        fdlength = self.FullData['length']\n",
        "        tirage = torch.rand((fdlength),device=Dvc)\n",
        "        subset = (tirage < threshold)\n",
        "        subset[0] = False\n",
        "        self.remove_from_play(subset)\n",
        "        return\n",
        "\n",
        "    def leave_in_play(self,instance):  # for now we assume that this is at the first stage\n",
        "        fdlength = self.FullData['length']\n",
        "        subset = torch.ones((fdlength),dtype = torch.bool,device=Dvc)\n",
        "        subset[0] = False\n",
        "        subset[instance] = False\n",
        "        self.remove_from_play(subset)\n",
        "        return\n",
        "\n",
        "    def initial_cut(self,x,y,p):\n",
        "        fdlength = self.FullData['length']\n",
        "        #assert fdlength == 28\n",
        "        instance = self.down[0,x,y,p]\n",
        "        subset = torch.ones((fdlength),dtype = torch.bool,device=Dvc)\n",
        "        subset[0] = False\n",
        "        subset[instance] = False\n",
        "        self.remove_from_play(subset)\n",
        "        return\n",
        "\n",
        "    def prune(self):\n",
        "        #\n",
        "        fdlength = self.FullData['length']\n",
        "        #\n",
        "        fd_lowerbound = self.lowerbound[0:fdlength]\n",
        "        fd_upperbound = self.upperbound[0:fdlength]\n",
        "        fd_inplay = self.inplay[0:fdlength]\n",
        "        #\n",
        "        badlocations = (fd_lowerbound > fd_upperbound) & fd_inplay\n",
        "        badlocations_count = badlocations.to(torch.int64).sum(0)\n",
        "        #\n",
        "        ##if badlocations_count > 0:\n",
        "            ##print(\"warning, we found\",itp(badlocations_count),\"bad locations\")\n",
        "        ##else:\n",
        "            ##print(\"all locations are good\")\n",
        "        #\n",
        "        attained = (fd_lowerbound == fd_upperbound) & fd_inplay\n",
        "        attained_count = attained.to(torch.int64).sum(0)\n",
        "        #\n",
        "        up_mod = torch.clamp(self.up[0:fdlength], 0, fdlength)\n",
        "        #\n",
        "        upperbound_up = self.upperbound[up_mod]\n",
        "        nonoptimal = (fd_lowerbound + 1) >= upperbound_up\n",
        "        nonoptimal[0] = False\n",
        "        nonoptimal_count = nonoptimal.to(torch.int64).sum(0)\n",
        "        #\n",
        "        to_remove = nonoptimal | attained\n",
        "        #\n",
        "        ##print(\"found\",itp(attained_count),\"attained and\",itp(nonoptimal_count),\"nonoptimal locations that we remove from play (along with everything below)\")\n",
        "        self.remove_from_play(to_remove)\n",
        "        ##print(\"done pruning\")\n",
        "        return\n",
        "        \n",
        "\n",
        "    def combo_init(self):\n",
        "        #print(\"------ initial combo segments ---------\")\n",
        "        #print(\"------ manage next stage (two iterations)\")\n",
        "        self.manage_next_stage()\n",
        "        self.manage_next_stage()\n",
        "        print(\"------ make initial cut at\",self.cutx,self.cuty,self.cutp)\n",
        "        self.initial_cut(self.cutx,self.cuty,self.cutp)\n",
        "        #print(\"------ calculate current upper bound\")\n",
        "        self.calculate_current_upperbound()\n",
        "        #print(\"------ recursive bound\")\n",
        "        self.recursive_bound(100)\n",
        "        #print(\"------ prune\")\n",
        "        self.prune()\n",
        "        #print(\"------ done with initial combo segment ---------\")\n",
        "        return\n",
        "\n",
        "    def combo_step(self):\n",
        "        #print(\"------ combo segment ---------\")\n",
        "        checkdone = self.check_done()\n",
        "        if checkdone:\n",
        "            print(\"|||\")\n",
        "            return 'done'\n",
        "        #print(\"------ manage next stage\")\n",
        "        self.manage_next_stage()\n",
        "        #print(\"------ calculate current upper bound\")\n",
        "        self.calculate_current_upperbound()\n",
        "        #print(\"------ recursive bound\")\n",
        "        self.recursive_bound(100)\n",
        "        #print(\"------ prune\")\n",
        "        self.prune()\n",
        "        #print(\"------ done with combo segment ---------\")\n",
        "        return 'continue'\n",
        "\n",
        "    def check_done(self):\n",
        "        fdlength = self.FullData['length']\n",
        "        #\n",
        "        fd_lowerbound = self.lowerbound[0:fdlength]\n",
        "        fd_upperbound = self.upperbound[0:fdlength]\n",
        "        fd_inplay = self.inplay[0:fdlength]\n",
        "        #\n",
        "        inplay_count = fd_inplay.to(torch.int64).sum(0)\n",
        "        if inplay_count > 1:\n",
        "            return False\n",
        "        else:\n",
        "            assert fd_inplay[0]\n",
        "            cutx = self.cutx\n",
        "            cuty = self.cuty\n",
        "            cutp = self.cutp\n",
        "            cut_instance = self.down[0,cutx,cuty,cutp]\n",
        "            assert self.lowerbound[cut_instance] == self.upperbound[cut_instance]\n",
        "            print(\"at initial cut location\",cutx,cuty,cutp,\"lower bound = upper bound =\",itp(self.lowerbound[cut_instance]))\n",
        "            print(\"this was for sigma =\",self.sigma)\n",
        "            #\n",
        "            lb_next = torch.zeros((self.alpha,self.alpha),dtype = torch.int64,device=Dvc)\n",
        "            ub_next = torch.zeros((self.alpha,self.alpha),dtype = torch.int64,device=Dvc)\n",
        "            for x in range(self.alpha):\n",
        "                for y in range(self.alpha):\n",
        "                    if self.availablexy[cut_instance,x,y]:\n",
        "                        lb_next[x,y] = 1\n",
        "                        ub_next[x,y] = 1\n",
        "                        for p in range(self.betaz):\n",
        "                            if self.availablexyp[cut_instance,x,y,p]:\n",
        "                                down_xyp = self.down[cut_instance,x,y,p]\n",
        "                                if down_xyp > 0:\n",
        "                                    lb_next_xyp = self.lowerbound[down_xyp]\n",
        "                                    assert lb_next_xyp > 0\n",
        "                                    lb_next[x,y] += lb_next_xyp\n",
        "                                    #\n",
        "                                    ub_next_xyp = self.upperbound[down_xyp]\n",
        "                                    ub_next[x,y] += ub_next_xyp\n",
        "            #\n",
        "            ##print(\"lower bounds for the next cut locations x,y are as follows:\")\n",
        "            ##print(nump(lb_next))\n",
        "            ##print(\"upper bounds\")\n",
        "            ##print(nump(ub_next))\n",
        "            ##print(\"full data length was\",itp(fdlength))\n",
        "            ##self.show_neural_network_results()\n",
        "            #\n",
        "        return True\n",
        "    \n",
        "    def check_done_print(self):\n",
        "        fdlength = self.FullData['length']\n",
        "        #\n",
        "        fd_lowerbound = self.lowerbound[0:fdlength]\n",
        "        fd_upperbound = self.upperbound[0:fdlength]\n",
        "        fd_inplay = self.inplay[0:fdlength]\n",
        "        #\n",
        "        inplay_count = fd_inplay.to(torch.int64).sum(0)\n",
        "        if inplay_count > 1:\n",
        "            print(\"not done: there are remaining locations in play\")\n",
        "            return False\n",
        "        else:\n",
        "            assert fd_inplay[0]\n",
        "            cutx = self.cutx\n",
        "            cuty = self.cuty\n",
        "            cutp = self.cutp\n",
        "            cut_instance = self.down[0,cutx,cuty,cutp]\n",
        "            assert self.lowerbound[cut_instance] == self.upperbound[cut_instance]\n",
        "            print(\"at initial cut location\",cutx,cuty,cutp,\"lower bound = upper bound =\",itp(self.lowerbound[cut_instance]))\n",
        "            print(\"this was for sigma =\",self.sigma)\n",
        "            #\n",
        "            lb_next = torch.zeros((self.alpha,self.alpha),dtype = torch.int64,device=Dvc)\n",
        "            ub_next = torch.zeros((self.alpha,self.alpha),dtype = torch.int64,device=Dvc)\n",
        "            for x in range(self.alpha):\n",
        "                for y in range(self.alpha):\n",
        "                    if self.availablexy[cut_instance,x,y]:\n",
        "                        lb_next[x,y] = 1\n",
        "                        ub_next[x,y] = 1\n",
        "                        for p in range(self.betaz):\n",
        "                            if self.availablexyp[cut_instance,x,y,p]:\n",
        "                                down_xyp = self.down[cut_instance,x,y,p]\n",
        "                                if down_xyp > 0:\n",
        "                                    lb_next_xyp = self.lowerbound[down_xyp]\n",
        "                                    assert lb_next_xyp > 0\n",
        "                                    lb_next[x,y] += lb_next_xyp\n",
        "                                    #\n",
        "                                    ub_next_xyp = self.upperbound[down_xyp]\n",
        "                                    ub_next[x,y] += ub_next_xyp\n",
        "            #\n",
        "            print(\"lower bounds for the next cut locations x,y are as follows:\")\n",
        "            print(nump(lb_next - 1))\n",
        "            print(\"upper bounds\")\n",
        "            print(nump(ub_next - 1))\n",
        "            print(\"lower and upper bounds should coincide. Add 1 to plug back into the previous cut location\")\n",
        "            print(\"full data length was\",itp(fdlength))\n",
        "            #self.show_neural_network_results()\n",
        "            #\n",
        "        return True\n",
        "    \n",
        "    def show_neural_network_results(self):  # not currently working, also our new network 2 has a different objective\n",
        "        #\n",
        "        fdlength = self.FullData['length']\n",
        "        #\n",
        "        subset = torch.zeros((fdlength),dtype = torch.bool,device=Dvc)\n",
        "        subset[0:500] = True\n",
        "        TruncatedFullData = self.rr1.detectsubdata(self.FullData,subset)\n",
        "        fd_network_output = self.Mm.network(TruncatedFullData).detach()\n",
        "        net2 = M.network2(TruncatedFullData)\n",
        "        fd_network2_output = net2.detach()\n",
        "        #\n",
        "        fdn2_exp_rootv = (10**fd_network2_output[0]).view(self.alpha*self.alpha)\n",
        "        avxy_rootv = self.availablexy[0].view(self.alpha*self.alpha)\n",
        "        fdn2_exp_rootv[~avxy_rootv] = 0.\n",
        "        fdn2_exp_rootmod = fdn2_exp_rootv.view(self.alpha,self.alpha)\n",
        "        print(\"network 2 output at root\")\n",
        "        print(numpr(fdn2_exp_rootmod,2))\n",
        "        #\n",
        "        cutx = self.cutx\n",
        "        cuty = self.cuty\n",
        "        cutp = self.cutp\n",
        "        cut_instance = self.down[0,cutx,cuty,cutp]\n",
        "        #\n",
        "        fd_network2_cut_instance = fd_network2_output[cut_instance]\n",
        "        fdn2_exp_v = (10**fd_network2_cut_instance).view(self.alpha*self.alpha)\n",
        "        print(\"network 2 gives the following matrix (after exponentiating base 10), unavailable = 0\")\n",
        "        avxy_v = self.availablexy[cut_instance].view(self.alpha*self.alpha)\n",
        "        fdn2_exp_v[~avxy_v] = 0.\n",
        "        fdn2_exp_mod = fdn2_exp_v.view(self.alpha,self.alpha)\n",
        "        print(numpr(fdn2_exp_mod,2))\n",
        "        #\n",
        "        fdn_exp_sum = torch.zeros((self.alpha,self.alpha),dtype = torch.float,device=Dvc)\n",
        "        for x in range(self.alpha):\n",
        "            for y in range(self.alpha):\n",
        "                if self.availablexy[cut_instance,x,y]:\n",
        "                    fdn_exp_sum[x,y] = 1.\n",
        "                    for p in range(self.betaz):\n",
        "                        if self.availablexyp[cut_instance,x,y,p]:\n",
        "                            down_xyp = self.down[cut_instance,x,y,p]\n",
        "                            if down_xyp > 0:\n",
        "                                fdn_next_xyp = 10**(fd_network_output[down_xyp])\n",
        "                                fdn_exp_sum[x,y] += fdn_next_xyp\n",
        "        #\n",
        "        print(\"summing the results of the first network gives the following matrix\")\n",
        "        print(numpr(fdn_exp_sum,2))\n",
        "        print(\"= = = = = =\")\n",
        "        return\n",
        "        \n",
        "    def combo_all(self):\n",
        "        self.combo_init()\n",
        "        for i in range(20):\n",
        "            ##print(\"===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===\")\n",
        "            ##print(\"===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===\")\n",
        "            print(\">>>>>>>>>>>>>>>>>>>>> combo step iteration number\",i)\n",
        "            ##print(\"===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===\")\n",
        "            ##print(\"===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===\")\n",
        "            step_result = self.combo_step()\n",
        "            if step_result == 'done':\n",
        "                break\n",
        "        ##print(\"===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===   ===\")\n",
        "        print(\"combo all is completed.\")\n",
        "        return\n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "class MinimizerHistory :    # this becomes the first element of the relations datatype\n",
        "    def __init__(self,model,sigma):\n",
        "        #\n",
        "        #\n",
        "        self.Mm = model\n",
        "        self.Pp = self.Mm.pp\n",
        "        self.Dd = Driver(self.Pp)\n",
        "        #\n",
        "        self.sigma = sigma\n",
        "        #\n",
        "        print(\"Minimizer History for sigma =\",sigma)\n",
        "        print(\"with profile_filter_on =\",self.Pp.profile_filter_on,\"and halfones_filter_on =\",self.Pp.halfones_filter_on)\n",
        "        \n",
        "        #\n",
        "        self.rr4 = Relations4(self.Pp)\n",
        "        self.rr3 = self.rr4.rr3\n",
        "        self.rr2 = self.rr4.rr2\n",
        "        self.rr1 = self.rr4.rr1\n",
        "        self.alpha = self.Pp.alpha\n",
        "        self.alpha2 = self.Pp.alpha2\n",
        "        self.alpha3 = self.Pp.alpha3\n",
        "        self.alpha3z = self.Pp.alpha3z\n",
        "        self.beta = self.Pp.beta\n",
        "        self.betaz = self.Pp.betaz\n",
        "        #\n",
        "        instancevector, proof_title = self.Dd.InOne(self.sigma)\n",
        "        self.InitialData = self.Dd.initialdata(instancevector,0)\n",
        "        #\n",
        "        self.results = torch.zeros((self.alpha,self.alpha,self.betaz),dtype = torch.int64,device=Dvc)\n",
        "        #\n",
        "        idlength = self.InitialData['length']\n",
        "        idprod = self.InitialData['prod']\n",
        "        self.availablexyp = self.rr1.availablexyp(idlength,idprod)[0]\n",
        "        #\n",
        "\n",
        "    def print_results(self):\n",
        "        #\n",
        "        results_sum = self.results.sum(2)\n",
        "        #\n",
        "        minimal = results_sum[0,0]\n",
        "        for x in range(self.alpha):\n",
        "            for y in range(self.alpha):\n",
        "                if results_sum[x,y] < minimal:\n",
        "                    minimal = results_sum[x,y]\n",
        "        root_minimum = minimal + 1\n",
        "        print(\"=+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=\")\n",
        "        print(\"=+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=\")\n",
        "        print(\"               aggregation of results for sigma =\",itp(self.sigma))\n",
        "        print(\"=+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=\")\n",
        "        print(\"=+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=   =+=\")\n",
        "        #\n",
        "        print(\"summed results according to initial cut location are:\")\n",
        "        print(nump(results_sum))\n",
        "        print(\"the minimal number of nodes including the root is\",itp(root_minimum))\n",
        "        print(\"this was with profile_filter_on =\",self.Pp.profile_filter_on,\"and halfones_filter_on =\",self.Pp.halfones_filter_on)\n",
        "        print(\"---------------------------------------------------------------------\")\n",
        "        return\n",
        "\n",
        "    def minimize_all(self):\n",
        "        #\n",
        "        for x in range(self.alpha):\n",
        "            for y in range(self.alpha):\n",
        "                for p in range(self.betaz):\n",
        "                    if self.availablexyp[x,y,p]:\n",
        "                        Min = Minimizer(self.Mm,self.sigma,x,y,p)\n",
        "                        cut_instance = Min.down[0,x,y,p]\n",
        "                        assert Min.lowerbound[cut_instance] == Min.upperbound[cut_instance]\n",
        "                        self.results[x,y,p] = Min.lowerbound[cut_instance]\n",
        "        self.print_results()\n",
        "        return\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8vbtDDznb39"
      },
      "source": [
        "Pp.profile_filter_on = True\n",
        "Pp.halfones_filter_on = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmQy8SLd5C-0"
      },
      "source": [
        "MH = MinimizerHistory(Mmr,12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGZSQ9BA5IBG"
      },
      "source": [
        "MH.minimize_all()  # this does all the cases in a row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWH69OsD5nGY"
      },
      "source": [
        "def stopcompile():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZeFVD6I5nGV"
      },
      "source": [
        "Min = Minimizer(Mm,3,0,0,0)  # individual cases: sigma, x, y, p\n",
        "Min.check_done_print()\n",
        "#time.sleep(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2FSGLpF5nGV"
      },
      "source": [
        "Min = Minimizer(Mm,3,0,0,1) # individual cases: sigma, x, y, p\n",
        "Min.check_done_print()\n",
        "#time.sleep(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3cwNf6c5nGV"
      },
      "source": [
        "Min = Minimizer(Mm,3,0,0,2) # individual cases: sigma, x, y, p \n",
        "Min.check_done_print()\n",
        "#time.sleep(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ihsUmWY5nGZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-m4E1DBTBe2"
      },
      "source": [
        "###### next: a class that can be used to search for specific examples, it was a debugging tool kept here for reference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9i7tszO6gFA"
      },
      "source": [
        "class FindWeirdStuff :    \n",
        "    def __init__(self,Dd,Mm):\n",
        "        #\n",
        "        #\n",
        "        self.Dd = Dd\n",
        "        self.Ll = self.Dd.Ll\n",
        "        self.Mm = Mm\n",
        "        #\n",
        "        self.Pp = self.Dd.Pp\n",
        "        #\n",
        "        self.rr4 = self.Dd.rr4\n",
        "        self.rr3 = self.rr4.rr3\n",
        "        self.rr2 = self.rr4.rr2\n",
        "        self.rr1 = self.rr4.rr1\n",
        "        self.alpha = self.Pp.alpha\n",
        "        self.alpha2 = self.Pp.alpha2\n",
        "        self.alpha3 = self.Pp.alpha3\n",
        "        self.alpha3z = self.Pp.alpha3z\n",
        "        self.beta = self.Pp.beta\n",
        "        self.betaz = self.Pp.betaz\n",
        "        #\n",
        "\n",
        "    def sample_box(self,xlower,xupper,ylower,yupper):\n",
        "        #\n",
        "        smb,DataBatch,scorebatch = self.Ll.selectminibatch(500)\n",
        "        #\n",
        "        if not smb:\n",
        "            print(\"select mini batch fails\")\n",
        "            return False,None\n",
        "        predictedscore = self.Mm.network(DataBatch)\n",
        "        #\n",
        "        detectionx = (xlower < scorebatch) & (scorebatch < xupper)\n",
        "        detectiony = (ylower < predictedscore) & (predictedscore < yupper)\n",
        "        detection = detectionx & detectiony\n",
        "        #\n",
        "        DetectedData = self.rr1.detectsubdata(DataBatch,detection)\n",
        "        #\n",
        "        return True, DetectedData\n",
        "\n",
        "    def printright(self,right,loc):\n",
        "        #\n",
        "        a=self.alpha\n",
        "        a2=a*a\n",
        "        a3=a*a*a\n",
        "        a3z = a3+1\n",
        "        b=self.beta\n",
        "        bz = b+1\n",
        "        #\n",
        "        prarray = torch.zeros((bz,a),dtype = torch.int64,device=Dvc)\n",
        "        for x in range(bz):\n",
        "            for y in range(a):\n",
        "                column = right[loc,x,y]\n",
        "                prarray[x,y] = self.Dd.printcolumn2(column)\n",
        "        print(nump(prarray))\n",
        "        return\n",
        "\n",
        "    def print_prod_left_right(self,Data,i):\n",
        "        prod = Data['prod']\n",
        "        left = Data['left']\n",
        "        right = Data['right']\n",
        "        #\n",
        "        print(\"---------------------------------------------------\")\n",
        "        print(\"at location\",itp(i),\"the prod, left and right are respectively\")\n",
        "        self.Dd.printprod(prod,i)\n",
        "        self.Dd.printleft(left,i)\n",
        "        self.printright(right,i)\n",
        "        print(\"---------------------------------------------------\")\n",
        "        return\n",
        "\n",
        "    def print_column_bool(self,column):\n",
        "        clength = len(column)\n",
        "        toprint = 3 * (10 ** (clength))\n",
        "        for q in range(clength):\n",
        "            toprint += (column[q].to(torch.int64) * (10 ** q) )\n",
        "        return toprint\n",
        "\n",
        "    def print_bool_tensor(self,a,b,c,btensor):\n",
        "        prarray = torch.zeros((a,b),dtype = torch.int64,device=Dvc)\n",
        "        for x in range(a):\n",
        "            for y in range(b):\n",
        "                column = btensor[x,y]\n",
        "                prarray[x,y] = self.print_column_bool(column)\n",
        "        print(nump(prarray))\n",
        "        return\n",
        "\n",
        "\n",
        "    def print_one_sample_from_box(self,xlower,xupper,ylower,yupper):\n",
        "        #\n",
        "        sb,DetectedData = self.sample_box(xlower,xupper,ylower,yupper)\n",
        "        #\n",
        "        if not sb:\n",
        "            print(\"exit from one sample box\")\n",
        "            return\n",
        "        length = DetectedData['length']\n",
        "        if length == 0:\n",
        "            print(\"didn't detect any samples in this box\")\n",
        "            return\n",
        "        #\n",
        "        AssocData = self.rr2.process(DetectedData)\n",
        "        activedetect, donedetect, impossibledetect = self.rr2.filterdata(AssocData)\n",
        "        permutation = torch.randperm(length,device=Dvc)\n",
        "        loc = permutation[0]\n",
        "        print(\"before processing:\")\n",
        "        self.print_prod_left_right(DetectedData,loc)\n",
        "        print(\"after processing:\")\n",
        "        self.print_prod_left_right(AssocData,loc)\n",
        "        print(\"the status of this location is :\",end = ' ')\n",
        "        if activedetect[loc]:\n",
        "            print(\"active\")\n",
        "        if donedetect[loc]:\n",
        "            print(\"done\")\n",
        "        if impossibledetect[loc]:\n",
        "            print(\"impossible\")\n",
        "        return\n",
        "\n",
        "    def av_root(self,DataToSplit,i):\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a2z = self.alpha2 +1\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        #\n",
        "        length = DataToSplit['length']\n",
        "        prod = DataToSplit['prod']\n",
        "        #\n",
        "        availablexypi = self.rr1.availablexyp(length,prod).view(length,a,a,bz)[i]\n",
        "        #\n",
        "        print(\"at root, availablexyp is:\")\n",
        "        self.print_bool_tensor(a,a,bz,availablexypi)\n",
        "        #\n",
        "        return\n",
        "\n",
        "    def split_by_hand(self,DataToSplit,i,x,y,p):\n",
        "        a = self.alpha\n",
        "        a2 = self.alpha2\n",
        "        a2z = self.alpha2 +1\n",
        "        a3 = self.alpha3\n",
        "        a3z = self.alpha3z\n",
        "        b = self.beta\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        #\n",
        "        length = DataToSplit['length']\n",
        "        prod = DataToSplit['prod']\n",
        "        #\n",
        "        availablexypi = self.rr1.availablexyp(length,prod).view(length,a,a,bz)[i]\n",
        "        #\n",
        "        available_instance = availablexypi[x,y,p]\n",
        "        if available_instance:\n",
        "            print(\"this instance\",itp(x),itp(y),itp(p),\"is available\")\n",
        "        else:\n",
        "            print(\"this instance\",itp(x),itp(y),itp(p),\"is not available\")\n",
        "            return\n",
        "        #\n",
        "        ivector = torch.zeros((1),dtype = torch.int64,device=Dvc)\n",
        "        ivector[:] = i\n",
        "        xvector = torch.zeros((1),dtype = torch.int64,device=Dvc)\n",
        "        xvector[:] = x\n",
        "        yvector = torch.zeros((1),dtype = torch.int64,device=Dvc)\n",
        "        yvector[:] = y\n",
        "        pvector = torch.zeros((1),dtype = torch.int64,device=Dvc)\n",
        "        pvector[:] = p\n",
        "        #\n",
        "        #\n",
        "        NewData = self.rr1.upsplitting(DataToSplit,ivector,xvector,yvector,pvector)\n",
        "        #\n",
        "        #\n",
        "        ndlength = NewData['length']\n",
        "        assert ndlength == 1\n",
        "        #\n",
        "        AssocNewData = self.rr2.process(NewData)\n",
        "        #\n",
        "        newactive,newdone,newimpossible = self.rr2.filterdata(AssocNewData)\n",
        "        if newactive[0]:\n",
        "            print(\"active\")\n",
        "        if newdone[0]:\n",
        "            print(\"done\")\n",
        "        if newimpossible[0]:\n",
        "            print(\"impossible\")\n",
        "        #\n",
        "        AND_prod = AssocNewData['prod']\n",
        "        AND_length = 1\n",
        "        AND_availablexyp = self.rr1.availablexyp(AND_length,AND_prod).view(1,a,a,bz)[0]\n",
        "        print(\"after cut at\",itp(x),itp(y),itp(p),\"availablexyp is:\")\n",
        "        self.print_bool_tensor(a,a,bz,AND_availablexyp)\n",
        "        print(\"prod is\")\n",
        "        self.print_bool_tensor(a,a,bz,AND_prod[0])\n",
        "        #\n",
        "        #\n",
        "        return AND_prod\n",
        "        \n",
        "    def show_cut_column(self,sigma,x,y):  # shows the results of (x,y,p) cut starting from root, for all p\n",
        "        #\n",
        "        InitialData = self.Dd.initialdata(self.Dd.InOne(sigma),0)\n",
        "        #\n",
        "        self.av_root(InitialData,0)\n",
        "        #\n",
        "        for p in range(self.betaz):\n",
        "            #\n",
        "            andprod = self.split_by_hand(InitialData,0,x,y,p)\n",
        "        return\n",
        "\n",
        "    def searchprod(self,Data,trprod):\n",
        "        #\n",
        "        a = self.alpha\n",
        "        bz = self.betaz\n",
        "        #\n",
        "        length = Data['length']\n",
        "        prod = Data['prod']\n",
        "        #\n",
        "        if length == 0:\n",
        "            print(\"length is 0\")\n",
        "            return 0,None\n",
        "        prodv = prod.view(length,a*a*bz)\n",
        "        trprodv = trprod.view(1,a*a*bz).expand(length,a*a*bz)\n",
        "        #\n",
        "        detection = (prodv == trprodv).all(1)\n",
        "        #\n",
        "        detected_indices = arangeic(length)[detection]\n",
        "        detected_length = detection.to(torch.int64).sum(0)\n",
        "        print(\"detected\",itp(detected_length),\"occurences out of\",itp(length))\n",
        "        return detected_length, detected_indices\n",
        "        \n",
        "\n",
        "    def tracer(self,trprod):\n",
        "        #\n",
        "        print(\"Examples:\",end=' ')\n",
        "        dl,di = self.searchprod(self.Ll.Examples,trprod)\n",
        "        print(\"ExamplesPrePool:\",end=' ')\n",
        "        dl,di = self.searchprod(self.Ll.ExamplesPrePool,trprod)\n",
        "        print(\"ExplorePrePool:\",end=' ')\n",
        "        dl,di = self.searchprod(self.Ll.ExplorePrePool,trprod)\n",
        "        print(\"OutlierPrePool:\",end=' ')\n",
        "        dl,di = self.searchprod(self.Ll.OutlierPrePool,trprod)\n",
        "        #\n",
        "        #\n",
        "        return\n",
        "\n",
        "    def tracer_root(self,sigma):\n",
        "        #\n",
        "        InitialData = self.Dd.initialdata(self.Dd.InOne(sigma),0)\n",
        "        trprod = InitialData['prod'][0]\n",
        "        #\n",
        "        self.tracer(trprod)\n",
        "        #\n",
        "        return\n",
        "        \n",
        "    def tracer_subroot(self,sigma,x,y,p):\n",
        "        #\n",
        "        InitialData = self.Dd.initialdata(self.Dd.InOne(sigma),0)\n",
        "        trprod = self.split_by_hand(InitialData,0,x,y,p)\n",
        "        #\n",
        "        self.tracer(trprod)\n",
        "        #\n",
        "        return\n",
        "        \n",
        "    def trace(self,stringtoprint,Data,sigma,x,y,p):\n",
        "        #\n",
        "        InitialData = self.Dd.initialdata(self.Dd.InOne(sigma),0)\n",
        "        trprod = self.split_by_hand(InitialData,0,x,y,p)\n",
        "        #\n",
        "        print(stringtoprint,end=' ')\n",
        "        dl,di = self.searchprod(Data,trprod)\n",
        "        return\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIO8xqo86f7Q"
      },
      "source": [
        "Fws = FindWeirdStuff(Dd,Mm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYUjkZk9A8UX"
      },
      "source": [
        "### a few things to do with that"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOKDsTRnqzet"
      },
      "source": [
        "Fws.tracer_root(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-FQJfCGo3fq"
      },
      "source": [
        "Fws.tracer_subroot(5,0,0,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHFcG82C9rGp"
      },
      "source": [
        "Fws.show_cut_column(5,2,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB8LY9qr6fvZ"
      },
      "source": [
        "Fws.print_one_sample_from_box(0.25,0.35,0.0,0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fSoPVbRO8s6"
      },
      "source": [
        "Fws.print_one_sample_from_box(0.0,0.05,0.0,0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfuHQqsL5nGZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYX2EyJ2wHsX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFl32757SYV4"
      },
      "source": [
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4mUo4HAAz5z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzrZVkYhAz2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGKXzHZSAzzI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDp0QPTuAzwN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGLkQp2MAzpt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC-NQ46nAzgo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obzpg_PWymMX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}